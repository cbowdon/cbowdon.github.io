[
  {
    "objectID": "proxy.html",
    "href": "proxy.html",
    "title": "Working with (around) proxies",
    "section": "",
    "text": "N.B. This article is ten years old and certainly does not represent best practices in the 2020s.\nThis is something that I seem to have to do on a monthly basis on various projects: a throwaway script that includes getting some resource from the Internet. This means I have to spar with my old nemesis the corporate proxy and her minions, the 407 and the SSL verification error.\n$DAYJOB uses an authenticated proxy and their own root certificate. It’s seamless for Office and IE users because their system is pre-configured. But not for those of us who are spinning up VMs and installing packages and come to think of it almost everything you need to do as a software engineer. (It also can /weaken/ rather than enhance security, the stated aim, but that’s an article for another day.)\nThis is a quick guide-by-example for how to authenticated against a proxy and ignore any SSL verification errors in a variety of scripting languages. The examples use only the standard library for a widely available (read: old) version of each language. The assumption is that if you can’t reach the Internet, you don’t have better packages or newer versions.\nIt is not an example of how to write good production software. It is a catalogue of dirty workarounds. You have been warned."
  },
  {
    "objectID": "proxy.html#bash",
    "href": "proxy.html#bash",
    "title": "Working with (around) proxies",
    "section": "Bash",
    "text": "Bash\nMany applications respect the =$http_proxy= environmental argument, so it’s worth setting this in your =.bash_profile=. Those that don’t often take command line flags for a proxy and to ignore SSL verification.\n  http_proxy=http://username:password@host:port\n  export $http_proxy\n\n  curl --proxy $http_proxy --insecure --url ... \n\n  git -c http.proxy=$http_proxy -c http.sslVerify=false clone ..."
  },
  {
    "objectID": "proxy.html#python-2",
    "href": "proxy.html#python-2",
    "title": "Working with (around) proxies",
    "section": "Python 2",
    "text": "Python 2\nThis works in Python 2.6 and 2.7. In Python 3 the principles are the same but urllib has been reorganized.\n\n  import logging\n  import ssl\n  import urllib2\n\n  logger = logging.getLogger('demo')\n   \n  def proxy_opener(http_proxy):\n      \"\"\"Return an opener that uses the given proxy and ignores SSL certs(!).\n      Proxy is of the form: http(s)://username:password@host:port\"\"\"\n      logger.warn('Ignoring SSL certificates')\n      try:\n          ctx = ssl.create_default_context()\n          ctx.check_hostname = False\n          ctx.verify_mode = ssl.CERT_NONE\n   \n          https = urllib2.HTTPSHandler(context=ctx)\n   \n      except AttributeError:\n          logger.debug('Python 2.6 does not support cert verification anyway')\n          https = urllib2.HTTPSHandler()\n\n          proxy = urllib2.ProxyHandler({'http': http_proxy,\n                                        'https': http_proxy})\n   \n      opener = urllib2.build_opener(proxy, https)\n   \n      return opener"
  },
  {
    "objectID": "proxy.html#ruby",
    "href": "proxy.html#ruby",
    "title": "Working with (around) proxies",
    "section": "Ruby",
    "text": "Ruby\nThis is basically a wrapper over Net::HTTP.start that pulls the proxy from the usual environment variable and disables SSL validation.\nMake sure that your Ruby is compiled with OpenSSL support. This definitely works in Ruby 2.3; as far as I can see it will work back to 1.8 but this hasn’t been tested.\n  require 'net/http'\n  require 'openssl'\n   \n  class CorporateProxy\n   \n    @@rgx = %r{\n              https?://  # scheme\n              (\\w.+)     # user\n              :(.+)      # pass\n              @([\\w.-]+) # host\n              :(\\d+)?    # port\n            }x\n   \n    def self.start(uri, opt, &block)\n      if not ENV.key?('http_proxy')\n        raise \"You'll need to define a proxy environment variable to continue.\"\n      end\n   \n      proxy = @@rgx.match(ENV['http_proxy']) do |match|\n        OpenStruct.new(:user =&gt; match[1],\n                       :pass =&gt; match[2],\n                       :addr =&gt; match[3],\n                       :port =&gt; match[4])\n      end\n   \n      Net::HTTP.start(uri.host, uri.port,\n                      proxy.addr, proxy.port,\n                      proxy.user, proxy.pass,\n                      opt.update(:verify_mode =&gt; OpenSSL::SSL::VERIFY_NONE),\n                      &block)\n    end\n  end"
  },
  {
    "objectID": "proxy.html#emacs-lisp",
    "href": "proxy.html#emacs-lisp",
    "title": "Working with (around) proxies",
    "section": "Emacs Lisp",
    "text": "Emacs Lisp\nThis works in Emacs 24.5 or newer. I’ve included an interactive function for setting the proxy password, which I’ve found to be quite convenient.\n  (defconst proxy-host \"example.com\")\n  (defconst proxy-port 1234)\n   \n  (defun make-proxy-url (host port &optional username password)\n    (concat\n     (when (or username password)\n       (format \"%s:%s@\"\n               (if (not username) \"\" username)\n               (if (not password) \"\" password)))\n     (format \"%s:%s\" host port)))\n   \n  (defun set-proxy (http-proxy)\n    \"Set proxy variables that Emacs uses from the provided HTTP-PROXY string.\"\n    (setenv \"HTTP_PROXY\" (format \"https://%s\" http-proxy))\n    (setq url-proxy-services (list (cons \"http\" http-proxy)\n                                   (cons \"https\" http-proxy))))\n   \n  (defun set-user-proxy ()\n    \"Set proxy using current user login name and asking for password.\"\n    (interactive)\n    (set-proxy (make-proxy-url proxy-host\n                               proxy-port\n                               (user-login-name)\n                               (read-passwd \"Password: \"))))"
  },
  {
    "objectID": "posts/f1-stan/f1.html",
    "href": "posts/f1-stan/f1.html",
    "title": "Latent skill of F1 drivers: fun with Bayesian models",
    "section": "",
    "text": "For a bit of fun, let’s try and model the performance of F1 drivers and constructors using Bayesian models. This is of course using data from the fantastic Ergast.\n\nlibrary(tidyverse)\n\nqualifying &lt;- read_csv(\"data/qualifying.csv\") |&gt; rename(quali_position = position)\nraces &lt;- read_csv(\"data/races.csv\") |&gt;\n  select(raceId, year, round, circuitId, name) |&gt;\n  rename(gp = name)\nresults &lt;- read_csv(\"data/results.csv\") |&gt; select(resultId, raceId, driverId, constructorId, grid, position, points, milliseconds)\nconstructors &lt;- read_csv(\"data/constructors.csv\") |&gt; select(constructorId, constructorRef)\ndrivers &lt;- read_csv(\"data/drivers.csv\") |&gt; select(driverId, driverRef)\n\nf1data &lt;- races |&gt;\n  merge(results) |&gt;\n  merge(qualifying) |&gt;\n  merge(constructors) |&gt;\n  merge(drivers) |&gt;\n  select(raceId, gp, year, round, constructorRef, driverRef, q1, q2, q3, quali_position, grid, position) |&gt;\n  mutate(position = ifelse(position == \"\\\\N\", NA, as.numeric(position)))\n\nMy model is an imitation of the f1-metrics model. We have latent variables of driver skill and car quality, which predict points. However I never did get hold of the paper for that, and am instead following the amazing Rasmus Bååth’s blog where he models the latent skill of football teams in La Liga.\nIn the interests of being educational - and because Lord knows I hate editing - I am writing this post as I go, including any blind-alleys and debugging.\n\nf2024 &lt;- f1data |&gt; filter(year == 2024)\n\nggplot(f2024) +\n  aes(x = position) +\n  facet_wrap(vars(driverRef)) +\n  geom_histogram(fill = \"dodgerblue4\", alpha = 0.5, colour = \"black\", linewidth = 0.1)\n\nggplot(f2024) +\n  aes(x = position) +\n  facet_wrap(vars(constructorRef)) +\n  geom_histogram(fill = \"dodgerblue4\", alpha = 0.5, colour = \"black\", linewidth = 0.1)\n\n\n\n\n\n\n\nFigure 1: Histograms of driver and constructor performance in F1 2024.\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Histograms of driver and constructor performance in F1 2024.\n\n\n\n\n\nPoints is simulated as a draw from a Poisson distribution parameterised by the driver performance and constructor performance.\n\\[\n\\text{points} \\sim \\text{Poisson}(\\text{perf}_{\\text{drv}} + \\text{perf}_{\\text{ctr}})\n\\]\nDriver performance is simulated as a draw from a normal distribution.\n\\[\n\\text{perf}_{\\text{drv}} \\sim \\text{Normal}(\\mu_{\\text{drv}}, \\sigma_{\\text{drv}}^2)\n\\]\nLikewise constructor performance is simulated as a draw from a normal distribution.\n\\[\n\\text{perf}_{\\text{ctr}} \\sim \\text{Normal}(\\mu_{\\text{ctr}}, \\sigma_{\\text{ctr}}^2)\n\\]\nWe will therefore have a likelihood function that is the product of the Poisson density of the points for each driver for each race. The number of parameters is very high, as a performance score for each driver and for each constructor.\n\nlog_likelihood &lt;- function(drv_race_pts, drv_perf, ctr_perf) {\n  ... # whew\n}\n\n\nCar performance\nLet’s start simpler, with a view of the car performance. Assume that at least one driver for every constructor maximised the car’s performance each weekend. THIS ISN’T TRUE. If Albon had a poor weekend, Sargeant wasn’t going to step up, as one of many examples on the grid in 2024. However it is a useful approximation.\n\nctr_positions &lt;- f2024 |&gt;\n  group_by(round, constructorRef) |&gt;\n  summarise(quali_position = min(quali_position), position = min(position)) |&gt;\n  mutate(\n    # re-rank\n    quali_position = rank(quali_position),\n    position = rank(position)\n  ) |&gt;\n  arrange(round, position)\n\nggplot(ctr_positions) +\n  aes(x = position) +\n  facet_wrap(vars(constructorRef)) +\n  geom_histogram(fill = \"dodgerblue4\", alpha = 0.5, colour = \"black\", linewidth = 0.1)\n\n\n\n\n\n\n\n\nA key point here is that we’ve recalculated positions using only the max driver’s position for each constructor, i.e. there are now only 10 finishing positions.\nAgain we can simulate the position \\(p\\) as a draw from a Poisson distribution, this time where \\(\\lambda_\\text{ctr}\\) is the performance of the constructor only.\n\\[\np \\sim \\text{Poisson}(\\lambda_{\\text{ctr}})\n\\]\nTo keep things simple, let’s start with an uninformative prior, a uniform distribution for the performance of the constructors.\n\\[\n\\lambda_{ctr} \\sim \\text{Uniform}(1, 10)\n\\]\nThen our likelihood function is much simpler. (We won’t actually need this, but I find it useful to write out for the sake of understanding.)\n\nlog_lik_ctr &lt;- function(ctr_positions, ctr_perfs) {\n  ctrs &lt;- sort(unique(ctr_positions$constructorRef))\n  log_lik &lt;- 1.0\n  for (i in 1:length(ctrs)) {\n    race_posns &lt;- filter(ctr_positions, constructorRef == ctrs[i])$position\n    log_lik &lt;- log_lik + sum(dpois(race_posns, lambda = ctr_perfs[i], log = TRUE))\n  }\n  log_lik\n}\n\nLet’s throw Stan at this now. Here’s our Stan model file.\ndata {\n  int&lt;lower=1&gt; n_ctrs;\n  int&lt;lower=1&gt; n_obs;\n  array[n_obs] int&lt;lower=1, upper=10&gt; ctrs;\n  array[n_obs] int&lt;lower=1, upper=10&gt; positions;\n}\nparameters {\n  array[n_ctrs] real&lt;lower=1, upper=10&gt; lambda;\n}\nmodel {\n  lambda ~ uniform(1, 10);\n  positions ~ normal(lambda[ctrs], 1) T[1, 10];\n\n  // the above \"distribution\" syntax is equivalent to:\n  //target += uniform_lpdf(lambda | 1, 10);\n  //target += poisson_lpmf(positions | lambda[ctrs]);\n}\n\nAnd here’s the R code for interacting with it via CmdStanR. I settled on CmdStanR rather than RStan after hitting too many nameless runtime errors through RStan - see here.\n\nlibrary(cmdstanr, quietly = TRUE)\ncheck_cmdstan_toolchain(fix = TRUE, quiet = TRUE)\n\nctrs &lt;- sort(unique(ctr_positions$constructorRef))\nctr_index &lt;- tibble(\n  constructorId = 1:length(ctrs),\n  constructorRef = ctrs,\n)\n\ndo.stan &lt;- function(ctr_positions) {\n  tidy_ctr_posns &lt;- ctr_positions |&gt; merge(ctr_index)\n\n  data_list &lt;- list(\n    n_ctrs = n_distinct(tidy_ctr_posns$constructorId),\n    n_obs = nrow(tidy_ctr_posns),\n    ctrs = tidy_ctr_posns$constructorId,\n    positions = tidy_ctr_posns$position\n  )\n  mod &lt;- cmdstan_model(\"f1.stan\", exe_file = \"f1.bin\")\n  mod$sample(data_list)\n}\n\nfit &lt;- do.stan(ctr_positions)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 1.1 seconds.\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 1.4 seconds.\nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 1.3 seconds.\nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 1.3 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 1.3 seconds.\nTotal execution time: 5.3 seconds.\n\nfit\n\n  variable    mean  median   sd  mad      q5     q95 rhat ess_bulk ess_tail\n lp__      -442.53 -442.13 2.46 2.26 -447.08 -439.25 1.00     1739     2298\n lambda[1]    6.96    6.96 0.21 0.21    6.63    7.30 1.00     4962     3071\n lambda[2]    5.96    5.96 0.21 0.21    5.61    6.30 1.00     4470     2732\n lambda[3]    2.58    2.59 0.23 0.22    2.21    2.95 1.00     4271     2157\n lambda[4]    6.29    6.29 0.21 0.20    5.96    6.64 1.00     4504     2586\n lambda[5]    1.56    1.56 0.26 0.28    1.14    2.01 1.00     2822     1546\n lambda[6]    4.12    4.12 0.21 0.21    3.77    4.46 1.00     4524     2638\n lambda[7]    6.67    6.68 0.21 0.21    6.33    7.02 1.00     4665     3164\n lambda[8]    3.65    3.65 0.21 0.22    3.30    3.99 1.00     4608     2405\n lambda[9]    8.37    8.36 0.23 0.23    7.99    8.76 1.00     4232     2670\n\n # showing 10 of 11 rows (change via 'max_rows' argument or 'cmdstanr_max_rows' option)\n\n\nLet’s tidy that up though, mapping parameters back to constructor names.\n\nplot_fit &lt;- function(fit) {\n  df &lt;- fit$draws(\"lambda\", format = \"draws_matrix\") |&gt; as_tibble()\n  colnames(df) &lt;- ctrs\n\n  ggplot(pivot_longer(df, all_of(ctrs))) +\n    aes(x = value) +\n    facet_wrap(vars(name)) +\n    geom_histogram(fill = \"dodgerblue4\", alpha = 0.5, colour = \"black\", linewidth = 0.1, binwidth=0.2)\n}\n\nplot_fit(fit)\n\n\n\n\n\n\n\nFigure 3: Posterior samples for constructor performance\n\n\n\n\n\nThe interesting thing is that this suggests the Red Bull was clearly the third-fastest car. I’m a little suspicious of that, because it claimed a sequence of 1-2s in the first third of the season. Let’s take a closer look at that.\nThe first thing to check is whether my dumb “max driver, 10 positions” model is problematic. Let’s look at Red Bull’s results.\n\n\n\nTable 1: Red Bull’s results in F1 2024\n\n\nf2024 |&gt;\n  filter(constructorRef == \"red_bull\") |&gt;\n  select(gp, round, driverRef, position) |&gt;\n  pivot_wider(names_from = \"driverRef\", values_from = \"position\") |&gt;\n  arrange(round) |&gt;\n  merge(\n    ctr_positions |&gt; filter(constructorRef == \"red_bull\") |&gt; select(round, position)\n  )\n\n   round                        gp perez max_verstappen position\n1      1        Bahrain Grand Prix     2              1        1\n2      2  Saudi Arabian Grand Prix     2              1        1\n3      3     Australian Grand Prix     5             NA       10\n4      4       Japanese Grand Prix     2              1        1\n5      5        Chinese Grand Prix     3              1        1\n6      6          Miami Grand Prix     4              2        2\n7      7 Emilia Romagna Grand Prix     8              1        1\n8      8         Monaco Grand Prix    NA              6       10\n9      9       Canadian Grand Prix    NA              1        9\n10    10        Spanish Grand Prix     8              1        1\n11    11       Austrian Grand Prix     7              5        4\n12    12        British Grand Prix    17              2        1\n13    13      Hungarian Grand Prix     7              5        4\n14    14        Belgian Grand Prix     7              4        3\n15    15          Dutch Grand Prix     6              2        2\n16    16        Italian Grand Prix     8              6        4\n17    17     Azerbaijan Grand Prix    17              5        4\n18    18      Singapore Grand Prix    10              2        2\n19    19  United States Grand Prix     7              3        2\n20    20    Mexico City Grand Prix    17              6        4\n21    21      São Paulo Grand Prix    11              1        1\n22    22      Las Vegas Grand Prix    10              5        3\n23    23          Qatar Grand Prix    NA              1        9\n24    24      Abu Dhabi Grand Prix    NA              6        8\n\n\n\n\nRound three is clearly a balls-up: how can Pere’z 5th place become the 10th constructor position? Let’s work through it again.\n\nf2024 |&gt;\n  group_by(round, constructorRef) |&gt;\n  summarise(position = min(position)) |&gt;\n  mutate(reranked = rank(position)) |&gt;\n  filter(round == 3)\n\n# A tibble: 10 × 4\n# Groups:   round [1]\n   round constructorRef position reranked\n   &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt;    &lt;dbl&gt;\n 1     3 alpine               13        7\n 2     3 aston_martin          6        3\n 3     3 ferrari               1        1\n 4     3 haas                  9        5\n 5     3 mclaren               3        2\n 6     3 mercedes             NA        9\n 7     3 rb                    7        4\n 8     3 red_bull             NA       10\n 9     3 sauber               14        8\n10     3 williams             11        6\n\n\nNA strikes again! Forgot to add the crucial na.rm parameter to the min. Right, a do-over.\n\nctr_positions &lt;- f2024 |&gt;\n  group_by(round, constructorRef) |&gt;\n  summarise(\n    # in the event of a double DNF, assign the last position\n    quali_position = pmin(min(quali_position, na.rm = TRUE), 10),\n    position = pmin(min(position, na.rm = TRUE), 10)\n  ) |&gt;\n  mutate(\n    # re-rank\n    quali_position = rank(quali_position),\n    position = rank(position)\n  ) |&gt;\n  arrange(round, position)\n\nggplot(ctr_positions) +\n  aes(x = position) +\n  facet_wrap(vars(constructorRef)) +\n  geom_histogram(fill = \"dodgerblue4\", alpha = 0.5, colour = \"black\", linewidth = 0.1)\n\n\n\n\n\n\n\nFigure 4: Constructor position histograms after fixing NA sorting.\n\n\n\n\n\nThat looks more like I’d expect. Let’s try Stan again.\n\nfit &lt;- do.stan(ctr_positions)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 1.0 seconds.\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 1.0 seconds.\nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 1.0 seconds.\nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 1.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 1.0 seconds.\nTotal execution time: 4.3 seconds.\n\nplot_fit(fit)\n\n\n\n\n\n\n\nFigure 5: Posterior samples after fixing NA sorting.\n\n\n\n\n\nThat is more intuitive: Red Bull is now much closer to McLaren and marginally ahead of Ferrari."
  },
  {
    "objectID": "posts/importance-sampling/index.html",
    "href": "posts/importance-sampling/index.html",
    "title": "Importance Sampling",
    "section": "",
    "text": "library(tidyverse)\nset.seed(42)\nThe aim of this post is to teach myself basic Bayesian methods. I’ll set up a toy example problem and solve it.\nLet’s assume we have a natural event that happens with a variable periodicity e.g. a woman’s menstrual cycle. I’m drawing from this excellent blog, but doing a much simpler model because I am but a baby.\nThe aim here is to see if we can find the mean period of the specific woman’s menstrual cycle given some samples and priors for the average cycle. First, priors from the blog linked above:\n# a normal distribution for the mean days between period starts\nprior_mean_mu &lt;- 27.7\nprior_mean_sd &lt;- 2.4\n# a half-normal distribution for the SD of days between period starts\nprior_sd_mu &lt;- 1.6\nprior_sd_sd &lt;- 2.05\nWe will set the true mean and SD (the params, \\(\\theta\\), that we’re looking for) and simulate some sample data, \\(d\\).\ntrue_mu &lt;- 26.3\ntrue_sd &lt;- 1.9\n\nsample_periods &lt;- rnorm(12, true_mu, true_sd)\nFor importance sampling we need a likelihood function, \\(L(d|\\theta)\\). Make the model a normal distribution and this is simply the probability density. We take the log of the likelihood because the numbers could be very small and subject to numerical error.\nlog_likelihood &lt;- function(sample_periods, mean_period, sd_period) {\n  sum(dnorm(sample_periods, mean_period, sd_period, log = TRUE))\n}\nA little sanity check: the likelihood of the true parameters should be greater than the likelihood of the priors.\nlog_likelihood(sample_periods, true_mu, true_sd) &gt; log_likelihood(sample_periods, prior_mean_mu, prior_mean_sd)\n\n[1] FALSE"
  },
  {
    "objectID": "posts/importance-sampling/index.html#importance-sampling-to-fit-the-model",
    "href": "posts/importance-sampling/index.html#importance-sampling-to-fit-the-model",
    "title": "Importance Sampling",
    "section": "Importance sampling to fit the model",
    "text": "Importance sampling to fit the model\n\nImportance sampling is a Monte Carlo method that is very easy to setup and that can work well if (1) the parameters space is small and (2) the priors are not too dissimilar from the posterior.\n\nI know that both these conditions are true in my toy example.\nFirst we must sample from the prior.\n\nn_samples &lt;- 100000\nprior &lt;- tibble(\n  mu_period = rnorm(n_samples, prior_mean_mu, prior_mean_sd),\n  sd_period = abs(rnorm(n_samples, 0, prior_sd_sd))\n)\n\nggplot(prior) +\n  aes(x = mu_period) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nggplot(prior) +\n  aes(x = sd_period) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThen we weight each draw by its likelihood.\n\nweights &lt;- sapply(1:n_samples, function(i) {\n  log_likelihood(sample_periods, prior$mu_period[i], prior$sd_period[i])\n})\n\nThen we resample the prior by this weighting.\n\n# n.b. sample will normalise the weights\nposterior &lt;- prior[sample(n_samples, replace = TRUE, prob = exp(weights)), ]\n\ndf &lt;- bind_rows(\n  mutate(prior, dist = \"prior\"),\n  mutate(posterior, dist = \"posterior\")\n) |&gt;\n  pivot_longer(c(mu_period, sd_period), names_to = \"param\", values_to = \"value\")\n\nggplot(df) +\n  aes(x = value, fill = dist) +\n  facet_wrap(vars(param), scales = \"free\") +\n  geom_histogram(alpha = 0.8, position = \"identity\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nHey Doctor, would you look at that! The posterior is much tighter and closer to the true values.\n\nposterior |&gt; summarise(mu = mean(mu_period), sd = mean(sd_period))\n\n# A tibble: 1 × 2\n     mu    sd\n  &lt;dbl&gt; &lt;dbl&gt;\n1  27.7  1.88\n\n\nHow does that compare to simply taking the mean and SD of the sample data?\n\ntibble(\n    mu = mean(sample_periods),\n    sd = sd(sample_periods)\n)\n\n# A tibble: 1 × 2\n     mu    sd\n  &lt;dbl&gt; &lt;dbl&gt;\n1  27.7  1.75\n\n\nIt’s not highly convincing in this case. I ran this a few times and it seemed that the SD estimation seemed to be closer to the true value than the SD of the sample, but the mean was rarely different. This is probably because it’s a very simple model and importance sampling is rather overkill."
  },
  {
    "objectID": "posts/importance-sampling/index.html#proposal-distribution-isnt-necessarily-the-prior",
    "href": "posts/importance-sampling/index.html#proposal-distribution-isnt-necessarily-the-prior",
    "title": "Importance Sampling",
    "section": "Proposal distribution isn’t necessarily the prior",
    "text": "Proposal distribution isn’t necessarily the prior\nIn Bååth’s other blog on importance sampling he explains that we are looking at the ratio of the likelihood of getting our sample from the target distribution relative to getting it from the proposal distribution (then normalised to form a new estimate of the target distribution).\nIf the proposal distribution is thinner at the ends than the target distribution, this is suboptimal because we will have fewer samples there. So a T distribution is a useful proposal for a normal target, because it has fatter tails (as controlled by the df degrees of freedom parameter).\nWe could choose a very sloppy proposal like a uniform distribution, but in that case we are suboptimal in terms of how quickly we explore the space. We would have a relatively high number of samples in the tails compared to the peak. Bååth has a neat method of using a Laplacian approximation as a proposal."
  },
  {
    "objectID": "posts/importance-sampling/index.html#aki-another-example",
    "href": "posts/importance-sampling/index.html#aki-another-example",
    "title": "Importance Sampling",
    "section": "Aki, another example",
    "text": "Aki, another example\nNow an example from this blog post. This one uses a T as the proposal.\n\napprox_samples &lt;- rt(100, df = 3)  # T with 3 deg free is our approximation to posterior\napprox_density &lt;- dt(approx_samples, df = 3, log = TRUE)  # get the prob density for each\nunnormalised_posterior &lt;- dnorm(approx_samples, log = TRUE, sd = sqrt(3))  # convert to actual posterior\n\n# weights are then...\nlog_imp_weights &lt;- unnormalised_posterior - approx_density\ndata.frame(log_imp_weights) %&gt;% \n  ggplot(aes(exp(log_imp_weights))) +\n  geom_density(fill = \"dodgerblue4\", alpha = 0.5) +\n  labs(title = \"Importance weights\",\n       subtitle = \"Approximating a normal with a t distribution\")\n\n\n\n\n\n\n\n# normalise...\nweights &lt;- exp(log_imp_weights) / sum(exp(log_imp_weights))\n\nmean_estimate &lt;- sum(approx_samples * weights)"
  },
  {
    "objectID": "posts/using-stan-from-r/index.html",
    "href": "posts/using-stan-from-r/index.html",
    "title": "Using Stan from R",
    "section": "",
    "text": "I’ve been trying to get the hang of Bayesian models with Stan. One of the hurdles has been using Stan from R, so in this post I’m jotting down what I’ve learned (mostly the hard way).\n\nRStan or CmdStanR?\nOn the whole I had a much better time using CmdStanR than RStan. When I made a mistake that led to a runtime exception, RStan would simply die with this kind of error:\nError in `unserialize()`:\n! error reading from connection\n     ▆\n  1. └─rstan::stan(...)\n  2.   ├─rstan::sampling(...)\n  3.   └─rstan::sampling(...)\n  4.     └─rstan (local) .local(object, ...)\n  5.       └─parallel::parLapplyLB(cl, X = 1:chains, fun = callFun)\n  6.         ├─base::do.call(...)\n  7.         └─parallel::clusterApplyLB(...)\n  8.           └─parallel:::dynamicClusterApply(cl, fun, length(x), argfun)\n  9.             └─parallel:::recvOneResult(cl)\n 10.               ├─parallel::recvOneData(cl)\n 11.               └─parallel:::recvOneData.SOCKcluster(cl)\n 12.                 └─base::unserialize(socklist[[n]])\nCmdStanR on the other hand would print something useful. Other aspects of the development experience were also nicer:\n\nbetter status updates (e.g. showing when compiling)\neditor support for stan files e.g. linting\n\n\n\nCommon Errors\n\nI got “Rejecting initial value” a lot: “log probability evaluates to log(0), i.e. negative infinity”. This is trying to tell you that the default initial value chosen by Stan has a probability of zero in your prior. You need to set constraints in the parameters block that match the prior distribution you choose. For example if you chose a uniform(1, 10) prior, you should add the constraint &lt;lower=1, upper=10&gt; to your parameter declaration."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Latent skill of F1 drivers: fun with Bayesian models\n\n\n\n\n\n\nbayesian\n\n\nr\n\n\nf1\n\n\n\n\n\n\n\n\n\nDec 27, 2024\n\n\nChris Bowdon\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Stan from R\n\n\n\n\n\n\nbayesian\n\n\nr\n\n\nstan\n\n\n\n\n\n\n\n\n\nDec 26, 2024\n\n\nChris Bowdon\n\n\n\n\n\n\n\n\n\n\n\n\nImportance Sampling\n\n\n\n\n\n\nbayesian\n\n\nr\n\n\n\n\n\n\n\n\n\nDec 23, 2024\n\n\nChris Bowdon\n\n\n\n\n\n\nNo matching items"
  }
]