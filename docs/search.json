[
  {
    "objectID": "proxy.html",
    "href": "proxy.html",
    "title": "Working with (around) proxies",
    "section": "",
    "text": "N.B. This article is ten years old and certainly does not represent best practices in the 2020s.\nThis is something that I seem to have to do on a monthly basis on various projects: a throwaway script that includes getting some resource from the Internet. This means I have to spar with my old nemesis the corporate proxy and her minions, the 407 and the SSL verification error.\n$DAYJOB uses an authenticated proxy and their own root certificate. It’s seamless for Office and IE users because their system is pre-configured. But not for those of us who are spinning up VMs and installing packages and come to think of it almost everything you need to do as a software engineer. (It also can /weaken/ rather than enhance security, the stated aim, but that’s an article for another day.)\nThis is a quick guide-by-example for how to authenticated against a proxy and ignore any SSL verification errors in a variety of scripting languages. The examples use only the standard library for a widely available (read: old) version of each language. The assumption is that if you can’t reach the Internet, you don’t have better packages or newer versions.\nIt is not an example of how to write good production software. It is a catalogue of dirty workarounds. You have been warned."
  },
  {
    "objectID": "proxy.html#bash",
    "href": "proxy.html#bash",
    "title": "Working with (around) proxies",
    "section": "Bash",
    "text": "Bash\nMany applications respect the =$http_proxy= environmental argument, so it’s worth setting this in your =.bash_profile=. Those that don’t often take command line flags for a proxy and to ignore SSL verification.\n  http_proxy=http://username:password@host:port\n  export $http_proxy\n\n  curl --proxy $http_proxy --insecure --url ... \n\n  git -c http.proxy=$http_proxy -c http.sslVerify=false clone ..."
  },
  {
    "objectID": "proxy.html#python-2",
    "href": "proxy.html#python-2",
    "title": "Working with (around) proxies",
    "section": "Python 2",
    "text": "Python 2\nThis works in Python 2.6 and 2.7. In Python 3 the principles are the same but urllib has been reorganized.\n\n  import logging\n  import ssl\n  import urllib2\n\n  logger = logging.getLogger('demo')\n   \n  def proxy_opener(http_proxy):\n      \"\"\"Return an opener that uses the given proxy and ignores SSL certs(!).\n      Proxy is of the form: http(s)://username:password@host:port\"\"\"\n      logger.warn('Ignoring SSL certificates')\n      try:\n          ctx = ssl.create_default_context()\n          ctx.check_hostname = False\n          ctx.verify_mode = ssl.CERT_NONE\n   \n          https = urllib2.HTTPSHandler(context=ctx)\n   \n      except AttributeError:\n          logger.debug('Python 2.6 does not support cert verification anyway')\n          https = urllib2.HTTPSHandler()\n\n          proxy = urllib2.ProxyHandler({'http': http_proxy,\n                                        'https': http_proxy})\n   \n      opener = urllib2.build_opener(proxy, https)\n   \n      return opener"
  },
  {
    "objectID": "proxy.html#ruby",
    "href": "proxy.html#ruby",
    "title": "Working with (around) proxies",
    "section": "Ruby",
    "text": "Ruby\nThis is basically a wrapper over Net::HTTP.start that pulls the proxy from the usual environment variable and disables SSL validation.\nMake sure that your Ruby is compiled with OpenSSL support. This definitely works in Ruby 2.3; as far as I can see it will work back to 1.8 but this hasn’t been tested.\n  require 'net/http'\n  require 'openssl'\n   \n  class CorporateProxy\n   \n    @@rgx = %r{\n              https?://  # scheme\n              (\\w.+)     # user\n              :(.+)      # pass\n              @([\\w.-]+) # host\n              :(\\d+)?    # port\n            }x\n   \n    def self.start(uri, opt, &block)\n      if not ENV.key?('http_proxy')\n        raise \"You'll need to define a proxy environment variable to continue.\"\n      end\n   \n      proxy = @@rgx.match(ENV['http_proxy']) do |match|\n        OpenStruct.new(:user =&gt; match[1],\n                       :pass =&gt; match[2],\n                       :addr =&gt; match[3],\n                       :port =&gt; match[4])\n      end\n   \n      Net::HTTP.start(uri.host, uri.port,\n                      proxy.addr, proxy.port,\n                      proxy.user, proxy.pass,\n                      opt.update(:verify_mode =&gt; OpenSSL::SSL::VERIFY_NONE),\n                      &block)\n    end\n  end"
  },
  {
    "objectID": "proxy.html#emacs-lisp",
    "href": "proxy.html#emacs-lisp",
    "title": "Working with (around) proxies",
    "section": "Emacs Lisp",
    "text": "Emacs Lisp\nThis works in Emacs 24.5 or newer. I’ve included an interactive function for setting the proxy password, which I’ve found to be quite convenient.\n  (defconst proxy-host \"example.com\")\n  (defconst proxy-port 1234)\n   \n  (defun make-proxy-url (host port &optional username password)\n    (concat\n     (when (or username password)\n       (format \"%s:%s@\"\n               (if (not username) \"\" username)\n               (if (not password) \"\" password)))\n     (format \"%s:%s\" host port)))\n   \n  (defun set-proxy (http-proxy)\n    \"Set proxy variables that Emacs uses from the provided HTTP-PROXY string.\"\n    (setenv \"HTTP_PROXY\" (format \"https://%s\" http-proxy))\n    (setq url-proxy-services (list (cons \"http\" http-proxy)\n                                   (cons \"https\" http-proxy))))\n   \n  (defun set-user-proxy ()\n    \"Set proxy using current user login name and asking for password.\"\n    (interactive)\n    (set-proxy (make-proxy-url proxy-host\n                               proxy-port\n                               (user-login-name)\n                               (read-passwd \"Password: \"))))"
  },
  {
    "objectID": "posts/f1-stan/f1.html",
    "href": "posts/f1-stan/f1.html",
    "title": "Latent performance of F1 cars and drivers: fun with Bayesian models",
    "section": "",
    "text": "For a bit of fun, let’s try and model the performance of F1 drivers and constructors using Bayesian models. This is of course using data from the fantastic Ergast.\nMy model will be an imitation of the f1-metrics model. We have latent variables of driver skill and car quality, which predict points/positions. However I never did get hold of the paper for that, and am instead following the amazing Rasmus Bååth’s blog where he models the latent skill of football teams in La Liga.\nI downloaded the CSVs from Ergast which we’ll first wrangle into shape. The weapon of choice is R and the tidyverse, since nothing else is quite so ergonomic for this type of analysis.\nlibrary(tidyverse)\n\nqualifying &lt;- read_csv(\"data/qualifying.csv\") |&gt; rename(quali_position = position)\nraces &lt;- read_csv(\"data/races.csv\") |&gt;\n  select(raceId, year, round, circuitId, name) |&gt;\n  rename(gp = name)\nresults &lt;- read_csv(\"data/results.csv\") |&gt; select(resultId, raceId, driverId, constructorId, grid, position, points, milliseconds)\nconstructors &lt;- read_csv(\"data/constructors.csv\") |&gt; select(constructorId, constructorRef)\ndrivers &lt;- read_csv(\"data/drivers.csv\") |&gt; select(driverId, driverRef)\n\nf1data &lt;- races |&gt;\n  merge(results) |&gt;\n  merge(qualifying) |&gt;\n  merge(constructors) |&gt;\n  merge(drivers) |&gt;\n  select(raceId, gp, year, round, constructorRef, driverRef, q1, q2, q3, quali_position, grid, position) |&gt;\n  mutate(position = ifelse(position == \"\\\\N\", NA, as.numeric(position)))\nIn the interests of being educational - and because Lord knows I hate editing - I am writing this post as I go, including any blind-alleys and debugging.\nf2024 &lt;- f1data |&gt; filter(year == 2024)\n\nggplot(f2024) +\n  aes(x = position) +\n  facet_wrap(vars(driverRef)) +\n  geom_histogram(fill = \"dodgerblue4\", alpha = 0.5, colour = \"black\", linewidth = 0.1, binwidth = 1)\n\nggplot(f2024) +\n  aes(x = position) +\n  facet_wrap(vars(constructorRef)) +\n  geom_histogram(fill = \"dodgerblue4\", alpha = 0.5, colour = \"black\", linewidth = 0.1, binwidth = 1)\n\n\n\n\n\n\n\nFigure 1: Histograms of driver and constructor performance in F1 2024.\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Histograms of driver and constructor performance in F1 2024.\nThe model goes like this. Points are simulated as a draw from a Poisson distribution parameterised by the driver performance and constructor performance.\n\\[\n\\text{points} \\sim \\text{Poisson}(\\text{perf}_{\\text{drv}} + \\text{perf}_{\\text{ctr}})\n\\]\nDriver performance is simulated as a draw from a normal distribution.\n\\[\n\\text{perf}_{\\text{drv}} \\sim \\text{Normal}(\\mu_{\\text{drv}}, \\sigma_{\\text{drv}}^2)\n\\]\nLikewise constructor performance is simulated as a draw from a normal distribution.\n\\[\n\\text{perf}_{\\text{ctr}} \\sim \\text{Normal}(\\mu_{\\text{ctr}}, \\sigma_{\\text{ctr}}^2)\n\\]\nWe will therefore have a likelihood function that is the product of the Poisson density of the points for each driver for each race. The number of parameters is very high, as a performance score for each driver and for each constructor.\nlog_likelihood &lt;- function(drv_race_pts, drv_perf, ctr_perf) {\n  ... # whew\n}"
  },
  {
    "objectID": "posts/f1-stan/f1.html#budget-data",
    "href": "posts/f1-stan/f1.html#budget-data",
    "title": "Latent performance of F1 cars and drivers: fun with Bayesian models",
    "section": "Budget data",
    "text": "Budget data\nThis is a bit of a secret, so we have to estimate. We do know that the budget cap is 135M USD and it’s unlikely that anyone is operating below the cap. We also know that driver and top executive salaries are exempt from the cap. The top teams are spending about 100M USD on their drivers, and prior to the budget cap’s implementation likely invested in facilities that they can use for free (like wind tunnels).\nWith that in mind, I asked 4o to search the web and estimate budgets for the teams. I sanity checked against this Blackbook Motorsport article.\nThese are probably reasonable estimates, and if we put a reasonable variance on the priors it will reflect the uncertainty.\n\nbudgets &lt;- tribble(\n  ~constructorRef, ~budget,\n  \"red_bull\", 400,\n  \"mercedes\", 400,\n  \"ferrari\", 400,\n  \"mclaren\", 250,\n  \"aston_martin\", 250,\n  \"alpine\", 200,\n  \"rb\", 150,\n  \"haas\", 150,\n  \"williams\", 150,\n  \"sauber\", 150\n) |&gt;\n  mutate(\n    norm_budget = 1 - budget / max(budget)\n  )\nknitr::kable(budgets)\n\n\n\nTable 2: Constructor budget estimates (USD millions)\n\n\n\n\n\n\nconstructorRef\nbudget\nnorm_budget\n\n\n\n\nred_bull\n400\n0.000\n\n\nmercedes\n400\n0.000\n\n\nferrari\n400\n0.000\n\n\nmclaren\n250\n0.375\n\n\naston_martin\n250\n0.375\n\n\nalpine\n200\n0.500\n\n\nrb\n150\n0.625\n\n\nhaas\n150\n0.625\n\n\nwilliams\n150\n0.625\n\n\nsauber\n150\n0.625\n\n\n\n\n\n\n\n\nLet’s update our model and run Stan again.\ndata {\n  int&lt;lower=1&gt; n_ctrs;\n  int&lt;lower=1&gt; n_obs;\n  array[n_ctrs] int ctr_budgets;\n  real&lt;lower=0&gt; ctr_budget_sd;\n  array[n_obs] int&lt;lower=1, upper=10&gt; ctrs;\n  array[n_obs] int&lt;lower=1, upper=10&gt; positions;\n}\nparameters {\n  array[n_ctrs] real&lt;lower=1, upper=10&gt; lambda;\n}\nmodel {\n  lambda ~ normal(ctr_budgets, ctr_budget_sd);\n  positions ~ normal(lambda[ctrs], 1) T[1, 10];\n}\n\n\ndata_list_b &lt;- make_data_list(ctr_positions)\ndata_list_b$ctr_budgets &lt;- 10 * arrange(budgets, constructorRef)$norm_budget\ndata_list_b$ctr_budget_sd &lt;- 1.5\nfit.b &lt;- run_stan(data_list_b, model_file = \"f1.b.stan\")\n\nplot_posterior(fit.b)\n\n\n\n\n\n\n\nFigure 7: Posterior samples from the model with budget-based priors.\n\n\n\n\n\nThis hasn’t made much difference. McLaren has eased back towards Red Bull because their budget is smaller. You could say they did a lot better than Red Bull relative to their budget.\n\nsp &lt;- sample_posterior(fit.b)\nsp |&gt;\n  group_by(constructorRef) |&gt;\n  summarise(mu = mean(position), sigma = sd(position)) |&gt;\n  arrange(mu) |&gt;\n  knitr::kable()\n\n\n\nTable 3: Mean and SD from posterior sample.\n\n\n\n\n\n\nconstructorRef\nmu\nsigma\n\n\n\n\nmclaren\n1.953993\n0.2508664\n\n\nred_bull\n2.058912\n0.2584320\n\n\nferrari\n2.469770\n0.2310382\n\n\nmercedes\n3.146093\n0.2072577\n\n\naston_martin\n6.232057\n0.1974960\n\n\nalpine\n7.094156\n0.2121044\n\n\nhaas\n7.110799\n0.1993153\n\n\nrb\n7.152165\n0.2054361\n\n\nwilliams\n7.755385\n0.2113910\n\n\nsauber\n8.071865\n0.2184111\n\n\n\n\n\n\n\n\nThis model suggests there’s very little nothing in it between Red Bull and McLaren, though the posterior distribution is wider for the top three teams than the others so there is more uncertainty.\nWas budget a bad choice for the prior? Not necessarily: this model of constructor performance incorporates an important factor that isn’t clear from the results, but which should affect an objective assessment of which teams are better. Or to put it another way, if I were Max Verstappen I’d still rather be driving the Red Bull than the McLaren next year, because Red Bull’s budget suggests a more capable team overall. (Not to mention the increased wind tunnel time that third place in the constructors’ gets relative to the winner.)\n\nplot_posterior_with_prior &lt;- function(ctrs, ctr_budgets, ctr_budget_sd, fit) {\n  s_post &lt;- sample_posterior(fit) |&gt; mutate(sample = \"posterior\")\n\n  ctr_index &lt;- tibble(constructorRef = ctrs, constructorId = 1:length(ctrs))\n\n  s_prior &lt;- ctr_budgets |&gt;\n    lapply(\\(mu) rnorm(100000, mu, ctr_budget_sd)) |&gt;\n    lapply(as_tibble) |&gt;\n    imap(\\(t, i) mutate(t, constructorId = i, sample = \"prior\")) |&gt;\n    bind_rows() |&gt;\n    rename(position = value) |&gt;\n    merge(ctr_index)\n\n  df &lt;- bind_rows(s_prior, s_post)\n\n  ggplot(df) +\n    aes(x = position, fill = sample) +\n    facet_wrap(vars(constructorRef)) +\n    geom_density(alpha = 0.5, colour = \"black\", linewidth = 0.1) +\n    scale_x_continuous(breaks=seq(1, 10, by=2), limits=c(1, 10))\n}\n\nplot_posterior_with_prior(ctrs, data_list_b$ctr_budgets, data_list_b$ctr_budget_sd, fit.b)\n\n\n\n\n\n\n\nFigure 8: Comparison of prior and posterior samples.\n\n\n\n\n\nPlotting the priors and posteriors together shows how McLaren over-performed and Mercedes continued to underperform.\nMore on this later."
  },
  {
    "objectID": "posts/importance-sampling/index.html",
    "href": "posts/importance-sampling/index.html",
    "title": "Importance Sampling",
    "section": "",
    "text": "library(tidyverse)\nset.seed(42)\nThe aim of this post is to teach myself basic Bayesian methods. I’ll set up a toy example problem and solve it.\nLet’s assume we have a natural event that happens with a variable periodicity e.g. a woman’s menstrual cycle. I’m drawing from this excellent blog, but doing a much simpler model because I am but a baby.\nThe aim here is to see if we can find the mean period of the specific woman’s menstrual cycle given some samples and priors for the average cycle. First, priors from the blog linked above:\n# a normal distribution for the mean days between period starts\nprior_mean_mu &lt;- 27.7\nprior_mean_sd &lt;- 2.4\n# a half-normal distribution for the SD of days between period starts\nprior_sd_mu &lt;- 1.6\nprior_sd_sd &lt;- 2.05\nWe will set the true mean and SD (the params, \\(\\theta\\), that we’re looking for) and simulate some sample data, \\(d\\).\ntrue_mu &lt;- 26.3\ntrue_sd &lt;- 1.9\n\nsample_periods &lt;- rnorm(12, true_mu, true_sd)\nFor importance sampling we need a likelihood function, \\(L(d|\\theta)\\). Make the model a normal distribution and this is simply the probability density. We take the log of the likelihood because the numbers could be very small and subject to numerical error.\nlog_likelihood &lt;- function(sample_periods, mean_period, sd_period) {\n  sum(dnorm(sample_periods, mean_period, sd_period, log = TRUE))\n}\nA little sanity check: the likelihood of the true parameters should be greater than the likelihood of the priors.\nlog_likelihood(sample_periods, true_mu, true_sd) &gt; log_likelihood(sample_periods, prior_mean_mu, prior_mean_sd)\n\n[1] FALSE"
  },
  {
    "objectID": "posts/importance-sampling/index.html#importance-sampling-to-fit-the-model",
    "href": "posts/importance-sampling/index.html#importance-sampling-to-fit-the-model",
    "title": "Importance Sampling",
    "section": "Importance sampling to fit the model",
    "text": "Importance sampling to fit the model\n\nImportance sampling is a Monte Carlo method that is very easy to setup and that can work well if (1) the parameters space is small and (2) the priors are not too dissimilar from the posterior.\n\nI know that both these conditions are true in my toy example.\nFirst we must sample from the prior.\n\nn_samples &lt;- 100000\nprior &lt;- tibble(\n  mu_period = rnorm(n_samples, prior_mean_mu, prior_mean_sd),\n  sd_period = abs(rnorm(n_samples, 0, prior_sd_sd))\n)\n\nggplot(prior) +\n  aes(x = mu_period) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nggplot(prior) +\n  aes(x = sd_period) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThen we weight each draw by its likelihood.\n\nweights &lt;- sapply(1:n_samples, function(i) {\n  log_likelihood(sample_periods, prior$mu_period[i], prior$sd_period[i])\n})\n\nThen we resample the prior by this weighting.\n\n# n.b. sample will normalise the weights\nposterior &lt;- prior[sample(n_samples, replace = TRUE, prob = exp(weights)), ]\n\ndf &lt;- bind_rows(\n  mutate(prior, dist = \"prior\"),\n  mutate(posterior, dist = \"posterior\")\n) |&gt;\n  pivot_longer(c(mu_period, sd_period), names_to = \"param\", values_to = \"value\")\n\nggplot(df) +\n  aes(x = value, fill = dist) +\n  facet_wrap(vars(param), scales = \"free\") +\n  geom_histogram(alpha = 0.8, position = \"identity\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nHey Doctor, would you look at that! The posterior is much tighter and closer to the true values.\n\nposterior |&gt; summarise(mu = mean(mu_period), sd = mean(sd_period))\n\n# A tibble: 1 × 2\n     mu    sd\n  &lt;dbl&gt; &lt;dbl&gt;\n1  27.7  1.88\n\n\nHow does that compare to simply taking the mean and SD of the sample data?\n\ntibble(\n    mu = mean(sample_periods),\n    sd = sd(sample_periods)\n)\n\n# A tibble: 1 × 2\n     mu    sd\n  &lt;dbl&gt; &lt;dbl&gt;\n1  27.7  1.75\n\n\nIt’s not highly convincing in this case. I ran this a few times and it seemed that the SD estimation seemed to be closer to the true value than the SD of the sample, but the mean was rarely different. This is probably because it’s a very simple model and importance sampling is rather overkill."
  },
  {
    "objectID": "posts/importance-sampling/index.html#proposal-distribution-isnt-necessarily-the-prior",
    "href": "posts/importance-sampling/index.html#proposal-distribution-isnt-necessarily-the-prior",
    "title": "Importance Sampling",
    "section": "Proposal distribution isn’t necessarily the prior",
    "text": "Proposal distribution isn’t necessarily the prior\nIn Bååth’s other blog on importance sampling he explains that we are looking at the ratio of the likelihood of getting our sample from the target distribution relative to getting it from the proposal distribution (then normalised to form a new estimate of the target distribution).\nIf the proposal distribution is thinner at the ends than the target distribution, this is suboptimal because we will have fewer samples there. So a T distribution is a useful proposal for a normal target, because it has fatter tails (as controlled by the df degrees of freedom parameter).\nWe could choose a very sloppy proposal like a uniform distribution, but in that case we are suboptimal in terms of how quickly we explore the space. We would have a relatively high number of samples in the tails compared to the peak. Bååth has a neat method of using a Laplacian approximation as a proposal."
  },
  {
    "objectID": "posts/importance-sampling/index.html#aki-another-example",
    "href": "posts/importance-sampling/index.html#aki-another-example",
    "title": "Importance Sampling",
    "section": "Aki, another example",
    "text": "Aki, another example\nNow an example from this blog post. This one uses a T as the proposal.\n\napprox_samples &lt;- rt(100, df = 3)  # T with 3 deg free is our approximation to posterior\napprox_density &lt;- dt(approx_samples, df = 3, log = TRUE)  # get the prob density for each\nunnormalised_posterior &lt;- dnorm(approx_samples, log = TRUE, sd = sqrt(3))  # convert to actual posterior\n\n# weights are then...\nlog_imp_weights &lt;- unnormalised_posterior - approx_density\ndata.frame(log_imp_weights) %&gt;% \n  ggplot(aes(exp(log_imp_weights))) +\n  geom_density(fill = \"dodgerblue4\", alpha = 0.5) +\n  labs(title = \"Importance weights\",\n       subtitle = \"Approximating a normal with a t distribution\")\n\n\n\n\n\n\n\n# normalise...\nweights &lt;- exp(log_imp_weights) / sum(exp(log_imp_weights))\n\nmean_estimate &lt;- sum(approx_samples * weights)"
  },
  {
    "objectID": "posts/using-stan-from-r/index.html",
    "href": "posts/using-stan-from-r/index.html",
    "title": "Using Stan from R",
    "section": "",
    "text": "I’ve been trying to get the hang of Bayesian models with Stan. One of the hurdles has been using Stan from R, so in this post I’m jotting down what I’ve learned (mostly the hard way).\n\nRStan or CmdStanR?\nOn the whole I had a much better time using CmdStanR than RStan. When I made a mistake that led to a runtime exception, RStan would simply die with this kind of error:\nError in `unserialize()`:\n! error reading from connection\n     ▆\n  1. └─rstan::stan(...)\n  2.   ├─rstan::sampling(...)\n  3.   └─rstan::sampling(...)\n  4.     └─rstan (local) .local(object, ...)\n  5.       └─parallel::parLapplyLB(cl, X = 1:chains, fun = callFun)\n  6.         ├─base::do.call(...)\n  7.         └─parallel::clusterApplyLB(...)\n  8.           └─parallel:::dynamicClusterApply(cl, fun, length(x), argfun)\n  9.             └─parallel:::recvOneResult(cl)\n 10.               ├─parallel::recvOneData(cl)\n 11.               └─parallel:::recvOneData.SOCKcluster(cl)\n 12.                 └─base::unserialize(socklist[[n]])\nCmdStanR on the other hand would print something useful. Other aspects of the development experience were also nicer:\n\nbetter status updates (e.g. showing when compiling)\neditor support for stan files e.g. linting\n\n\n\nCommon Errors\n\nI got “Rejecting initial value” a lot: “log probability evaluates to log(0), i.e. negative infinity”. This is trying to tell you that the default initial value chosen by Stan has a probability of zero in your prior. You need to set constraints in the parameters block that match the prior distribution you choose. For example if you chose a uniform(1, 10) prior, you should add the constraint &lt;lower=1, upper=10&gt; to your parameter declaration."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Latent performance of F1 cars and drivers: fun with Bayesian models\n\n\n\n\n\n\nbayesian\n\n\nr\n\n\nf1\n\n\n\n\n\n\n\n\n\nDec 27, 2024\n\n\nChris Bowdon\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Stan from R\n\n\n\n\n\n\nbayesian\n\n\nr\n\n\nstan\n\n\n\n\n\n\n\n\n\nDec 26, 2024\n\n\nChris Bowdon\n\n\n\n\n\n\n\n\n\n\n\n\nImportance Sampling\n\n\n\n\n\n\nbayesian\n\n\nr\n\n\n\n\n\n\n\n\n\nDec 23, 2024\n\n\nChris Bowdon\n\n\n\n\n\n\nNo matching items"
  }
]