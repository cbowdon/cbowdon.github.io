[
  {
    "objectID": "proxy.html",
    "href": "proxy.html",
    "title": "Working with (around) proxies",
    "section": "",
    "text": "N.B. This article is ten years old and certainly does not represent best practices in the 2020s.\nThis is something that I seem to have to do on a monthly basis on various projects: a throwaway script that includes getting some resource from the Internet. This means I have to spar with my old nemesis the corporate proxy and her minions, the 407 and the SSL verification error.\n$DAYJOB uses an authenticated proxy and their own root certificate. It’s seamless for Office and IE users because their system is pre-configured. But not for those of us who are spinning up VMs and installing packages and come to think of it almost everything you need to do as a software engineer. (It also can /weaken/ rather than enhance security, the stated aim, but that’s an article for another day.)\nThis is a quick guide-by-example for how to authenticated against a proxy and ignore any SSL verification errors in a variety of scripting languages. The examples use only the standard library for a widely available (read: old) version of each language. The assumption is that if you can’t reach the Internet, you don’t have better packages or newer versions.\nIt is not an example of how to write good production software. It is a catalogue of dirty workarounds. You have been warned."
  },
  {
    "objectID": "proxy.html#bash",
    "href": "proxy.html#bash",
    "title": "Working with (around) proxies",
    "section": "Bash",
    "text": "Bash\nMany applications respect the =$http_proxy= environmental argument, so it’s worth setting this in your =.bash_profile=. Those that don’t often take command line flags for a proxy and to ignore SSL verification.\n  http_proxy=http://username:password@host:port\n  export $http_proxy\n\n  curl --proxy $http_proxy --insecure --url ... \n\n  git -c http.proxy=$http_proxy -c http.sslVerify=false clone ..."
  },
  {
    "objectID": "proxy.html#python-2",
    "href": "proxy.html#python-2",
    "title": "Working with (around) proxies",
    "section": "Python 2",
    "text": "Python 2\nThis works in Python 2.6 and 2.7. In Python 3 the principles are the same but urllib has been reorganized.\n\n  import logging\n  import ssl\n  import urllib2\n\n  logger = logging.getLogger('demo')\n   \n  def proxy_opener(http_proxy):\n      \"\"\"Return an opener that uses the given proxy and ignores SSL certs(!).\n      Proxy is of the form: http(s)://username:password@host:port\"\"\"\n      logger.warn('Ignoring SSL certificates')\n      try:\n          ctx = ssl.create_default_context()\n          ctx.check_hostname = False\n          ctx.verify_mode = ssl.CERT_NONE\n   \n          https = urllib2.HTTPSHandler(context=ctx)\n   \n      except AttributeError:\n          logger.debug('Python 2.6 does not support cert verification anyway')\n          https = urllib2.HTTPSHandler()\n\n          proxy = urllib2.ProxyHandler({'http': http_proxy,\n                                        'https': http_proxy})\n   \n      opener = urllib2.build_opener(proxy, https)\n   \n      return opener"
  },
  {
    "objectID": "proxy.html#ruby",
    "href": "proxy.html#ruby",
    "title": "Working with (around) proxies",
    "section": "Ruby",
    "text": "Ruby\nThis is basically a wrapper over Net::HTTP.start that pulls the proxy from the usual environment variable and disables SSL validation.\nMake sure that your Ruby is compiled with OpenSSL support. This definitely works in Ruby 2.3; as far as I can see it will work back to 1.8 but this hasn’t been tested.\n  require 'net/http'\n  require 'openssl'\n   \n  class CorporateProxy\n   \n    @@rgx = %r{\n              https?://  # scheme\n              (\\w.+)     # user\n              :(.+)      # pass\n              @([\\w.-]+) # host\n              :(\\d+)?    # port\n            }x\n   \n    def self.start(uri, opt, &block)\n      if not ENV.key?('http_proxy')\n        raise \"You'll need to define a proxy environment variable to continue.\"\n      end\n   \n      proxy = @@rgx.match(ENV['http_proxy']) do |match|\n        OpenStruct.new(:user =&gt; match[1],\n                       :pass =&gt; match[2],\n                       :addr =&gt; match[3],\n                       :port =&gt; match[4])\n      end\n   \n      Net::HTTP.start(uri.host, uri.port,\n                      proxy.addr, proxy.port,\n                      proxy.user, proxy.pass,\n                      opt.update(:verify_mode =&gt; OpenSSL::SSL::VERIFY_NONE),\n                      &block)\n    end\n  end"
  },
  {
    "objectID": "proxy.html#emacs-lisp",
    "href": "proxy.html#emacs-lisp",
    "title": "Working with (around) proxies",
    "section": "Emacs Lisp",
    "text": "Emacs Lisp\nThis works in Emacs 24.5 or newer. I’ve included an interactive function for setting the proxy password, which I’ve found to be quite convenient.\n  (defconst proxy-host \"example.com\")\n  (defconst proxy-port 1234)\n   \n  (defun make-proxy-url (host port &optional username password)\n    (concat\n     (when (or username password)\n       (format \"%s:%s@\"\n               (if (not username) \"\" username)\n               (if (not password) \"\" password)))\n     (format \"%s:%s\" host port)))\n   \n  (defun set-proxy (http-proxy)\n    \"Set proxy variables that Emacs uses from the provided HTTP-PROXY string.\"\n    (setenv \"HTTP_PROXY\" (format \"https://%s\" http-proxy))\n    (setq url-proxy-services (list (cons \"http\" http-proxy)\n                                   (cons \"https\" http-proxy))))\n   \n  (defun set-user-proxy ()\n    \"Set proxy using current user login name and asking for password.\"\n    (interactive)\n    (set-proxy (make-proxy-url proxy-host\n                               proxy-port\n                               (user-login-name)\n                               (read-passwd \"Password: \"))))"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "T. rex size distributions\n\n\n\n\n\n\ndinosaurs\n\n\nr\n\n\n\n\n\n\n\n\n\nFeb 15, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nGaussian Processes in R\n\n\n\n\n\n\ngaussian\n\n\nbayesian\n\n\nstan\n\n\nr\n\n\n\n\n\n\n\n\n\nJan 25, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCan we predict who wins the Traitors with an R simulation?\n\n\n\n\n\n\ntraitors\n\n\nmonte carlo\n\n\nr\n\n\n\n\n\n\n\n\n\nJan 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nLatent F1 team and driver performance: fun with Bayesian models\n\n\n\n\n\n\nbayesian\n\n\nr\n\n\nf1\n\n\nstan\n\n\n\n\n\n\n\n\n\nDec 27, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Stan from R\n\n\n\n\n\n\nbayesian\n\n\nr\n\n\nstan\n\n\n\n\n\n\n\n\n\nDec 26, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nImportance Sampling\n\n\n\n\n\n\nbayesian\n\n\nr\n\n\n\n\n\n\n\n\n\nDec 23, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/using-stan-from-r/index.html",
    "href": "posts/using-stan-from-r/index.html",
    "title": "Using Stan from R",
    "section": "",
    "text": "I’ve been trying to get the hang of Bayesian models with Stan. One of the hurdles has been using Stan from R, so in this post I’m jotting down what I’ve learned (mostly the hard way).\n\nRStan or CmdStanR?\nOn the whole I had a much better time using CmdStanR than RStan. When I made a mistake that led to a runtime exception, RStan would simply die with this kind of error:\nError in `unserialize()`:\n! error reading from connection\n     ▆\n  1. └─rstan::stan(...)\n  2.   ├─rstan::sampling(...)\n  3.   └─rstan::sampling(...)\n  4.     └─rstan (local) .local(object, ...)\n  5.       └─parallel::parLapplyLB(cl, X = 1:chains, fun = callFun)\n  6.         ├─base::do.call(...)\n  7.         └─parallel::clusterApplyLB(...)\n  8.           └─parallel:::dynamicClusterApply(cl, fun, length(x), argfun)\n  9.             └─parallel:::recvOneResult(cl)\n 10.               ├─parallel::recvOneData(cl)\n 11.               └─parallel:::recvOneData.SOCKcluster(cl)\n 12.                 └─base::unserialize(socklist[[n]])\nCmdStanR on the other hand would print something useful. Other aspects of the development experience were also nicer:\n\nbetter status updates (e.g. showing when compiling)\neditor support for stan files e.g. linting\n\nI found installing CmdStanR, Stan and the rest of the toolchain from Macports to be very easy. However your mileage may vary when it comes to getting an R installation from Macports, due to some esoteric g95 and xcode compiler errors. I ended up having to locate the R and R-group files on my system and edit them. Hopefully the Macports team will resolve this, because I really appreciate being able to install R from there.\n\n\nEnsure show_messages is TRUE\nFor obvious reasons.\n\n\n“Rejecting initial value” errors\nI got “Rejecting initial value” a lot:\n\nlog probability evaluates to log(0), i.e. negative infinity\n\nThis is trying to tell you that the default initial value chosen by Stan has a probability of zero in your prior. You need to set constraints in the parameters block that match the prior distribution you choose. For example if you chose a uniform(1, 10) prior, you should add the constraint &lt;lower=1, upper=10&gt; to your parameter declaration.\n\n\nDon’t forget to set parallel_chains\nOn my system getOption(\"mc.cores\") was unset and Stan defaulted to running all four chains on a single core. Oops. You can also have within-chain parallelism, but I’ve read varying accounts on the usefulness of this (presumably there’s more communication required, which is usually the death of parallelism gains).\nOther software can apparently use GPUs for calculations by the way (e.g. the TensorFlow-based software) so if you have massive calculations to do those may be worth investigating.\n\n\nMaximum tree-depth warnings\nHitting maximum tree-depth for the majority of samples was because of an identifiability problem in my model.\nThe linked advice within the warning is fairly clear: https://mc-stan.org/misc/warnings#maximum-treedepth. If you’re wondering whether your Rhat and ESS values are good, read higher up that page.\n\n\nResolving unidentified or weakly-identified models\nI don’t have a comprehensive understanding of this, but the general idea is that your problem doesn’t have a unique solution, or more precisely there are multiple parameter values that would give the same distribution of observed data. Here’s a dumb example: imagine you are modelling your data as draws from a Normal distribution where the mean is the sum of two parameters, i.e. observations ~ normal(a + b, 1). If a and b don’t correspond to real phenomena captured in your observations, the values could be all over the place and the model will struggle. You need to constrain the parameters.\nA simple way to do this is to transform your parameters with the inverse logistic function, which has a range (output) between 0 and 1. You could also pin one of your parameters to a constant, though I found that harder to follow and more difficult to make work.\nHere’s an example from my F1 project, simplified slightly.\n#| label: stan-constraints-example\ndata {\n  int&lt;lower=1&gt; n_ctrs;\n  int&lt;lower=1&gt; n_drvs;\n  int&lt;lower=1&gt; n_obs;\n  array[n_obs] int&lt;lower=1, upper=20&gt; positions;\n  array[n_obs, 2] int position_indices;\n}\nparameters {\n  // Raw will be sampled from prior distribution\n  vector[n_ctrs] lambda_ctrs_raw;\n  vector[n_drvs] lambda_drvs_raw;\n}\ntransformed parameters {\n  // Transformed will be used as the parameter\n  vector[n_ctrs] lambda_ctrs;\n  vector[n_drvs] lambda_drvs;\n  \n  // The transformed params can only be between 1 and 10\n  // because the inv_logit function has range [0, 1]\n  lambda_ctrs = 1 + 9 * inv_logit(lambda_ctrs_raw);\n  lambda_drvs = 1 + 9 * inv_logit(lambda_drvs_raw);\n}\nmodel {\n  lambda_ctrs_raw ~ std_normal();\n  lambda_drvs_raw ~ std_normal();\n  \n  for (k in 1 : n_obs) {\n    int i = position_indices[k, 1];\n    int j = position_indices[k, 2];\n    positions[k] ~ poisson(exp(lambda_ctrs[i] + lambda_drvs[j]));\n  }"
  },
  {
    "objectID": "posts/importance-sampling/index.html",
    "href": "posts/importance-sampling/index.html",
    "title": "Importance Sampling",
    "section": "",
    "text": "The aim of this post is to teach myself basic Bayesian methods. I’ll set up a toy example problem and solve it.\nLet’s assume we have a natural event that happens with a variable periodicity e.g. a woman’s menstrual cycle. I’m drawing from this excellent blog on predicting a pregnancy, but doing a much simpler model because I am but a baby myself.\nThe aim here is to see if we can find the mean period of the cycle given some samples and priors for the average cycle. First, priors from the blog linked above:\n# a normal distribution for the mean days between period starts\nprior_mean_mu &lt;- 27.7\nprior_mean_sd &lt;- 2.4\n# a half-normal distribution for the SD of days between period starts\nprior_sd_mu &lt;- 1.6\nprior_sd_sd &lt;- 2.05\nWe will set the true mean and SD (the params, \\(\\theta\\), that we’re looking for) and simulate some sample data, \\(d\\).\nCode\nlibrary(tidyverse)\nset.seed(42)\ntrue_mu &lt;- 26.3\ntrue_sd &lt;- 1.9\n\nsample_periods &lt;- rnorm(12, true_mu, true_sd)\nFor importance sampling we need a likelihood function, \\(L(d|\\theta)\\). Make the model a normal distribution and this is simply the probability density. We take the log of the likelihood because the numbers could be very small and subject to numerical error.\nlog_likelihood &lt;- function(sample_periods, mean_period, sd_period) {\n  sum(dnorm(sample_periods, mean_period, sd_period, log = TRUE))\n}\nA little sanity check: the likelihood of the true parameters should be greater than the likelihood of the priors, for a large enough dataset.\nten_year_sample &lt;- rnorm(120, true_mu, true_sd)\nlog_likelihood(ten_year_sample, true_mu, true_sd) &gt; log_likelihood(ten_year_sample, prior_mean_mu, prior_mean_sd)\n\n[1] TRUE"
  },
  {
    "objectID": "posts/importance-sampling/index.html#importance-sampling-to-fit-the-model",
    "href": "posts/importance-sampling/index.html#importance-sampling-to-fit-the-model",
    "title": "Importance Sampling",
    "section": "Importance sampling to fit the model",
    "text": "Importance sampling to fit the model\n\nImportance sampling is a Monte Carlo method that is very easy to setup and that can work well if (1) the parameters space is small and (2) the priors are not too dissimilar from the posterior.\n\nI know that both these conditions are true in my toy example.\nFirst we must sample from the prior.\n\nn_samples &lt;- 100000\nprior &lt;- tibble(\n  mu_period = rnorm(n_samples, prior_mean_mu, prior_mean_sd),\n  sd_period = abs(rnorm(n_samples, 0, prior_sd_sd))\n)\n\nggplot(prior) +\n  aes(x = mu_period) +\n  geom_histogram(alpha=0.5, colour=\"black\", linewidth=0.1, fill=\"dodgerblue4\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nggplot(prior) +\n  aes(x = sd_period) +\n  geom_histogram(alpha=0.5, colour=\"black\", linewidth=0.1, fill=\"dodgerblue4\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThen we weight each draw by its likelihood.\n\nweights &lt;- sapply(1:n_samples, function(i) {\n  log_likelihood(sample_periods, prior$mu_period[i], prior$sd_period[i])\n})\n\nThen we resample the prior by this weighting.\n\n# n.b. sample will normalise the weights\nposterior &lt;- prior[sample(n_samples, replace = TRUE, prob = exp(weights)), ]\n\ndf &lt;- bind_rows(\n  mutate(prior, dist = \"prior\"),\n  mutate(posterior, dist = \"posterior\")\n) |&gt;\n  pivot_longer(c(mu_period, sd_period), names_to = \"param\", values_to = \"value\")\n\nggplot(df) +\n  aes(x = value, fill = dist) +\n  facet_wrap(vars(param), scales = \"free\") +\n  scale_fill_manual(values=c(\"dodgerblue1\", \"dodgerblue4\")) +\n  geom_histogram(alpha = 0.5, colour=\"black\", linewidth=0.1, position = \"identity\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nHey Doctor, would you look at that! The posterior is much tighter and closer to the true values.\n\nposterior |&gt; summarise(mu = mean(mu_period), sd = mean(sd_period))\n\n# A tibble: 1 × 2\n     mu    sd\n  &lt;dbl&gt; &lt;dbl&gt;\n1  27.7  1.88\n\n\nHow does that compare to simply taking the mean and SD of the sample data?\n\ntibble(\n    mu = mean(sample_periods),\n    sd = sd(sample_periods)\n)\n\n# A tibble: 1 × 2\n     mu    sd\n  &lt;dbl&gt; &lt;dbl&gt;\n1  27.7  1.75\n\n\nIt’s not highly convincing in this case. I ran this a few times and it seemed that the SD estimation seemed to be closer to the true value than the SD of the sample, but the mean was rarely different. This is probably because it’s a very simple model and importance sampling is rather overkill."
  },
  {
    "objectID": "posts/importance-sampling/index.html#proposal-distribution-isnt-necessarily-the-prior",
    "href": "posts/importance-sampling/index.html#proposal-distribution-isnt-necessarily-the-prior",
    "title": "Importance Sampling",
    "section": "Proposal distribution isn’t necessarily the prior",
    "text": "Proposal distribution isn’t necessarily the prior\nIn Bååth’s other blog on importance sampling he explains that we are looking at the ratio of the likelihood of getting our sample from the target distribution relative to getting it from the proposal distribution (then normalised to form a new estimate of the target distribution).\nIf the proposal distribution is thinner at the ends than the target distribution, this is suboptimal because we will have fewer samples there. So a T distribution is a useful proposal for a normal target, because it has fatter tails (as controlled by the df degrees of freedom parameter).\nWe could choose a very sloppy proposal like a uniform distribution, but in that case we are suboptimal in terms of how quickly we explore the space. We would have a relatively high number of samples in the tails compared to the peak. Bååth has a neat method of using a Laplacian approximation as a proposal."
  },
  {
    "objectID": "posts/importance-sampling/index.html#quick-look-at-another-example",
    "href": "posts/importance-sampling/index.html#quick-look-at-another-example",
    "title": "Importance Sampling",
    "section": "Quick look at another example",
    "text": "Quick look at another example\nNow an example from this blog post. This one uses a T as the proposal.\n\napprox_samples &lt;- rt(100, df = 3)  # T with 3 deg free is our approximation to posterior\napprox_density &lt;- dt(approx_samples, df = 3, log = TRUE)  # get the prob density for each\nunnormalised_posterior &lt;- dnorm(approx_samples, log = TRUE, sd = sqrt(3))  # convert to actual posterior\n\n# weights are then...\nlog_imp_weights &lt;- unnormalised_posterior - approx_density\ndata.frame(log_imp_weights) %&gt;% \n  ggplot(aes(exp(log_imp_weights))) +\n  geom_density(fill = \"dodgerblue4\", alpha = 0.5) +\n  labs(title = \"Importance weights\",\n       subtitle = \"Approximating a normal with a t distribution\")\n\n\n\n\n\n\n\n# normalise...\nweights &lt;- exp(log_imp_weights) / sum(exp(log_imp_weights))\n\nmean_estimate &lt;- sum(approx_samples * weights)\n\nThe point of using the Student T distribution as the proposal is to have a proposal with better coverage of the tails."
  },
  {
    "objectID": "posts/gaussian-processes/index.html",
    "href": "posts/gaussian-processes/index.html",
    "title": "Gaussian Processes in R",
    "section": "",
    "text": "I stumbled upon Gaussian Processes when looking into how the MIPRO prompt optimisation algorithm works and felt compelled to learn more about it. It’s a very nifty application of Bayesian modelling to analyse functions where data points are hard to get.\nThis post is based on a great lesson by Carpentries, which is part of a longer series on probabilistic programming in R. Very worth checking out if you want to know more."
  },
  {
    "objectID": "posts/gaussian-processes/index.html#the-kernel-k",
    "href": "posts/gaussian-processes/index.html#the-kernel-k",
    "title": "Gaussian Processes in R",
    "section": "The Kernel \\(K\\)",
    "text": "The Kernel \\(K\\)\n\\(K\\) is the most interesting parameter, being the covariance of the Gaussian Process. There’s some art to what function is chosen for \\(K\\), though I don’t want to get hung up on that at this point. Suffice it to say that the popular choice is squared exponential covariance, which is parameterised by \\(\\alpha\\) and \\(\\lambda\\).\n\\[\nK(x, x') = \\alpha^2 e^{\\frac{(x - x')^2}{2\\lambda}}\n\\]\nIf we implement the kernel and draw from \\(GP\\), we can see some potential functions. Each draw from \\(GP\\) will give us a vector of \\(f(x)\\) for the input \\(x\\) vector. We get a discrete approximation of the continuous function.\n\nset.seed(42)\nlibrary(tidyverse)\n\nsq_exp_cov &lt;- function(x, lambda, alpha) {\n  n &lt;- length(x)\n\n  K &lt;- matrix(0, n, n)\n\n  for (i in 1:n) {\n    for (j in 1:n) {\n      diff &lt;- sqrt(sum((x[i] - x[j])^2))\n      K[i, j] &lt;- alpha^2 * exp(-diff^2 / (2 * lambda^2))\n    }\n  }\n\n  K\n}\n\nx &lt;- 0:100\n\nGP &lt;- function(alpha, lambda, n_samples = 10) {\n  K &lt;- sq_exp_cov(x, lambda, alpha)\n  mat &lt;- MASS::mvrnorm(\n    n_samples,\n    mu = rep(0, length(x)), # Set the average level of the process to 0\n    Sigma = K\n  )\n  t(mat) |&gt;\n    data.frame() |&gt;\n    mutate(alpha = alpha, lambda = lambda, x = x) |&gt;\n    pivot_longer(c(-x, -alpha, -lambda), names_to = \"sample\", values_to = \"y\")\n}\n\ngrd &lt;- expand.grid(alpha = c(0.5, 1, 2), lambda = c(1, 10, 100))\nsamples &lt;- map2(grd$alpha, grd$lambda, GP) |&gt; bind_rows()\n\nlabeller &lt;- function(variable, value) {\n  if (variable == \"alpha\") {\n    sprintf(\"α = %s\", value)\n  } else {\n    sprintf(\"λ = %s\", value)\n  }\n}\n\nggplot(samples) +\n  aes(x = x, y = y, group = sample, colour = sample) +\n  facet_wrap(~ alpha * ~lambda, labeller = labeller) +\n  geom_line(show.legend = FALSE) +\n  labs(title = \"Samples of f(x) from GP\") +\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\nVarying \\(\\alpha\\) and \\(\\lambda\\) gives us an idea of how these hyperparameters change the functions we draw. \\(\\alpha\\) controls the standard deviation and \\(\\lambda\\) controls the correlation between \\(x\\) points."
  },
  {
    "objectID": "posts/gaussian-processes/index.html#the-effect-of-noise",
    "href": "posts/gaussian-processes/index.html#the-effect-of-noise",
    "title": "Gaussian Processes in R",
    "section": "The effect of noise",
    "text": "The effect of noise\nIn that model \\(\\sigma\\) controls the amount of noise we believe we have on the observations. Increasing that number will increase the uncertainty of the model, and correspondingly will make it more likely the MCMC simulation diverges."
  },
  {
    "objectID": "posts/gaussian-processes/index.html#optimising-with-cholesky-decomposition",
    "href": "posts/gaussian-processes/index.html#optimising-with-cholesky-decomposition",
    "title": "Gaussian Processes in R",
    "section": "Optimising with Cholesky decomposition",
    "text": "Optimising with Cholesky decomposition\nWe can speed up the simulation with a trick call Cholesky decomposition. We decompose \\(K = LL^T\\) and reparameterise \\(f = \\mu + L\\eta\\). If \\(\\eta\\) is normally distributed with mean zero and unit variance, this is equivalent.\ndata {\n  // Observed data\n  int n_data;\n  array[n_data] real x_data;\n  array[n_data] real y_data;\n  \n  // Observation error\n  real&lt;lower=0&gt; sigma;\n  \n  // x values for which we aim to predict y\n  int n_pred;\n  array[n_pred] real x_pred;\n  \n  // Hyperparameters for the kernel\n  real alpha;\n  real lambda;\n}\ntransformed data {\n  // We join the x observations and desired x prediction points\n  int n = n_data + n_pred;\n  \n  array[n] real x;\n  x[1 : n_data] = x_data;\n  x[(n_data + 1): n] = x_pred;\n  \n  // We calculate the Kernel values for all x\n  matrix[n, n] K;\n  K = gp_exp_quad_cov(x, alpha, lambda);\n  \n  // Add nugget on diagonal for numerical stability\n  for (i in 1 : n) {\n    K[i, i] = K[i, i] + 1e-6;\n  }\n}\nparameters {\n  // This is what we want to estimate\n  vector[n] f;\n}\nmodel {\n  // Likelihood is tested against the observations\n  y_data ~ normal(f[1 : n_data], sigma);\n\n  // f is sampled from GP\n  // The domain of the GP prior is all x\n  // We assume the mean is always 0\n  f ~ multi_normal(rep_vector(0, n), K);\n}\n\nmodel &lt;- cmdstan_model(stan_file = \"gp-cholesky.stan\", exe = \"gp-cholesky.stan.bin\")\n\nsamples &lt;- model$sample(\n  list(\n    n_data = nrow(df6),\n    x_data = as.array(df6$x),\n    y_data = as.array(df6$y),\n    sigma = 0.1,\n    n_pred = length(x_pred),\n    x_pred = x_pred,\n    alpha = 1,\n    lambda = 1\n  ),\n  parallel_chains = 4,\n  show_messages = FALSE # disabled to avoid polluting the blog post, should be TRUE\n)\n\nsamples\n\n variable   mean median   sd  mad     q5    q95 rhat ess_bulk ess_tail\n   lp__   -25.77 -25.48 4.77 4.69 -34.13 -18.42 1.00     1469     2478\n   eta[1]  -0.80  -0.80 0.10 0.10  -0.96  -0.64 1.00     5898     3414\n   eta[2]  -0.84  -0.84 0.10 0.10  -1.00  -0.68 1.00     6116     2987\n   eta[3]   1.26   1.26 0.12 0.12   1.06   1.46 1.00     5400     3419\n   eta[4]  -0.02  -0.02 0.11 0.11  -0.20   0.16 1.00     5701     2939\n   eta[5]  -1.38  -1.38 0.10 0.10  -1.54  -1.22 1.00     8086     2528\n   eta[6]   0.40   0.40 0.11 0.11   0.22   0.58 1.00     5329     3242\n   eta[7]   0.01   0.00 0.98 0.95  -1.62   1.62 1.00     7582     2950\n   eta[8]   0.00   0.01 1.00 1.00  -1.65   1.65 1.00     6981     3104\n   eta[9]   0.00   0.01 0.99 1.00  -1.68   1.62 1.00     7338     2935\n\n # showing 10 of 95 rows (change via 'max_rows' argument or 'cmdstanr_max_rows' option)\n\n\nIf we run that version convergence is much faster."
  },
  {
    "objectID": "posts/t.rex/index.html",
    "href": "posts/t.rex/index.html",
    "title": "T. rex size distributions",
    "section": "",
    "text": "I ought to be spending this weekend working on a risk intelligence thing for work, but I’ve stumbled across something that brings my inner child immense joy and such things cannot be ignored. It is this 2019 paper “An Older and Exceptionally Large Adult Specimen of Tyrannosaurus rex” by Persons et al. First it’s cool to see paleontology in action, instead of filtered through talking head documentaries. Second, the diligent authors have personally measured 12 T. rex specimens, which is a decent chunk of the 29 or-so specimens discovered to-date, and so we have a cool dataset to play with.\n\n\nCode\nlibrary(tidyverse)\nlibrary(knitr)\n\nt.rex &lt;- read_tsv(\"t.rex-persons.tsv\", col_types = \"ccnnnnnnnnnnnnnnn\") |&gt;\n  rename(Specimen = Specimens) |&gt;\n  mutate(Morph = factor(Morph, levels = c(\"Robust\", \"Gracile\")))\n\nkable(t.rex, format.args = list(decimal.mark = \".\", big.mark = \",\"))\n\n\n\n\nTable 1: T. rex bone measurements in millimetres (Table 1 from Persons et al 2019).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpecimen\nMorph\nDentary tooth row length\nScapula blade width\nManual p. I-1 length\nIlium length\nFemur length\nFemur circumference\nProximal femur width\nTibia length\nTibia shaft width\nFibula length\nFibula shaft width\nAstragalus height\nAstragalus width\nPedal p. IV-1 length\nBody mass (kg)\n\n\n\n\nRSM P2523.8\nRobust\n595\n74.8\n98.5\n1,545\n1,333\n590\n426\n1,140\n184\n995\n62\n328\n310\n184\n8,870\n\n\nCM 9380 (AMNH 973)\nRobust\n508\n73.0\nNA\n1,540\n1,269\n534\n399\n1,166\n150\n1,025\nNA\nNA\nNA\n145\n6,740\n\n\nFMNH PR2081\nRobust\n585\n68.0\n78.0\n1,525\n1,321\n580\n380\n1,140\n160\n1,030\n59\nNA\nNA\n154\n8,462\n\n\nMOR 1125\nRobust\n492\n65.7\nNA\nNA\n1,150\n515\n370\n1,060\n150\n915\n53\n280\n295\n170\n6,100\n\n\nRTMP 81.12.1, NMC 9950\nRobust\nNA\nNA\nNA\nNA\nNA\n495\nNA\n1,095\n155\n985\n65\nNA\nNA\n140\n5,469\n\n\nBHI 3033\nGracile\n575\nNA\nNA\n1,540\n1,350\n505\n350\n1,065\n158\n945\n55\n325\n280\nNA\n5,779\n\n\nMOR 555\nGracile\n635\n65.0\n89.0\n1,470\n1,280\n520\n370\n1,150\n170\n1,035\n56\nNA\n235\n176\n6,264\n\n\nMOR 980\nGracile\n546\nNA\nNA\n1,397\n1,232\n483\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n5,112\n\n\nRTMP 81.6.1\nGracile\n530\nNA\nNA\nNA\n1,210\n460\n270\n1,030\n160\nNA\n52\nNA\nNA\n107\n4,469\n\n\nBM R8040 (AMNH 5881)\nNA\nNA\nNA\nNA\nNA\nNA\n480\n330\nNA\nNA\nNA\nNA\nNA\nNA\n124\n5,025\n\n\nMOR 009\nNA\nNA\nNA\n90.0\n1,180\n1,100\n469\nNA\n1,105\n140\n930\nNA\nNA\nNA\nNA\n4,714\n\n\nUSNM 6183\nNA\nNA\nNA\nNA\nNA\n1,040\n426\nNA\n910\nNA\nNA\nNA\nNA\nNA\nNA\n3,617\n\n\n\n\n\n\n\n\nWe can also add “Goliath”, the exciting 2024 specimen. Bear in mind that Persons didn’t personally measure this fossil, so that’s a source of potential error. I took the measurements from the few social media posts about the fossil. The lack of information on this specimen seems strange to me, I even wonder if it’s a hoax. It wouldn’t be the first time.\n\n\nCode\ncampione.body.mass.kg &lt;- function(min.femoral.circ.mm) 10**((2.754 * log10(min.femoral.circ.mm) - 0.683) - 3)\n\ngoliath &lt;- read_tsv(\"t.rex-goliath.tsv\", col_types = \"ccnnnnnnnnnnnnnnn\") |&gt;\n  mutate(\n    Morph = factor(Morph, levels = c(\"Robust\", \"Gracile\")),\n    `Body mass (kg)` = campione.body.mass.kg(`Femur circumference`)\n  )\n\nt.rex &lt;- bind_rows(t.rex, goliath)\nkable(select(goliath, Specimen, `Femur length`, `Femur circumference`, `Body mass (kg)`))\n\n\n\n\nTable 2: Reported measurements for the Goliath specimen.\n\n\n\n\n\n\nSpecimen\nFemur length\nFemur circumference\nBody mass (kg)\n\n\n\n\nGoliath\n1371\n648\n11483.59\n\n\n\n\n\n\n\n\nThe most interesting bone is the femur, being as it’s more likely to be present and is used to estimate body mass. The equation used is from Campione et al 2014. Bear in mind they suggest a 25% prediction error.\nAn interesting thing about T. rex is that there are two morphs, gracile and robust, which I understand to be layman’s terms for “elven” and “chonky”. Let’s see if that’s visible in the femur measurements.\n\n\nCode\nggplot(t.rex) +\n  aes(x = `Femur length`, y = `Femur circumference`, colour = Morph, label = Specimen) +\n  geom_label(nudge_y = 10, na.rm = TRUE) +\n  geom_point(na.rm = TRUE) +\n  scale_x_continuous(limits = c(1000, 1400)) +\n  scale_y_continuous(limits = c(400, 700)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 1: Femur circumference and length for T. rex specimens.\n\n\n\n\n\nLooking at this scatterplot, there’s a linear relationship between length and circumference, and Goliath sits neatly on it. (Too neatly? Hmm. Should it even be linear? Wouldn’t the square-cube law suggest femurs should get thicker faster than they get longer?) However it’s not obvious if Goliath should be gracile or robust. In fact, it’s not particularly clear from the femurs alone why some are gracile and some robust; CM 9380 (Holotype) and MOR 555 (Wankel) are very close, for example. We need more features. Unfortunately, the T. rex data are very sparse, so we need to impute missing values.\nTibia length is relatively complete, can we use that?\n\n\nCode\nggplot(t.rex) +\n  aes(x = `Femur length`, y = `Tibia length`, colour = Morph) +\n  geom_point() +\n  scale_x_continuous(limits = c(1000, 1400)) +\n  scale_y_continuous(limits = c(900, 1200)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 2: Femur length and tibia length for T. rex specimens.\n\n\n\n\n\nThe relationship between fibia length and tibia length is surprisingly not very informative.\n\n\nCode\nggplot(t.rex) +\n  aes(x = `Femur circumference`, y = `Tibia length`, colour = Morph) +\n  geom_point() +\n  scale_x_continuous(limits = c(400, 700)) +\n  scale_y_continuous(limits = c(900, 1200)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 3: Femur circumference and tibia length for T. rex specimens.\n\n\n\n\n\nFemur circumference relates a little better. Let’s try a dead simple linear model to predict the missing femur lengths. We can include body mass, which is a function of femur circumference, to try and capture some of the knowledge from that model.\n\nreg &lt;- lm(`Femur length` ~ `Femur circumference` * `Body mass (kg)` + `Tibia length`, data = t.rex, subset = !is.na(t.rex$`Tibia length`) & !is.na(`Femur length`))\nsummary(reg)\n\n\nCall:\nlm(formula = `Femur length` ~ `Femur circumference` * `Body mass (kg)` + \n    `Tibia length`, data = t.rex, subset = !is.na(t.rex$`Tibia length`) & \n    !is.na(`Femur length`))\n\nResiduals:\n       1        2        3        4        6        7        9       11 \n  -2.758   10.381    3.116 -116.353   97.051   30.241   49.109  -69.132 \n      12 \n  -1.655 \n\nCoefficients:\n                                         Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                            -1.136e+04  2.430e+04  -0.467    0.665\n`Femur circumference`                   5.020e+01  1.037e+02   0.484    0.654\n`Body mass (kg)`                       -3.832e+00  8.868e+00  -0.432    0.688\n`Tibia length`                         -2.585e-01  8.777e-01  -0.294    0.783\n`Femur circumference`:`Body mass (kg)`  3.317e-03  8.054e-03   0.412    0.702\n\nResidual standard error: 88.3 on 4 degrees of freedom\nMultiple R-squared:  0.6787,    Adjusted R-squared:  0.3574 \nF-statistic: 2.112 on 4 and 4 DF,  p-value: 0.2433\n\n\nIt’s not great, though slightly better than the other combinations I tried and probably the best we could do with such a small dataset. The additional imputed femur length (RTMP 81.12.1) is feasible enough.\n\n\nCode\npreds &lt;- predict(reg, newdata = t.rex)\n\nt.rex |&gt;\n  mutate(`Femur length` = coalesce(`Femur length`, preds)) |&gt;\n  select(`Specimen`, `Morph`, `Femur length`, `Femur circumference`, `Tibia length`, `Body mass (kg)`) |&gt;\n  kable()\n\n\n\n\nTable 3: Imputed femur length for RTMP 81.12.1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpecimen\nMorph\nFemur length\nFemur circumference\nTibia length\nBody mass (kg)\n\n\n\n\nRSM P2523.8\nRobust\n1333.000\n590\n1140\n8870.00\n\n\nCM 9380 (AMNH 973)\nRobust\n1269.000\n534\n1166\n6740.00\n\n\nFMNH PR2081\nRobust\n1321.000\n580\n1140\n8462.00\n\n\nMOR 1125\nRobust\n1150.000\n515\n1060\n6100.00\n\n\nRTMP 81.12.1, NMC 9950\nRobust\n1230.292\n495\n1095\n5469.00\n\n\nBHI 3033\nGracile\n1350.000\n505\n1065\n5779.00\n\n\nMOR 555\nGracile\n1280.000\n520\n1150\n6264.00\n\n\nMOR 980\nGracile\n1232.000\n483\nNA\n5112.00\n\n\nRTMP 81.6.1\nGracile\n1210.000\n460\n1030\n4469.00\n\n\nBM R8040 (AMNH 5881)\nNA\nNA\n480\nNA\n5025.00\n\n\nMOR 009\nNA\n1100.000\n469\n1105\n4714.00\n\n\nUSNM 6183\nNA\n1040.000\n426\n910\n3617.00\n\n\nGoliath\nNA\n1371.000\n648\nNA\n11483.59\n\n\n\n\n\n\n\n\nI’m not sure what else to do with this. Must be tough being a paleontologist, working with so little direct data!"
  },
  {
    "objectID": "posts/traitors/index.html",
    "href": "posts/traitors/index.html",
    "title": "Can we predict who wins the Traitors with an R simulation?",
    "section": "",
    "text": "Three years in, I am still absolutely hooked on the Traitors (the TV series). Sure, yes, they stretch ten minutes of gameplay into sixty minutes of “you know I love you, but…” and the weirdly maddening attempts to make “yourself” a polite version of “you”. And yes, there’s massive thumb-shaped groove on the scales because the producers are never going to let the game end early. But all the same, it’s such fun watching the players plot and scheme and make terrible, terrible decisions.\nOccasionally the show includes contestants who are expert game players. Frankly they tend to underwhelm - with the honourable exception being Kate in Traitors Australia - yet it makes me wonder about optimum strategies. And what’s more fun than playing a game? Making a model of the game in R!\nHere we go, starting with a simple model and building up to more realistic strategies. If you’re here just for some insights into the Traitors, you can skip over the code without missing much of the logic.\n\n\nCode\n#  Plumbing and other irrelevant code is behind the folds.\nlibrary(knitr)\nlibrary(tidyverse)\n\nset.seed(42) # trad\n\norange &lt;- rgb(236 / 255, 137 / 255, 51 / 255)\nmauve &lt;- rgb(89 / 255, 14 / 255, 50 / 255)\n\n\n\nSimple model\nThe minimum players for Werewolf, a.k.a. Mafia (the game on which Traitors is based) is six, with four villagers and two wolves. In this version of the game there is no recruitment and there are no shields. Let’s model this simple version and build up from there. We’ll set some basic default strategies for the traitors and the faithful.\n\ngame &lt;- function(\n    n_players = 6,\n    n_traitors = 2,\n    traitor_strategy = random_traitors,\n    faithful_strategy = random_faithful) {\n\n  while (TRUE) {\n\n    # There's an implicit player index:\n    #  c(traitor, traitor, faithful, faithful, ..., faithful)\n\n    # Traitors are always the leading elements in the index, so we can select them with an index range.\n\n    # End-game condition\n    if (n_traitors == 0 || n_players - n_traitors &lt; n_traitors) {\n      break\n    }\n\n    #############\n    # DAY PHASE #\n    #############\n\n    # Traitors and faithfuls update the votes array according to their strategies\n    \n    votes &lt;- rep(0, n_players)\n\n    if (n_players - n_traitors == 1) {\n      # special case for 2 players 1 traitor\n      # coded here to avoid having to handle it repeatedly\n      votes[1:n_traitors] &lt;- n_players\n    } else {\n      votes[1:n_traitors] &lt;- traitor_strategy(n_players, n_traitors)\n    }\n    votes[(n_traitors + 1):n_players] &lt;- faithful_strategy(n_players, n_traitors)\n\n    # Claudia counts the votes\n    banished &lt;- which.max(tabulate(votes))\n\n    n_players &lt;- n_players - 1 # BANISHMENT!\n\n    # If it was a traitor who was banished, decrement\n    if (banished &lt;= n_traitors) {\n      n_traitors &lt;- n_traitors - 1\n    }\n\n    # Check if the game is done yet\n    if (n_traitors == 0 || n_players - n_traitors &lt; n_traitors) {\n      break\n    }\n\n    ###############\n    # NIGHT PHASE #\n    ###############\n    n_players &lt;- n_players - 1 # MURDER!\n  }\n\n  list(n_players_remaining = n_players, n_traitors_remaining = n_traitors)\n}\n\nrandom_faithful &lt;- function(n_players, n_traitors) {\n  # vote for a single random player\n  target &lt;- sample(1:n_players, size = 1)\n  rep(target, n_players - n_traitors)\n}\n\nrandom_traitors &lt;- function(n_players, n_traitors) {\n  # vote a single random faithful\n  target &lt;- sample((n_traitors + 1):n_players, size = 1)\n  rep(target, n_traitors)\n}\n\n# Simulate one random game\ngame()\n\n$n_players_remaining\n[1] 1\n\n$n_traitors_remaining\n[1] 1\n\n\nThat’s it. All we need to do now is simulate a lot of games and analyse the results. dplyr is our friend for this, as usual.\n\nsim_games &lt;- function(..., .game = game, .n_games = 10000) {\n  1:.n_games |&gt;\n    map(\\(x) .game(...), .progress = TRUE) |&gt;\n    bind_rows() |&gt;\n    mutate(\n      traitors_win = n_traitors_remaining &gt; 0,\n      winner = ifelse(n_traitors_remaining &gt; 0, \"Traitors\", \"Faithful\")\n    )\n}\n\nplot_game_results &lt;- function(results, n_players, n_traitors) {\n  traitor_win_prob &lt;- mean(results$traitors_win)\n\n  ggplot(\n    results |&gt;\n      group_by(winner) |&gt;\n      count()\n  ) +\n    aes(y = winner, x = n, fill = winner, label = winner) +\n    geom_bar(stat = \"identity\", colour = \"black\", orientation = \"y\", alpha = 0.9) +\n    scale_fill_manual(values = c(mauve, orange)) +\n    labs(\n      title = sprintf(\n        \"Game result for %s players, %s traitors. Traitor win prob: %s\",\n        n_players,\n        n_traitors,\n        round(traitor_win_prob, digits = 2)\n      ),\n      x = \"Wins\",\n      y = NULL,\n      fill = \"Winner\"\n    ) +\n    theme_minimal() +\n    theme(aspect.ratio = 0.5)\n}\n\nresults &lt;- sim_games(6, 2)\nplot_game_results(results, 6, 2)\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\nThat’s quite in favour of the traitors, but the picture is different when we change the balance to be the same as the show.\n\nresults &lt;- sim_games(22, 3)\nplot_game_results(results, 22, 3)\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\nNow the game is nearly perfectly balanced.\nTo check this simulation is on the right track, I had a little look at a paper on Mafia by Migdał, 2024, which includes this formula for traitor winning probability:\n\\[\np(n, m) \\propto \\frac{m}{\\sqrt{n}}\n\\]\nWhere \\(n\\) is the number of players, \\(m\\) is the number of traitors, and \\(p\\) is the winning probability.\nMigdał calculates the exact probabilities for games with a single traitor and a pseudo-random faithful vote strategy, so we can compare the simulation to the calculation.\nThe formula in this case is:\n\\[\np(n, 1) = \\frac{(n -1)!!}{n!!}\n\\]\n\\(n!!\\) is the double factorial of \\(n\\), i.e. \\(1 \\times 3 \\times ... \\times n\\) if \\(n\\) is odd and \\(2 \\times 4 \\times ... \\times n\\) if \\(n\\) is even.\n\n\nCode\nmigdal &lt;- function(n) {\n  if (n == 1) {\n    1\n  } else {\n    if (n %% 2 == 0) {\n      prod(seq(1, n - 1, 2)) / prod(seq(2, n, 2))\n    } else {\n      prod(seq(2, n - 1, 2)) / prod(seq(1, n, 2))\n    }\n  }\n}\n\nsim_probs &lt;- 1:20 |&gt;\n  map(\\(n) mean(sim_games(n, 1)$traitors_win), .progress = TRUE) |&gt;\n  as.double()\n\nmigdal_probs &lt;- 1:20 |&gt;\n  map(migdal) |&gt;\n  as.double()\n\nggplot(tibble(n_players = 1:20, sim_prob = sim_probs)) +\n  aes(x = n_players, y = sim_prob) +\n  geom_point(colour = \"red\") +\n  geom_line(data = tibble(n_players = 1:20, migdal_prob = migdal_probs), mapping = aes(y = migdal_prob)) +\n  scale_y_continuous(limits = c(0, 1.5))\n\n\n\n\n\n\n\n\nFigure 3: Comparison of simulated (points) to calculated (line) win probabilities.\n\n\n\n\n\nThe sawtooth pattern is one of the interesting results of Migdał’s paper: an odd number of players strongly favours the traitors, to the extent that having 8 faithful and 1 traitor gives the traitor a better win chance than having 3 faithful and 1 traitor, despite there being more than twice as many faithful in the former.\nThat has a useful implication for the producers of the Traitors: if they want to keep the game balanced, the timing of adding new players is crucial. I suspect they already know this.\n\n\nHow good is our random vote assumption?\nLet’s see how random the votes actually are. Happily, Traitors is a big enough phenomenon that people have recorded the players’ votes online. Here for example are the votes from Australia’s first season. Below the fold is some code for manipulating the copy-pasted votes into a dataframe.\n\n\nCode\n# Some fixes were required: in Aus two players abstained from a vote, and one quit.\n# I converted these into pseudo-votes for an imaginary player.\n\nvotes &lt;- list(\n  # Aus 1\n  list(\n    c(20, 2, 1),\n    c(16, 2, 1, 1, 1),\n    c(4, 4, 3, 3, 2, 1, 1, 1),\n    c(11, 5, 2),\n    c(7, 6, 2, 1),\n    c(7, 5, 1, 1),\n    c(9, 3, 1),\n    c(9, 1, 1),\n    c(8, 2),\n    c(4, 1, 1, 1, 1),\n    c(5, 1, 1),\n    c(4, 1),\n    c(3, 1),\n    c(2, 1)\n  ),\n\n  # UK 1\n  list(\n    c(11, 8),\n    c(8, 6, 3),\n    c(11, 4),\n    c(9, 4),\n    c(7, 3, 1),\n    c(7, 1, 1),\n    c(6, 2),\n    c(4, 2),\n    c(3, 2),\n    c(1, 3),\n    c(3, 0)\n  ),\n\n  # UK 2\n  list(\n    c(17, 1, 1),\n    c(10, 6, 1),\n    c(7, 6, 2, 1),\n    c(7, 4, 1, 1, 1),\n    c(6, 5, 2, 1),\n    c(7, 3, 2, 1),\n    c(6, 2, 2),\n    c(4, 2, 1, 1),\n    c(4, 2),\n    c(4, 1),\n    c(2, 2),\n    c(3, 1),\n    c(3, 0)\n  ),\n\n  # US\n  list(\n    c(15, 2, 2, 1, 1),\n    c(4, 4, 4, 3, 1, 1, 1, 1),\n    c(7, 6, 3),\n    c(14, 1, 1, 1, 1),\n    c(12, 2, 1, 1),\n    c(7, 3, 2, 1, 1),\n    c(9, 2, 1),\n    c(7, 4),\n    c(6, 2, 1),\n    c(6, 1, 1),\n    c(4, 1, 1),\n    c(4, 1),\n    c(4, 0),\n    c(3, 1),\n    c(1, 2),\n    c(2, 1)\n  )\n)\n\npad &lt;- function(v, new_length, value) {\n  v2 &lt;- rep(value, new_length)\n  v2[1:length(v)] &lt;- v\n  v2\n}\n\nvotes_df &lt;- tibble(v = flatten(votes)) |&gt;\n  rowwise() |&gt;\n  filter(sum(v) &gt; 5) |&gt; # ignore the finals, which are a little different\n  mutate(\n    n_players = sum(v),\n    v = list(pad(v, n_players, 0)), # pad 0s for with each player that didn't receive a vote\n  )\n\n\nWe can guess by looking that these aren’t drawn from a uniform distribution, and \\(\\chi^2\\) agrees.\n\nvotes_df |&gt;\n  mutate(p.value = chisq.test(v, simulate.p.value = TRUE)[\"p.value\"]) |&gt;\n  unnest(p.value) |&gt;\n  head(5) |&gt;  # the whole table is not that interesting\n  kable(digits = 5)\n\n\n\nTable 1: Completely unnecessary \\(\\chi^2\\) goodness-of-fit tests.\n\n\n\n\n\n\n\n\n\n\n\nv\nn_players\np.value\n\n\n\n\n20, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n23\n0.0005\n\n\n16, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n21\n0.0005\n\n\n4, 4, 3, 3, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n19\n0.0085\n\n\n11, 5, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n18\n0.0005\n\n\n7, 6, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n16\n0.0005\n\n\n\n\n\n\n\n\nIt looks to me like the faithful do herd together to whomp one or two unlucky players. Sometimes this is pre-arranged, sometimes it’s just groupthink (“I’ve just got nothing to go on, so I put yourself…”).\nLet’s implement a voting strategy that’s closer to this reality. We’ll take the mean of the vote distributions as the probability of voting for target one, two, three, etc, to form a multinomial distribution, so that our simulation reflects actual vote distributions.\n\nn_rounds &lt;- nrow(votes_df)\nn_players &lt;- max(votes_df$n_players)\n\nmat &lt;- matrix(0, nrow = n_rounds, ncol = n_players)\n\nfor (i in 1:n_rounds) {\n  v &lt;- votes_df$v[[i]]\n  mat[i, 1:length(v)] &lt;- v / sum(v)\n}\n\nvote_prob &lt;- colSums(mat) / nrow(mat)\n\nbarplot(vote_prob)\n\n\n\n\n\n\n\nFigure 4: Mean of normalised vote counts across each round.\n\n\n\n\n\nIn a typical round, we’d expect about 62% of players to vote for player A, and 23% for player B, etc. This doesn’t relate to the probability of player A or B being traitors, just the expected vote split.\nLet’s make that a strategy function and simulate it.\n\nrandom_herd &lt;- function(n_players, n_traitors) {\n  shuffled &lt;- sample(n_players, n_players)\n  sample(shuffled, size = n_players - n_traitors, replace = TRUE, prob = vote_prob[1:n_players])\n}\n\nresults &lt;- sim_games(n_players = 22, n_traitors = 3, traitor_strategy = random_traitors, faithful_strategy = random_herd)\nplot_game_results(results, 22, 3)\n\n\n\n\n\n\n\nFigure 5\n\n\n\n\n\nHaving a split vote isn’t much worse than the fully random strategy, though it still gives the traitors a slight advantage. Could the faithfuls do better? Well yes, if they were any good at spotting traitors.\n\n\nSmarter faithfuls\nLet’s grant the faithfuls a glimmer of intuition: they are twice as suspicious of traitors as faithfuls, i.e. a traitor is twice as likely to be the vote target as a faithful. We’ll call that the suspicion factor, where a value of 1 means a traitor is equally suspicious as any other player to the faithfuls.\nWe can model this with a suspicion vector, which is the (normalised) level of suspicion for each player. We’ll need a new game function, which we’ll call game2. This version of the game allows both faithfuls and traitors to influence the suspicion on each player each day.\n\ngame2 &lt;- function(\n    n_players = 22,\n    n_traitors = 3,\n    traitors_influence = no_influence,\n    faithfuls_influence = no_influence,\n    traitors_strategy = random_traitors2,\n    faithfuls_strategy = random_faithful2) {\n  while (TRUE) {\n\n    if (n_traitors == 0 || n_players - n_traitors &lt; n_traitors) {\n      break\n    }\n\n    #############\n    # DAY PHASE #\n    #############\n\n    # This is the new bit:\n    # traitors and faithfuls influence suspicion\n    suspicions &lt;- rep(1, n_players) |&gt;\n      faithfuls_influence(n_traitors) |&gt;\n      traitors_influence(n_traitors)\n    suspicions &lt;- suspicions / sum(suspicions)  # normalise\n\n    # Voting is as-before\n    votes &lt;- rep(0, n_players)\n\n    if (n_players - n_traitors == 1) {\n      votes[1:n_traitors] &lt;- n_players\n    } else {\n      votes[1:n_traitors] &lt;- traitors_strategy(suspicions, n_players, n_traitors)\n    }\n    votes[(n_traitors + 1):n_players] &lt;- faithfuls_strategy(suspicions, n_players, n_traitors)\n\n    banished &lt;- which.max(tabulate(votes))\n\n    n_players &lt;- n_players - 1 # BANISHMENT!\n    if (banished &lt;= n_traitors) {\n      n_traitors &lt;- n_traitors - 1\n    }\n\n    if (n_traitors == 0 || n_players - n_traitors &lt; n_traitors) {\n      break\n    }\n\n    ###############\n    # NIGHT PHASE #\n    ###############\n    n_players &lt;- n_players - 1 # MURDER!\n  }\n\n  list(n_players_remaining = n_players, n_traitors_remaining = n_traitors)\n}\n\nrandom_faithful2 &lt;- function(suspicions, n_players, n_traitors) {\n  # vote for a single random player\n  target &lt;- sample(1:n_players, size = 1)\n  rep(target, n_players - n_traitors)\n}\n\nrandom_traitors2 &lt;- function(suspicions, n_players, n_traitors) {\n  # as before: vote a single random faithful\n  target &lt;- sample((n_traitors + 1):n_players, size = 1)\n  rep(target, n_traitors)\n}\n\nno_influence &lt;- function(suspicions, n_traitors) suspicions\n\nWe set the default influence to be a pass-through i.e. no influence, and set the same default vote strategies as before (adapted to accept but ignore a suspicions argument). In that case we see that the game is balanced the same as before. That’s a useful sanity check.\n\nresults &lt;- sim_games(.game = game2)\nplot_game_results(results, 22, 3)\n\n\n\n\n\n\n\nFigure 6\n\n\n\n\n\nWe can check with the random herd strategy too.\n\nrandom_herd2 &lt;- function(suspicion, n_players, n_traitors) {\n  shuffled &lt;- sample(n_players, n_players)\n  sample(shuffled, size = n_players - n_traitors, replace = TRUE, prob = vote_prob[1:n_players])\n}\n\nresults &lt;- sim_games(faithfuls_strategy = random_herd2, .game = game2)\nplot_game_results(results, 22, 3)\n\n\n\n\n\n\n\nFigure 7\n\n\n\n\n\nAlso the same. But now let’s grant the faithfuls some strong intuition, twice as much suspicion on traitors as faithfuls. To support different levels of suspicion we can use a closure.\n\nincreased_suspicion &lt;- function(suspicion_factor) {\n  function(suspicions, n_traitors) {\n    suspicions[1:n_traitors] &lt;- suspicion_factor\n    suspicions\n  }\n}\n\n# e.g. increased_suspicion(2) returns a function that updates the traitors' suspicion values to 2\n\nWe also define a version of the herd strategy that selects the players to whomp according to their relative suspicion.\n\nsuspicious_herd &lt;- function(suspicions, n_players, n_traitors) {\n  shuffled &lt;- sample(n_players, size = n_players, prob = suspicions)\n  sample(shuffled, size = n_players - n_traitors, replace = TRUE, prob = vote_prob[1:n_players])\n}\n\nHow does that shake out for the faithfuls?\n\nresults &lt;- sim_games(faithfuls_influence = increased_suspicion(2), faithfuls_strategy = suspicious_herd, .game = game2)\nplot_game_results(results, 22, 3)\n\n\n\n\n\n\n\nFigure 8\n\n\n\n\n\nMagnificently! That puts the balance resoundingly in favour of the faithfuls, showing how fragile the traitors’ position really is. We can explore how the traitors’ win probability depends on how savvy the faithful are.\n\nsuspicions &lt;- c(0.5, 1.0, 1.2, 1.5, 2.0, 5.0, 10.0)\ns_df &lt;- suspicions |&gt;\n  map(\n    \\(s)\n    sim_games(\n      n_players = 22,\n      n_traitors = 3,\n      faithfuls_influence = increased_suspicion(s),\n      faithfuls_strategy = suspicious_herd,\n      .game = game2\n    ) |&gt;\n      mutate(suspicion = s),\n    .progress = TRUE\n  ) |&gt;\n  bind_rows() |&gt;\n  group_by(suspicion) |&gt;\n  summarise(traitor_win_prob = mean(traitors_win))\n\n ■■■■■■■■■■■■■■■■■■■■■■■■■■■       86% |  ETA:  2s\n\nggplot(s_df) +\n  aes(x = suspicion, y = traitor_win_prob) +\n  geom_area(fill = orange, colour = \"black\") +\n  scale_x_continuous(breaks = seq(0, 10, 1)) +\n  scale_y_continuous(limits = c(0, 1)) +\n  labs(\n    x = \"Suspicion relative to faithful\",\n    y = \"Traitors win probability\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 9: Traitor win probability as a function of suspicion.\n\n\n\n\n\nThe odds get exponentially worse the more the faithful suspect the traitors. And vice-versa: if the traitors are less-suspected than the faithful, they stand a strong chance of winning the game.\n\n\nSmarter traitors\nNaturally the traitors will try to influence faithfuls towards other faithfuls. (Or other traitors, but let’s leave that strategy aside for now.) We could model that as the traitors being able to increase the suspicion level of other players. If they have limited influence, perhaps it makes sense for them to make a concerted effort to put suspicion on one other player.\n\nsow_suspicion &lt;- function(suspicion_factor) {\n  function(suspicions, n_traitors) {\n    suspicions[n_traitors + 1] &lt;- suspicion_factor\n    suspicions\n  }\n}\n\nresults &lt;- sim_games(\n  traitors_influence = sow_suspicion(2),\n  faithfuls_influence = increased_suspicion(2),\n  faithfuls_strategy = suspicious_herd,\n  .game = game2\n)\nplot_game_results(results, 22, 3)\n\n\n\n\n\n\n\nFigure 10\n\n\n\n\n\nIt’s an improvement! They’ve dragged the win probability from 0.24 up to 0.32. To completely nullify the faithfuls’ advantage though they would need to sow enough suspicion to make the traitors below the group’s mean level of suspicion.\nHappily for the traitors, the faithfuls seem likely to manage that themselves, by fixating on a “twinkle in the eye” or other red herrings. As we’ve seen, it’s enough for the traitors to wait for faithfuls to get suspicious of another faithful and run with that.\n\n\nWhere next?\n\nAll models are wrong, especially yours Chris.\nGeorge Box (ghost of)\n\nIt hardly needs to be said that this model is a much-simplified version of reality. We’ve barely touched on the psychological aspects that make the game so fun, as well as many of the TV’s specific game mechanics:\n\nshields\nrecruitment\nthe final\n\nThose would certainly change the balance.\nAt the moment the suspicions vector is also too simplistic. I was tempted to make some kind of model that draws suspicions from a Dirichlet distribution, but decided that making a more complex model to simulate player behaviour that is sometimes almost chaotic is probably not a productive route.\nThere’s more we could do with the model we have so far too, for example looking at how the traitor win probability increases as the game progresses. For example at the time of writing, the Season 3 in the UK has 8 faithfuls and 2 traitors, with one murder pending. If we repeat the last result with those numbers, the traitors are on-track to win.\n\nresults &lt;- sim_games(\n  n_players = 7,\n  n_traitors = 2,\n  traitors_influence = sow_suspicion(2),\n  faithfuls_influence = increased_suspicion(2),\n  faithfuls_strategy = suspicious_herd,\n  .game = game2\n)\nplot_game_results(results, 7, 2)\n\n\n\n\n\n\n\nFigure 11\n\n\n\n\n\nIncidentally, the traitors decided against recruitment. However that would have been an even better move.\n\nresults &lt;- sim_games(\n  n_players = 8,\n  n_traitors = 3,\n  traitors_influence = sow_suspicion(2),\n  faithfuls_influence = increased_suspicion(2),\n  faithfuls_strategy = suspicious_herd,\n  .game = game2\n)\nplot_game_results(results, 8, 3)\n\n\n\n\n\n\n\nFigure 12\n\n\n\n\n\nSomebody better teach those traitors to screw around with code!"
  },
  {
    "objectID": "posts/f1-stan/f1.html",
    "href": "posts/f1-stan/f1.html",
    "title": "Latent F1 team and driver performance: fun with Bayesian models",
    "section": "",
    "text": "For a bit of fun, let’s try and model the performance of F1 drivers and constructors in the 2024 season using Bayesian models. The aim is to more to learn about using Stan than determine deep truths about F1, since there’s far more to performance than we can easily model.\nWe’ll vaguely imitate the f1-metrics model, which I used to eagerly await after each season. In this model there are latent variables of driver skill and car quality, which predict points/positions. However I never did get hold of the paper for that, and am instead following the amazing Rasmus Bååth’s blog where he models the latent skill of football teams in La Liga.\nI gathered CSVs of race data from the fantastic Ergast. The weapon of choice is R and the tidyverse, since nothing else is quite so ergonomic for this type of analysis. You can find the code behind the folds if you want to play along.\nCode\nlibrary(tidyverse)\nlibrary(knitr) # this lib is for rendering tables nicely\n\nqualifying &lt;- read_csv(\"data/qualifying.csv\") |&gt; rename(quali_position = position)\nraces &lt;- read_csv(\"data/races.csv\") |&gt;\n  select(raceId, year, round, circuitId, name) |&gt;\n  rename(gp = name)\nresults &lt;- read_csv(\"data/results.csv\") |&gt; select(resultId, raceId, driverId, constructorId, grid, position, points, milliseconds)\nconstructors &lt;- read_csv(\"data/constructors.csv\") |&gt; select(constructorId, constructorRef)\ndrivers &lt;- read_csv(\"data/drivers.csv\") |&gt; select(driverId, driverRef)\ncircuits &lt;- read_csv(\"data/circuits.csv\") |&gt; select(circuitId, circuitRef)\n\nctr_drvs &lt;- tribble(\n  ~constructorRef, ~driverRef,\n  \"alpine\", \"doohan\",\n  \"alpine\", \"gasly\",\n  \"alpine\", \"ocon\",\n  \"aston_martin\", \"alonso\",\n  \"aston_martin\", \"stroll\",\n  \"ferrari\", \"bearman\",\n  \"ferrari\", \"leclerc\",\n  \"ferrari\", \"sainz\",\n  \"haas\", \"bearman\",\n  \"haas\", \"hulkenberg\",\n  \"haas\", \"kevin_magnussen\",\n  \"mclaren\", \"norris\",\n  \"mclaren\", \"piastri\",\n  \"mercedes\", \"hamilton\",\n  \"mercedes\", \"russell\",\n  \"rb\", \"lawson\",\n  \"rb\", \"ricciardo\",\n  \"rb\", \"tsunoda\",\n  \"red_bull\", \"max_verstappen\",\n  \"red_bull\", \"perez\",\n  \"sauber\", \"bottas\",\n  \"sauber\", \"zhou\",\n  \"williams\", \"albon\",\n  \"williams\", \"colapinto\",\n  \"williams\", \"sargeant\",\n)\n\nctr_colours &lt;- tribble(\n  ~constructorRef, ~colour,\n  \"red_bull\", \"#3671c6\",\n  \"mercedes\", \"#26f5d2\",\n  \"ferrari\", \"#e80220\",\n  \"mclaren\", \"#ff8001\",\n  \"aston_martin\", \"#229971\",\n  \"alpine\", \"#F363B9\", # Should be \"#0093cc\" but there's too much blue\n  \"williams\", \"#63c4ff\",\n  \"rb\", \"#6592ff\",\n  \"sauber\", \"#52e252\",\n  \"haas\", \"#b6babd\"\n) |&gt;\n  arrange(constructorRef)\n\ndrv_colours &lt;- ctr_colours |&gt;\n  merge(ctr_drvs) |&gt;\n  group_by(driverRef) |&gt;\n  arrange(constructorRef) |&gt;\n  slice_head(n = 1) |&gt; # alphabetical first for multi-team drivers i.e. Bearman\n  select(driverRef, colour) |&gt;\n  arrange(driverRef)\n\nf1data &lt;- races |&gt;\n  merge(results) |&gt;\n  merge(qualifying) |&gt;\n  merge(constructors) |&gt;\n  merge(drivers) |&gt;\n  merge(circuits) |&gt;\n  select(raceId, gp, year, round, circuitRef, constructorRef, driverRef, q1, q2, q3, quali_position, grid, position) |&gt;\n  mutate(position = ifelse(position == \"\\\\N\", NA, as.numeric(position)))\nIn the interests of being educational - and because Lord knows I hate editing - I am writing this post as I go, including any blind-alleys and debugging.\nLet’s start with some visualisations of the data we have.\nCode\nf2024 &lt;- f1data |&gt; filter(year == 2024)\n\nggplot(f2024) +\n  aes(x = position, fill = driverRef) +\n  facet_wrap(~driverRef) +\n  scale_fill_manual(values = drv_colours$colour) +\n  geom_histogram(alpha = 0.5, colour = \"black\", linewidth = 0.1, binwidth = 1, show.legend = FALSE)\n\n\n\n\n\n\n\n\nFigure 1: Histograms of driver performance in F1 2024.\nThe model goes like this. Points are simulated as a draw from a Poisson distribution parameterised by the driver performance and constructor performance.\n\\[\n\\text{points} \\sim \\text{Poisson}(e^{\\text{perf}_{\\text{drv}} + \\text{perf}_{\\text{ctr}}})\n\\]\nDriver performance is simulated as a draw from a normal distribution.\n\\[\n\\text{perf}_{\\text{drv}} \\sim \\text{Normal}(\\mu_{\\text{drv}}, \\sigma_{\\text{drv}}^2)\n\\]\nLikewise constructor performance is simulated as a draw from a normal distribution.\n\\[\n\\text{perf}_{\\text{ctr}} \\sim \\text{Normal}(\\mu_{\\text{ctr}}, \\sigma_{\\text{ctr}}^2)\n\\]\nWe will therefore have a likelihood function that is the product of the Poisson density of the points for each driver for each race. The number of parameters is very high, as a performance score for each driver and for each constructor."
  },
  {
    "objectID": "posts/f1-stan/f1.html#more-informative-priors",
    "href": "posts/f1-stan/f1.html#more-informative-priors",
    "title": "Latent F1 team and driver performance: fun with Bayesian models",
    "section": "More informative priors",
    "text": "More informative priors\nWhat makes some constructors better than others? Adrian Newey Money. Let’s reflect this in our priors.\n\\[\n\\lambda_{\\text{ctr}} \\sim \\text{Normal}(\\text{budget}_\\text{ctr}, \\sigma)\n\\]\n\nBudget data\nThis is a bit of a secret, so we have to estimate. We do know that the budget cap is 135M USD and it’s unlikely that anyone is operating below the cap. We also know that driver and top executive salaries are exempt from the cap. The top teams are spending about 100M USD on their drivers, and prior to the budget cap’s implementation likely invested in facilities that they can use for free (like wind tunnels).\nWith that in mind, I asked GPT-4o to search the web and estimate budgets for the teams. It came back with these results, which I sanity checked against this Blackbook Motorsport article.\n\n\nCode\nbudgets &lt;- tribble(\n  ~constructorRef, ~budget,\n  \"red_bull\", 400,\n  \"mercedes\", 400,\n  \"ferrari\", 400,\n  \"mclaren\", 250,\n  \"aston_martin\", 250,\n  \"alpine\", 200,\n  \"rb\", 150,\n  \"haas\", 150,\n  \"williams\", 150,\n  \"sauber\", 150\n) |&gt;\n  mutate(\n    norm_budget = 1 - budget / max(budget)\n  )\nkable(select(budgets, -norm_budget))\n\n\n\n\nTable 3: Constructor budget estimates (USD millions)\n\n\n\n\n\n\nconstructorRef\nbudget\n\n\n\n\nred_bull\n400\n\n\nmercedes\n400\n\n\nferrari\n400\n\n\nmclaren\n250\n\n\naston_martin\n250\n\n\nalpine\n200\n\n\nrb\n150\n\n\nhaas\n150\n\n\nwilliams\n150\n\n\nsauber\n150\n\n\n\n\n\n\n\n\nLet’s update our model and run Stan again.\ndata {\n  int&lt;lower=1&gt; n_ctrs;\n  int&lt;lower=1&gt; n_obs;\n  array[n_ctrs] int&lt;lower=1, upper=10&gt; prior_mus;\n  real&lt;lower=0&gt; prior_sd;\n  array[n_obs] int&lt;lower=1, upper=10&gt; ctrs;\n  array[n_obs] int&lt;lower=1, upper=10&gt; positions;\n}\nparameters {\n  array[n_ctrs] real&lt;lower=1, upper=10&gt; lambda;\n}\nmodel {\n  lambda ~ normal(prior_mus, prior_sd);\n  positions ~ poisson(lambda[ctrs]) T[1, 10];\n}\n\n\n\nCode\ndata_list_b &lt;- make_data_list(ctr_positions)\ndata_list_b$prior_mus &lt;- 1 + 10 * arrange(budgets, constructorRef)$norm_budget\ndata_list_b$prior_sd &lt;- 1.5\nfit.b &lt;- run_stan(data_list_b, model_file = \"f1.b.stan\")\n\nplot_posterior(fit.b)\n\n\n\n\n\n\n\n\nFigure 6: Posterior samples from the model with budget-based priors.\n\n\n\n\n\nThis hasn’t made much difference. McLaren has eased back towards Red Bull because their budget is smaller. You could say they did a lot better than Red Bull relative to their budget.\n\n\nCode\nsp &lt;- sample_posterior(fit.b)\nsp |&gt;\n  group_by(constructorRef) |&gt;\n  summarise(mu = mean(position), sigma = sd(position)) |&gt;\n  arrange(mu) |&gt;\n  kable(digits = 2)\n\n\n\n\nTable 4: Mean and SD from posterior sample.\n\n\n\n\n\n\nconstructorRef\nmu\nsigma\n\n\n\n\nmclaren\n2.06\n0.33\n\n\nred_bull\n2.07\n0.32\n\n\nferrari\n2.40\n0.32\n\n\nmercedes\n3.02\n0.36\n\n\naston_martin\n6.42\n0.56\n\n\nalpine\n7.80\n0.66\n\n\nhaas\n8.00\n0.67\n\n\nrb\n8.07\n0.67\n\n\nwilliams\n8.82\n0.63\n\n\nsauber\n9.12\n0.56\n\n\n\n\n\n\n\n\nThis model suggests there’s very little nothing in it between Red Bull and McLaren, though the posterior distribution is wider for the top three teams than the others so there is more uncertainty.\nWas budget a bad choice for the prior? Not necessarily: this model of constructor performance incorporates an important factor that isn’t clear from the results, but which should affect an objective assessment of which teams are better. Or to put it another way, if I were Max Verstappen I’d still rather be driving the Red Bull than the McLaren next year, because Red Bull’s budget suggests a more capable team overall. (Not to mention the increased wind tunnel time that third place in the constructors’ gets relative to the winner.)\n\n\nCode\nplot_posterior_with_prior &lt;- function(ctrs, prior_mus, prior_sd, fit) {\n  s_post &lt;- sample_posterior(fit) |&gt; mutate(sample = \"posterior\")\n\n  ctr_index &lt;- tibble(constructorRef = ctrs, constructorId = 1:length(ctrs))\n\n  s_prior &lt;- prior_mus |&gt;\n    lapply(\\(mu) rnorm(100000, mu, prior_sd)) |&gt;\n    lapply(as_tibble) |&gt;\n    imap(\\(t, i) mutate(t, constructorId = i, sample = \"prior\")) |&gt;\n    bind_rows() |&gt;\n    rename(position = value) |&gt;\n    merge(ctr_index)\n\n  df &lt;- bind_rows(s_prior, s_post)\n\n  ggplot(df) +\n    aes(x = position, fill = sample) +\n    facet_wrap(~constructorRef) +\n    geom_density(alpha = 0.5, linewidth = 0.1) +\n    scale_x_continuous(breaks = seq(1, 10, by = 2), limits = c(1, 10)) +\n    scale_fill_manual(values = c(\"dodgerblue1\", \"dodgerblue4\"))\n}\n\nplot_posterior_with_prior(ctrs, data_list_b$prior_mus, data_list_b$prior_sd, fit.b)\n\n\n\n\n\n\n\n\nFigure 7: Comparison of prior and posterior samples.\n\n\n\n\n\nPlotting the priors and posteriors together shows how McLaren over-performed and Mercedes continued to underperform.\n\n\nPast performance\nAnother, very easy way to set the priors is to consider the constructors’ performance over the previous seasons with the same regulations, in this case 2022 and 2023 (the ground-effect era). This is really easy to do - we would just take a weighted mean of the constructors’ finishing positions as mu for our priors, and perhaps set the variance proportional to the difference. Not particularly innovative or interesting though, so let’s skip it.\n\n\nCar-limited circuits\nLong-time F1 fans will know that at some tracks, the cars almost always finish in pairs, e.g. both Ferraris, then both Mercedes, and so on. Somehow at these circuits driver skill makes little difference; we could say the circuits are car-limited rather than driver-limited. That’s no fun to watch, but I wonder if it’s useful for establishing car performance?\nLet’s look back at historic race results to see if some circuits are more likely to finish in two-by-two order. We can score this by the median difference in position between finishing drivers.\n\n\nCode\ncircuits2024 &lt;- races |&gt;\n  merge(circuits) |&gt;\n  filter(year == 2024) |&gt;\n  select(circuitRef)\n\nposition_gaps &lt;- f1data |&gt;\n  filter(year &gt; 2014) |&gt; # hybrid era onwards, to try and keep it relevant to modern F1 car sizes\n  merge(circuits2024) |&gt; # circuits from 2024 only\n  select(year, round, circuitRef, constructorRef, driverRef, position) |&gt;\n  filter(position != \"\\\\N\") |&gt; # ignore DNFs\n  mutate(position = as.integer(position)) |&gt;\n  group_by(year, round, circuitRef, constructorRef) |&gt;\n  filter(n() == 2) |&gt; # must be a double finish\n  summarise(gap = max(position) - min(position), max_position = max(position)) |&gt;\n  group_by(circuitRef) |&gt;\n  mutate(median_gap = median(gap))\n\nggplot(position_gaps) +\n  aes(x = gap) +\n  facet_wrap(vars(circuitRef)) +\n  geom_histogram(fill = \"dodgerblue1\", alpha = 0.5, colour = \"black\", linewidth = 0.1, binwidth = 1) +\n  geom_vline(mapping = aes(xintercept = median_gap), colour = \"dodgerblue4\")\n\n\n\n\n\n\n\n\nFigure 8: Distribution of finishing position gaps between teammates by circuit, with median line.\n\n\n\n\n\nPerhaps unsurprisingly given that F1 isn’t a spec series, the gap between drivers of the same car is usually very low. I went back and forth on whether to discard cars that finished outside the top ten points-paying positions, as this often includes cars that were retired to save expense or were wounded but finished the race well below their potential for the sake of data gathering. In the end I kept them in because we need two-car finishes to calculate the statistics and setting a threshold would mean re-coding all the finish positions.\nIt turns out to make little difference anyway. The race with the highest probability of teammates finishing in consecutive positions is Suzuka, whether or not you apply a threshold.\n\n\nCode\nrounds2024 &lt;- races |&gt;\n  filter(year == 2024) |&gt;\n  merge(circuits) |&gt;\n  select(circuitRef, round) |&gt;\n  rename(round2024 = round)\n\np_gap_1 &lt;- position_gaps |&gt;\n  count(circuitRef, gap) |&gt;\n  group_by(circuitRef) |&gt;\n  mutate(p_gap = n / sum(n)) |&gt;\n  filter(gap == 1) |&gt;\n  ungroup() |&gt;\n  merge(rounds2024)\n\np_gap_1 |&gt;\n  slice_max(p_gap, n = 10) |&gt;\n  kable(digits = 2)\n\n\n\n\nTable 5: Circuits with highest probability of having teammates finishing together.\n\n\n\n\n\n\ncircuitRef\ngap\nn\np_gap\nround2024\n\n\n\n\nsuzuka\n1\n29\n0.48\n4\n\n\nalbert_park\n1\n19\n0.41\n3\n\n\nrodriguez\n1\n26\n0.39\n20\n\n\njeddah\n1\n10\n0.38\n2\n\n\nyas_marina\n1\n29\n0.36\n24\n\n\nimola\n1\n10\n0.33\n7\n\n\nhungaroring\n1\n26\n0.32\n13\n\n\nshanghai\n1\n15\n0.31\n5\n\n\nbahrain\n1\n25\n0.30\n1\n\n\ninterlagos\n1\n19\n0.30\n21\n\n\n\n\n\n\n\n\nHow did the cars line up at Suzuka in 2024, noting that this was round 4 of a 24-round championship?\n\n\nCode\nsuzuka2024 &lt;- f1data |&gt;\n  filter(year == 2024) |&gt;\n  filter(circuitRef == \"suzuka\") |&gt;\n  select(year, round, circuitRef, constructorRef, driverRef, quali_position, position)\n\nsuzuka2024 |&gt;\n  drop_na() |&gt;\n  group_by(constructorRef) |&gt;\n  filter(n() == 2) |&gt;\n  arrange(position) |&gt;\n  kable()\n\n\n\n\nTable 6: Results for Suzuka 2024 (two-car finishes only)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyear\nround\ncircuitRef\nconstructorRef\ndriverRef\nquali_position\nposition\n\n\n\n\n2024\n4\nsuzuka\nred_bull\nmax_verstappen\n1\n1\n\n\n2024\n4\nsuzuka\nred_bull\nperez\n2\n2\n\n\n2024\n4\nsuzuka\nferrari\nsainz\n4\n3\n\n\n2024\n4\nsuzuka\nferrari\nleclerc\n8\n4\n\n\n2024\n4\nsuzuka\nmclaren\nnorris\n3\n5\n\n\n2024\n4\nsuzuka\naston_martin\nalonso\n5\n6\n\n\n2024\n4\nsuzuka\nmercedes\nrussell\n9\n7\n\n\n2024\n4\nsuzuka\nmclaren\npiastri\n6\n8\n\n\n2024\n4\nsuzuka\nmercedes\nhamilton\n7\n9\n\n\n2024\n4\nsuzuka\nhaas\nhulkenberg\n12\n11\n\n\n2024\n4\nsuzuka\naston_martin\nstroll\n16\n12\n\n\n2024\n4\nsuzuka\nhaas\nkevin_magnussen\n18\n13\n\n\n2024\n4\nsuzuka\nalpine\nocon\n15\n15\n\n\n2024\n4\nsuzuka\nalpine\ngasly\n17\n16\n\n\n\n\n\n\n\n\nThe cars are quite well in order here, if you ignore the gulf between Alonso and Stroll. It looks like Suzuka is really a car-limited track. Incidentally, this would suggest that in round 4, McLaren and Mercedes were quite evenly matched.\n\nCounter-arguments\nAre we sure Suzuka is a car-limited track?\n\nSuzuka is a track “drivers love”! Surely it’s a more skill-dependent track?\nI would have expected so! Suzuka is described as a technical track, with minimal room for error. Perhaps there aren’t as many potential lines that a driver could take to distinguish themselves. It’s also a good track for overtaking, so any driver that does qualify out of position is able to recover. Or perhaps the reason drivers love it is because they get to drive their cars on the limit: the limit of the car’s performance.\nEither way, statistically drivers are more likely to end up in pairs in Suzuka than anywhere else.\n\n\nSuzuka was an early round. Perhaps car performance converges over the season?\nOn the one hand we’d expect to see cars start at different levels but converge throughout the season, as the teams spot innovations on other cars that they can copy. Or secret design documents that they can copy, ahem. On the other hand, constructors in a tight battle would be motivated to develop their cars further - for example in 2022 when Red Bull overhauled an early Ferrari advantage.\nAs it happens, historically Suzuka has been mid-to-late season, which suggests that being an early round in 2024 isn’t the reason for being car-limited.\n\n\nCode\nraces |&gt;\n  filter(year &gt;= 2014) |&gt;\n  group_by(year) |&gt;\n  mutate(n_rounds = n_distinct(round)) |&gt;\n  ungroup() |&gt;\n  merge(circuits) |&gt;\n  filter(circuitRef == \"suzuka\") |&gt;\n  select(year, round, n_rounds) |&gt;\n  arrange(year, round) |&gt;\n  kable()\n\n\n\n\nTable 7: Suzuka’s placement in each season\n\n\n\n\n\n\nyear\nround\nn_rounds\n\n\n\n\n2014\n15\n19\n\n\n2015\n14\n19\n\n\n2016\n17\n21\n\n\n2017\n16\n20\n\n\n2018\n17\n21\n\n\n2019\n17\n21\n\n\n2022\n18\n22\n\n\n2023\n16\n22\n\n\n2024\n4\n24\n\n\n\n\n\n\n\n\nHowever the fact that it was an early round in 2024 does mean that car performance will have changed over the season. Suzuka can only ever be a point-in-time assessment.\n\n\nAre we sure that consecutive finishes aren’t indicative of similar driver skill levels?\nThe top teams can also hire the best drivers, is it not possible that we’re still seeing the drivers finishing in skill order? I.e. it’s actually driver-limited, but the drivers happen to be mostly paired with similarly-fast drivers. Alonso and Stroll being the clear exception in the 2024 race results.\nOn the other hand, between 2023 and 2024 there were no line-up changes among the top 5 teams, so we can easily compare across the two years.\n\n\nCode\nsuzuka2023 &lt;- f1data |&gt;\n  filter(year == 2023) |&gt;\n  filter(circuitRef == \"suzuka\") |&gt;\n  select(year, round, circuitRef, constructorRef, driverRef, quali_position, position)\n\nsuzuka2023 |&gt;\n  drop_na() |&gt;\n  group_by(constructorRef) |&gt;\n  filter(n() == 2) |&gt;\n  arrange(position) |&gt;\n  kable()\n\n\n\n\nTable 8: Results for Suzuka 2024 (two-car finishes only)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyear\nround\ncircuitRef\nconstructorRef\ndriverRef\nquali_position\nposition\n\n\n\n\n2023\n16\nsuzuka\nmclaren\nnorris\n3\n2\n\n\n2023\n16\nsuzuka\nmclaren\npiastri\n2\n3\n\n\n2023\n16\nsuzuka\nferrari\nleclerc\n4\n4\n\n\n2023\n16\nsuzuka\nmercedes\nhamilton\n7\n5\n\n\n2023\n16\nsuzuka\nferrari\nsainz\n6\n6\n\n\n2023\n16\nsuzuka\nmercedes\nrussell\n8\n7\n\n\n2023\n16\nsuzuka\nalpine\nocon\n14\n9\n\n\n2023\n16\nsuzuka\nalpine\ngasly\n12\n10\n\n\n2023\n16\nsuzuka\nalphatauri\nlawson\n11\n11\n\n\n2023\n16\nsuzuka\nalphatauri\ntsunoda\n9\n12\n\n\n2023\n16\nsuzuka\nhaas\nhulkenberg\n18\n14\n\n\n2023\n16\nsuzuka\nhaas\nkevin_magnussen\n15\n15\n\n\n\n\n\n\n\n\nThe order of constructors is very different, despite all drivers being the same except for Alpha Tauri/RB. Just for fun, here’s a side-by-side comparison for each driver for 2023 and 2024.\n\n\nCode\ns24 &lt;- suzuka2024 |&gt;\n  select(constructorRef, driverRef, position) |&gt;\n  rename(position2024 = position)\n\ns23 &lt;- suzuka2023 |&gt;\n  # correct to the new names for joining\n  # (yes I could have used constructorId, but opaque IDs are so tedious to handle)\n  mutate(constructorRef = ifelse(constructorRef == \"alphatauri\", \"rb\", constructorRef)) |&gt;\n  mutate(constructorRef = ifelse(constructorRef == \"alfa\", \"sauber\", constructorRef)) |&gt;\n  select(constructorRef, driverRef, position) |&gt;\n  rename(position2023 = position)\n\nmerge(s23, s24, all = TRUE) |&gt;\n  arrange(position2023) |&gt;\n  kable()\n\n\n\n\nTable 9: Drivers’ results for Suzuka in 2023 and 2024\n\n\n\n\n\n\nconstructorRef\ndriverRef\nposition2023\nposition2024\n\n\n\n\nred_bull\nmax_verstappen\n1\n1\n\n\nmclaren\nnorris\n2\n5\n\n\nmclaren\npiastri\n3\n8\n\n\nferrari\nleclerc\n4\n4\n\n\nmercedes\nhamilton\n5\n9\n\n\nferrari\nsainz\n6\n3\n\n\nmercedes\nrussell\n7\n7\n\n\naston_martin\nalonso\n8\n6\n\n\nalpine\nocon\n9\n15\n\n\nalpine\ngasly\n10\n16\n\n\nrb\nlawson\n11\nNA\n\n\nrb\ntsunoda\n12\n10\n\n\nsauber\nzhou\n13\nNA\n\n\nhaas\nhulkenberg\n14\n11\n\n\nhaas\nkevin_magnussen\n15\n13\n\n\naston_martin\nstroll\nNA\n12\n\n\nrb\nricciardo\nNA\nNA\n\n\nred_bull\nperez\nNA\n2\n\n\nsauber\nbottas\nNA\n14\n\n\nwilliams\nalbon\nNA\nNA\n\n\nwilliams\nsargeant\nNA\n17\n\n\n\n\n\n\n\n\n\n\nDon’t certain tracks favour certain cars (e.g. due to having fewer slow corners)?\nThis is a valid objection. We can only really say that this reflects car performance at tracks like Suzuka. Suzuka is a high-speed, high-downforce track with more high-speed than low-speed corners. You can kind of tell by looking that only corners 2, 9, 11, and 14 are slow.\n\n\n\nSuzuka Circuit Layout\n\n\nData and image from the F1 Formbook.\nSo perhaps another way of looking at it is that low-speed corners are where drivers make the most difference. That aligns with the excellent recent analysis by Mark Hughes where he explains how drivers like Leclerc and Hamilton make up their time in slow corners.\nNaturally certain cars will also be better at slow corners, and that’s not represented at Suzuka. We might be able to gain a better understanding with sector times, but sadly I don’t have this data.\n\n\n\n\nSuzuka-based priors\nAt this point I’m convinced enough that Suzuka is a useful indicator of relative car performance to be a prior, so let’s take the position of each team’s fastest car round Suzuka as constructor performance priors. Luckily there weren’t any double-DNFs - at least one car from each team finished - so every team is represented.\nThe Stan model is the same, we’re just injecting different prior mus and SDs.\n\n\nCode\nsuzuka_positions &lt;- suzuka2024 |&gt;\n  drop_na() |&gt;\n  group_by(constructorRef) |&gt;\n  summarise(position = min(position)) |&gt;\n  mutate(position = rank(position)) |&gt;\n  arrange(constructorRef)\n\ndata_list_suzuka &lt;- make_data_list(ctr_positions)\ndata_list_suzuka$prior_mus &lt;- suzuka_positions$position\ndata_list_suzuka$prior_sd &lt;- 1\n\nfit_suzuka &lt;- run_stan(data_list_suzuka, \"f1.b.stan\")\n\nplot_posterior(fit_suzuka)\n\n\n\n\n\n\n\n\nFigure 9: Posterior samples for the Suzuka-position priors.\n\n\n\n\n\nIt’s not dramatically different. It shouldn’t be, the model and data are the same and the priors are very similar to the budget-based priors - the fun was in figuring out that Suzuka is car-limited.\n\n\nCode\nplot_posterior_with_prior(ctrs, data_list_suzuka$prior_mus, data_list_suzuka$prior_sd, fit_suzuka)\n\n\n\n\n\n\n\n\nFigure 10: Posterior samples alongside the Suzuka-position priors.\n\n\n\n\n\nMost teams didn’t move far from their priors. I’d draw similar conclusions to before: the McLaren was fastest, Red Bull was a close second, and Ferrari were not far behind.\n\n\nCode\nsp &lt;- sample_posterior(fit_suzuka)\nsp |&gt;\n  group_by(constructorRef) |&gt;\n  summarise(mu = mean(position), sigma = sd(position)) |&gt;\n  arrange(mu) |&gt;\n  kable(digits = 2)\n\n\n\n\nTable 10: Mean and SD from posterior sample, with Suzuka priors.\n\n\n\n\n\n\nconstructorRef\nmu\nsigma\n\n\n\n\nred_bull\n2.01\n0.31\n\n\nmclaren\n2.05\n0.32\n\n\nferrari\n2.42\n0.32\n\n\nmercedes\n3.39\n0.39\n\n\naston_martin\n6.07\n0.49\n\n\nrb\n7.47\n0.56\n\n\nhaas\n7.77\n0.58\n\n\nalpine\n8.55\n0.64\n\n\nsauber\n9.08\n0.52\n\n\nwilliams\n9.39\n0.44"
  },
  {
    "objectID": "posts/f1-stan/f1.html#use-of-positions-rather-than-points",
    "href": "posts/f1-stan/f1.html#use-of-positions-rather-than-points",
    "title": "Latent F1 team and driver performance: fun with Bayesian models",
    "section": "Use of positions rather than points",
    "text": "Use of positions rather than points\nPhillips makes a good argument for a non-linear reward function, for it punishes drivers more fairly for DNFs. That would be an improvement here."
  },
  {
    "objectID": "posts/f1-stan/f1.html#no-assignment-of-dnf-blame",
    "href": "posts/f1-stan/f1.html#no-assignment-of-dnf-blame",
    "title": "Latent F1 team and driver performance: fun with Bayesian models",
    "section": "No assignment of DNF blame",
    "text": "No assignment of DNF blame\nI’ve just discarded all that data, which flatters the DNF-prone drivers. Though I note that the drivers near the bottom of the performance rankings in my model were more DNF-prone in 2024 anyway, so perhaps there’s a correlation between low performance and DNFs, or those drivers had some races where they wounded the car but finished anyway."
  },
  {
    "objectID": "posts/f1-stan/f1.html#only-a-single-season-considered",
    "href": "posts/f1-stan/f1.html#only-a-single-season-considered",
    "title": "Latent F1 team and driver performance: fun with Bayesian models",
    "section": "Only a single season considered",
    "text": "Only a single season considered\nWith more data for each driver - across different cars - the model would be better able to distinguish driver and car performance. Note that it would be necessary to model each constructor by year because the car performance varies so much.\nThis suggests that we might benefit from having multiple representations of car performance within each year, e.g. splitting each team into A and B specs according to when they brought their largest upgrade. The challenge with that (apart from the tedious business of determining the break points) is that we’d have less data for each car and also there are perhaps several step changes in performance throughout the season."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is the personal blog of Chris Bowdon. Hi! By day I am the CTO of Polecat Intelligence, working with AI and Big Data, but you will be shocked to learn that I am also into maths and code. The posts here cover the bits and pieces I do that are not for work.\nIf you’ve spotted an error or have a question about the content on this site, I’d be grateful if you could raise a GitHub issue. If you’d like to get in touch about anything else, please look me up on LinkedIn."
  }
]