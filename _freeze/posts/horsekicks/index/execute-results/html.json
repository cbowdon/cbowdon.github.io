{
  "hash": "454de31e966f1f94f7e9a81dd138b90f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Introduction to Generalised Linear Models with Prussian Horse Kicks\"\ndate: \"2025-08-03\"\ncategories: [r, stats, glms]\ncode-fold: false\nwarning: false\n---\n\n\n\nTime to finally patch a hole in the leaky roof of my knowledge: what are Generalised Linear Models anyway?\n\n## Groundwork: what are Linear Models anyway?\n\nGeneralised Linear Models (GLMs) are a short step from Linear Models, _provided you have the right understanding of linear models_. There's also some stats jargon to get a handle on. So we'll start with an intuitive explanation of linear models.\n\nWith a linear model, we have some quantity (the response variable) that we're trying to predict from some other quantities (the predictors). The predictors are knowable things that you can measure with good precision, such as age and weight. The response variable is something with natural variation and/or measurement uncertainty, like maximum jump height. Different people of the same age and weight will have different maximum jumps. For every age and weight combination, we aim to predict the average jump height.\n\nThe natural variation usually (but not always) follows a _normal_ distribution for given predictors. That is to say, we should expect a bell curve of jump heights for people who are 80 years of age and weigh 80kg. For people who are 18 years of age and weight 60kg, we should expect a bell curve with a different (probably higher) mean but the same variance. Having the the variance in jump height _not_ change for different ages and weights is important, we'll come back to that later.\n\nWhat we do in a linear model is devise a weighted sum of our predictors (a linear combination, in the lingo) that best predicts the mean of the bell curve. Here's the general form of it:\n\n$$\nE[Y] = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 ...\n$$\n\n$X_1$ could be age and $X_2$ could be weight. The $\\beta$ values are the intercept and coefficients that we learn.\n\nHere $E[Y]$ means the _expectation_ of Y, which in this case is just the mean. Strictly speaking, we should say that we're predicting the mean _conditioned on the predictors_ because it's not the mean of all maximum jump height observations, it's the mean of the maximum jump heights for a particular age and weight. So we ought to write $E[Y | X]$.\n\nTo determine those coefficients we use an algorithm like Ordinary Least Squares (OLS) which works to find the coefficients that give the means with the smallest (squared) residuals, i.e. minimal squared distance between the observations and the mean for any given predictors. We don't need to know too much about how this works today, just that it's an efficient and useful.\n\n## Next level: the GLM\n\nWith that under our belt, GLMs aren't so scary. There are just two things to recognise.\n\nFirst, a normal distribution isn't always appropriate for the natural variation. What if rather than jump height, our response variable was something discrete, like number of pets? You can't have a non-integer or negative number of pets. We'd want to swap the normal distribution for a discrete distribution from the same exponential family, most likely a Poisson distribution.\n\nThat leads directly to the second thing: the mean of a Poisson distribution must be positive, so we need to transform the result of that weighted sum in some way that maps it into the valid range of Poisson means. This transformation is called the _link function_. The form of this would be:\n\n$$\ng(\\mu) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 ...\n$$\n\nwhere $g$ is the link function, and $\\mu$ is the mean of the target distribution. If we had a normal distribution and no need to transform the result of the weighted sum, $g$ would simply be the identity function. For a Poisson distribution, the usual choice is $log$, because it's the inverse of a function $exp$ that maps any number to positive number.\n\nFor different types of data, you'd choose a different type of distribution:\n\n- Poisson for counts\n- Binomial for binary probabilities\n- Gamma for positive continuous data\n- Normal for normally distributed data\n\nEach of these has a \"canonical\" (standard) link function:\n\n- Poisson: log\n- Binomial: logit\n- Gamma: inverse\n- Normal: identity\n\nYou can choose another link function, but let's leave that for another day.\n\nRemember that the linear model assumed the variance was constant? GLMs don't need this, because for distributions other than the normal distribution, the variance is a function of the mean. The technical term for this property is heteroscedasticity (whereas a constant mean is homoscedasticity).\n\nOne more important consequence: because of the link function, we don't necessarily have a mean of the response variable that varies linearly with the predictors. In other words we can model non-linear relationships, which is of course _powerful_.\n\n## Play time\n\nTime to learn by playing. We can use a quirky dataset compiled by Russian statistician Ladislaus von Bortkiewicz in the late 1800s: deaths by horse kick per year for regiments in the Prussian army. In February 2025 Antony Unwin and Bill Venables updated the dataset with additional data and realised von Bortkiewicz's original dream of [publishing it to CRAN](https://cloud.r-project.org/web/packages/Horsekicks/vignettes/hkdeaths.html).\n\n\n\n::: {#tbl-hkdeaths .cell tbl-cap='Sample of the horse kick data (tidied).'}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(Horsekicks)\n\nhk.tidy <- hkdeaths |>\n  pivot_longer(\n    c(kick, drown, fall),\n    names_to = \"death.type\",\n    values_to = \"death.count\"\n  )\n\nhk.tidy |> head() |> kable()\n```\n\n::: {.cell-output-display}\n\n\n| year|corps | regiments| NCOs| vonB_kick|death.type | death.count|\n|----:|:-----|---------:|----:|---------:|:----------|-----------:|\n| 1875|G     |         8|    0|         0|kick       |           0|\n| 1875|G     |         8|    0|         0|drown      |           3|\n| 1875|G     |         8|    0|         0|fall       |           0|\n| 1875|I     |         6|    0|         0|kick       |           0|\n| 1875|I     |         6|    0|         0|drown      |           5|\n| 1875|I     |         6|    0|         0|fall       |           0|\n\n\n:::\n:::\n\n\n\nUnwin and Venables added deaths by drowning, and deaths by falling off a horse among other improvements. The data cover 14 corps, each with a fixed number of regiments, over 33 years. A regiment is about 500 soldiers, as far as I can tell. I have no idea where Prussia was or where it's gone.\n\nLet's explore the data visually. Plotting deaths over time shows that equestrian misadventures seem quite stable, but deaths by drowning did reduce. We will fit a GLM to these time series.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprussian.colours <- c(\"#333333\", \"#F9BE11\", \"#BE0007\")\n\nhk.year <- hk.tidy |>\n  group_by(year, death.type) |>\n  summarise(death.count = sum(death.count))\n\nggplot(hk.year) +\n  aes(x = year, y = death.count, group = death.type, colour = death.type) +\n  geom_line() +\n  scale_colour_manual(values = prussian.colours, name = \"Death type\") +\n  labs(x = \"Year\", y = \"Number of deaths\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![Accidental death by type over time.](index_files/figure-html/fig-deaths-over-time-1.png){#fig-deaths-over-time width=672}\n:::\n:::\n\n\n\nNote that for each death type and each year we actually have a distribution across all regiments, not just a single number. We could choose to plot the distributions as a series of box plots.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(hk.tidy) +\n  aes(x = year, y = death.count, group = year, fill = death.type) +\n  facet_wrap(~death.type, dir = \"v\") +\n  geom_boxplot(alpha = 0.5) +\n  scale_fill_manual(values = prussian.colours, name = \"Death type\") +\n  theme_minimal() +\n  labs(x = \"Year\", y = \"Number of deaths\")\n```\n\n::: {.cell-output-display}\n![Distributions of deaths for each year and death type.](index_files/figure-html/fig-boxplots-1.png){#fig-boxplots width=672}\n:::\n:::\n\n\n\nTime for that model. Since you can't literally be kicked half to death, at least not as far as _Preussische Statistik_ was concerned, all the death statistics are integer counts. Therefore the obvious choice for the distribution is Poisson. We don't have a reason to provide a different link function, so we just take the canonical link function for Poisson (log).\n\nR makes this trivial to do. `glm` is a built-in function:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm(\n  formula = drown ~ year, # i.e. drownings are the response variable, year is the predictor\n  family = poisson,\n  data = hkdeaths\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:  glm(formula = drown ~ year, family = poisson, data = hkdeaths)\n\nCoefficients:\n(Intercept)         year  \n   44.05820     -0.02256  \n\nDegrees of Freedom: 461 Total (i.e. Null);  460 Residual\nNull Deviance:\t    854.3 \nResidual Deviance: 766.9 \tAIC: 2170\n```\n\n\n:::\n:::\n\n\n\nThe result is an intercept (which we don't care much about) and a coefficient for the year of -0.022, which tells us that the model predicts that every year the mean number of drownings is 2.2% lower. There's a little gotcha in that interpretation: we need to remember to invert the link function. It's actually predicting that the mean number of drownings is $exp(-0.022)$ times lower, but that's a small number and $exp$ behaves quite linearly with small numbers: $exp(-0.022) \\approx 0.978$.\n\n`ggplot2` makes all the above even easier by allowing us to bung the above model into `stat_smooth` and run it for each death type.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(hk.year) +\n  aes(x = year, y = death.count, group = death.type, colour = death.type) +\n  geom_point() +\n  stat_smooth(\n    method = \"glm\",\n    formula = y ~ x,\n    method.args = list(family = poisson)\n  ) +\n  scale_colour_manual(values = prussian.colours, name = \"Death type\") +\n  theme_minimal() +\n  labs(x = \"Year\", y = \"Number of deaths\")\n```\n\n::: {.cell-output-display}\n![Death by type over time, with GLM fits.](index_files/figure-html/fig-deaths-over-time-fitted-1.png){#fig-deaths-over-time-fitted width=672}\n:::\n:::\n\n\n\nEt voila, we even get shaded confidence intervals. That's really all there was to it!",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}