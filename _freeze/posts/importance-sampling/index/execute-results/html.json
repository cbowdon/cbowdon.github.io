{
  "hash": "9f639efab6208ea3526072059d284393",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Importance Sampling example\"\nformat:\n    html:\n        embed-resources: true\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4          ✔ readr     2.1.5     \n✔ forcats   1.0.0          ✔ stringr   1.5.1     \n✔ ggplot2   3.5.1          ✔ tibble    3.2.1.9040\n✔ lubridate 1.9.4          ✔ tidyr     1.3.1     \n✔ purrr     1.0.2          \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nset.seed(42)\n```\n:::\n\n\n\nThe aim of this post is to teach myself basic Bayesian methods. I'll set up a toy example problem and solve it.\n\nLet's assume we have a natural event that happens with a variable periodicity e.g. a woman's menstrual cycle. I'm drawing from [this excellent blog](https://www.sumsar.net/blog/2015/11/a-bayesian-model-to-calculate-whether-my-wife-is-pregnant/), but doing a much simpler model because I am but a baby.\n\nThe aim here is to see if we can find the mean period of the specific woman's menstrual cycle given some samples and priors for the average cycle. First, priors from the blog linked above:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# a normal distribution for the mean days between period starts\nprior_mean_mu <- 27.7\nprior_mean_sd <- 2.4\n# a half-normal distribution for the SD of days between period starts\nprior_sd_mu <- 1.6\nprior_sd_sd <- 2.05\n```\n:::\n\n\n\nWe will set the true mean and SD (the params, $\\theta$, that we're looking for) and simulate some sample data, $d$.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrue_mu <- 26.3\ntrue_sd <- 1.9\n\nsample_periods <- rnorm(12, true_mu, true_sd)\n```\n:::\n\n\n\nFor importance sampling we need a likelihood function, $L(d|\\theta)$. Make the model a normal distribution and this is simply the probability density. We take the log of the likelihood because the numbers could be very small and subject to numerical error.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlog_likelihood <- function(sample_periods, mean_period, sd_period) {\n  sum(dnorm(sample_periods, mean_period, sd_period, log = TRUE))\n}\n```\n:::\n\n\n\nA little sanity check: the likelihood of the true parameters should be greater than the likelihood of the priors.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlog_likelihood(sample_periods, true_mu, true_sd) > log_likelihood(sample_periods, prior_mean_mu, prior_mean_sd)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] FALSE\n```\n\n\n:::\n:::\n\n\n\n## Importance sampling to fit the model\n\n> Importance sampling is a Monte Carlo method that is very easy to setup and that can work well if (1) the parameters space is small and (2) the priors are not too dissimilar from the posterior. \n\nI know that both these conditions are true in my toy example.\n\nFirst we must sample from the prior.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn_samples <- 100000\nprior <- tibble(\n  mu_period = rnorm(n_samples, prior_mean_mu, prior_mean_sd),\n  sd_period = abs(rnorm(n_samples, 0, prior_sd_sd))\n)\n\nggplot(prior) +\n  aes(x = mu_period) +\n  geom_histogram()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(prior) +\n  aes(x = sd_period) +\n  geom_histogram()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\nThen we weight each draw by its likelihood.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nweights <- sapply(1:n_samples, function(i) {\n  log_likelihood(sample_periods, prior$mu_period[i], prior$sd_period[i])\n})\n```\n:::\n\n\n\nThen we resample the prior by this weighting.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# n.b. sample will normalise the weights\nposterior <- prior[sample(n_samples, replace = TRUE, prob = exp(weights)), ]\n\ndf <- bind_rows(\n  mutate(prior, dist = \"prior\"),\n  mutate(posterior, dist = \"posterior\")\n) |>\n  pivot_longer(c(mu_period, sd_period), names_to = \"param\", values_to = \"value\")\n\nggplot(df) +\n  aes(x = value, fill = dist) +\n  facet_wrap(vars(param), scales = \"free\") +\n  geom_histogram(alpha = 0.8, position = \"identity\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\nHey Doctor, would you look at that! The posterior is much tighter and closer to the true values.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nposterior |> summarise(mu = mean(mu_period), sd = mean(sd_period))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 2\n     mu    sd\n  <dbl> <dbl>\n1  27.7  1.88\n```\n\n\n:::\n:::\n\n\n\nHow does that compare to simply taking the mean and SD of the sample data?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(\n    mu = mean(sample_periods),\n    sd = sd(sample_periods)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 2\n     mu    sd\n  <dbl> <dbl>\n1  27.7  1.75\n```\n\n\n:::\n:::\n\n\n\nIt's not highly convincing in this case. I ran this a few times and it seemed that the SD estimation seemed to be closer to the true value than the SD of the sample, but the mean was rarely different. This is probably because it's a very simple model and importance sampling is rather overkill.\n\n## Proposal distribution isn't necessarily the prior\n\nIn Bååth's [other blog](https://www.sumsar.net/blog/2013/12/shaping_up_laplace_approximation/) on importance sampling he explains that we are looking at the ratio of the likelihood of getting our sample from the target distribution relative to getting it from the proposal distribution (then normalised to form a new estimate of the target distribution).\n\nIf the proposal distribution is thinner at the ends than the target distribution, this is suboptimal because we will have fewer samples there. So a T distribution is a useful proposal for a normal target, because it has fatter tails (as controlled by the `df` degrees of freedom parameter).\n\nWe could choose a very sloppy proposal like a uniform distribution, but in that case we are suboptimal in terms of how quickly we explore the space. We would have a relatively high number of samples in the tails compared to the peak. Bååth has a neat method of using a Laplacian approximation as a proposal.\n\n## Aki, another example\n\nNow an example from [this blog post](https://david-salazar.github.io/posts/bayesian-statistics/2020-06-27-bayesian-data-analysis-week-4-importance-sampling.html). This one uses a T as the proposal.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\napprox_samples <- rt(100, df = 3)  # T with 3 deg free is our approximation to posterior\napprox_density <- dt(approx_samples, df = 3, log = TRUE)  # get the prob density for each\nunnormalised_posterior <- dnorm(approx_samples, log = TRUE, sd = sqrt(3))  # convert to actual posterior\n\n# weights are then...\nlog_imp_weights <- unnormalised_posterior - approx_density\ndata.frame(log_imp_weights) %>% \n  ggplot(aes(exp(log_imp_weights))) +\n  geom_density(fill = \"dodgerblue4\", alpha = 0.5) +\n  labs(title = \"Importance weights\",\n       subtitle = \"Approximating a normal with a t distribution\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# normalise...\nweights <- exp(log_imp_weights) / sum(exp(log_imp_weights))\n\nmean_estimate <- sum(approx_samples * weights)\n```\n:::",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}