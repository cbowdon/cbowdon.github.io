{
  "hash": "69cc72f43eb1fc726ca7063f801bd115",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Gaussian Processes in R\"\ndate: \"2025-01-25\"\ncategories: [gaussian, bayesian, stan, r]\n---\n\n\n\nI stumbled upon Gaussian Processes when looking into how the MIPRO prompt optimisation algorithm works and felt compelled to learn more about it. It's a very nifty application of Bayesian modelling to analyse functions where data points are hard to get.\n\nThis post is based on [a great lesson by Carpentries](https://carpentries-incubator.github.io/statistical-probabilistic-programming-r/instructor/gaussian-processes.html), which is part of a longer series on probabilistic programming in R. Very worth checking out if you want to know more.\n\n# WTH are Gaussian Processes?\n\nImagine your data $y$ are noisy outputs of a function of $x$, which is the function you're trying to model.\n\n$$\ny \\sim N(f(x), \\sigma^2)\n$$\n\nWe don't know $f(x)$, so we imagine that there's some distribution $GP$ that generates functions and the $f(x)$ we've observed is one function drawn from that distribution. $GP$ (the Gaussian Process) is parameterised by a mean, $\\mu$ and a kernel, $K$. It's a multivariate normal distribution, i.e. the normal distribution generalised to multiple dimensions.\n\n$$\nf(x) \\sim GP(\\mu, K)\n$$\n\nWhat's the point of this? Well it allows us to use our Bayesian toolbox (hello again [Stan](https://mc-stan.org)) to refine our estimate of $GP$, and so the samples we draw from a posterior of $GP$ are closer to the true $f(x)$.\n\n## The Kernel $K$\n\n$K$ is the most interesting parameter, being the covariance of the Gaussian Process. There's some art to what function is chosen for $K$, though I don't want to get hung up on that at this point. Suffice it to say that the popular choice is squared exponential covariance, which is parameterised by $\\alpha$ and $\\lambda$.\n\n$$\nK(x, x') = \\alpha^2 e^{\\frac{(x - x')^2}{2\\lambda}}\n$$\n\nIf we implement the kernel and draw from $GP$, we can see some potential functions. Each draw from $GP$ will give us a vector of $f(x)$ for the input $x$ vector. We get a discrete approximation of the continuous function.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\nlibrary(tidyverse)\n\nsq_exp_cov <- function(x, lambda, alpha) {\n  n <- length(x)\n\n  K <- matrix(0, n, n)\n\n  for (i in 1:n) {\n    for (j in 1:n) {\n      diff <- sqrt(sum((x[i] - x[j])^2))\n      K[i, j] <- alpha^2 * exp(-diff^2 / (2 * lambda^2))\n    }\n  }\n\n  K\n}\n\nx <- 0:100\n\nGP <- function(alpha, lambda, n_samples = 10) {\n  K <- sq_exp_cov(x, lambda, alpha)\n  mat <- MASS::mvrnorm(\n    n_samples,\n    mu = rep(0, length(x)), # Set the average level of the process to 0\n    Sigma = K\n  )\n  t(mat) |>\n    data.frame() |>\n    mutate(alpha = alpha, lambda = lambda, x = x) |>\n    pivot_longer(c(-x, -alpha, -lambda), names_to = \"sample\", values_to = \"y\")\n}\n\ngrd <- expand.grid(alpha = c(0.5, 1, 2), lambda = c(1, 10, 100))\nsamples <- map2(grd$alpha, grd$lambda, GP) |> bind_rows()\n\nlabeller <- function(variable, value) {\n  if (variable == \"alpha\") {\n    sprintf(\"α = %s\", value)\n  } else {\n    sprintf(\"λ = %s\", value)\n  }\n}\n\nggplot(samples) +\n  aes(x = x, y = y, group = sample, colour = sample) +\n  facet_wrap(~ alpha * ~lambda, labeller = labeller) +\n  geom_line(show.legend = FALSE) +\n  labs(title = \"Samples of f(x) from GP\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/fig-kernels-1.png){#fig-kernels width=672}\n:::\n:::\n\n\n\nVarying $\\alpha$ and $\\lambda$ gives us an idea of how these hyperparameters change the functions we draw. $\\alpha$ controls the standard deviation and $\\lambda$ controls the correlation between $x$ points.\n\n# Gaussian Process Regression - using data to refine $GP$\n\nImagine we've observed some (noisy) data from a mystery function.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf6 <- data.frame(\n  x = c(-2.76, 2.46, -1.52, -4.34, 4.54, 1),\n  y = c(-0.81, -0.85, 0.76, -0.41, -1.48, 0.2)\n)\n```\n:::\n\n\n\nWe intend to predict the values of $y$ at some other $x$ beyond the ones we've observed, to see what $f(x)$ might look like. We'll choose [-5, 5] as the domain.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx_pred <- seq(-5, 5, by = 0.25)\n# you'll get a smoother approximation for smaller increments, but at increase computation cost\n```\n:::\n\n\n\nThen we model it all in Stan. Personally I find Stan models easier to read in the order `model`, `data`, `parameters`.\n\n```stan\ndata {\n  // Observed data\n  int n_data;\n  array[n_data] real x_data;\n  array[n_data] real y_data;\n  \n  // Observation error\n  real<lower=0> sigma;\n  \n  // x values for which we aim to predict y\n  int n_pred;\n  array[n_pred] real x_pred;\n  \n  // Hyperparameters for the kernel\n  real alpha;\n  real lambda;\n}\ntransformed data {\n  // We join the x observations and desired x prediction points\n  int n = n_data + n_pred;\n  \n  array[n] real x;\n  x[1 : n_data] = x_data;\n  x[(n_data + 1): n] = x_pred;\n  \n  // We calculate the Kernel values for all x\n  matrix[n, n] K;\n  K = gp_exp_quad_cov(x, alpha, lambda);\n  \n  // Add nugget on diagonal for numerical stability\n  for (i in 1 : n) {\n    K[i, i] = K[i, i] + 1e-6;\n  }\n}\nparameters {\n  // This is what we want to estimate\n  vector[n] f;\n}\nmodel {\n  // Likelihood is tested against the observations\n  y_data ~ normal(f[1 : n_data], sigma);\n\n  // f is sampled from GP\n  // The domain of the GP prior is all x\n  // We assume the mean is always 0\n  f ~ multi_normal(rep_vector(0, n), K);\n}\n\n``` \n\nThe `model` section gives the high-level view of what we're doing here: simulating those equations from earlier. The main thing to understand is that the domain of GP is both the $x$ observations and the additional points. We're interested in the model's uncertainty about what $f$ looks like at the points we haven't observed.\n\nHere's the R code to run that model. I prefer to use `CmdStanR` - [see here](../using-stan-from-r/) for more on that and some general tips.\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n\n```\nThis is cmdstanr version 0.8.1\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n- CmdStan path: /Users/cbowdon/micromamba/envs/rland/bin/cmdstan\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n- CmdStan version: 2.36.0\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n variable   mean median   sd  mad     q5    q95 rhat ess_bulk ess_tail\n     lp__ -25.89 -25.62 4.71 4.58 -34.20 -18.74 1.00     1538     2577\n     f[1]  -0.80  -0.79 0.10 0.10  -0.96  -0.63 1.00     3861     3304\n     f[2]  -0.84  -0.84 0.10 0.09  -1.01  -0.68 1.00     4951     3194\n     f[3]   0.75   0.75 0.10 0.10   0.59   0.90 1.00     4998     3239\n     f[4]  -0.41  -0.41 0.10 0.10  -0.57  -0.24 1.00     5386     3131\n     f[5]  -1.47  -1.47 0.10 0.10  -1.63  -1.30 1.00     5378     2822\n     f[6]   0.20   0.19 0.10 0.10   0.03   0.36 1.00     5048     3493\n     f[7]  -0.12  -0.13 0.57 0.57  -1.05   0.83 1.00     2267     2691\n     f[8]  -0.20  -0.20 0.38 0.37  -0.82   0.42 1.00     2129     2680\n     f[9]  -0.32  -0.32 0.17 0.17  -0.61  -0.03 1.00     2572     2588\n\n # showing 10 of 48 rows (change via 'max_rows' argument or 'cmdstanr_max_rows' option)\n```\n\n\n:::\n:::\n\n\n\nThe R-hat and ESS summaries tell you if the model has converged. This one has, but I had to double the maximum treedepth. Let's now plot the draws of $f(x)$ from the $GP$ posterior.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx_vals <- c(df6$x, x_pred)\n\ndraws <- samples$draws(format = \"draws_matrix\") |>\n  as_tibble() |>\n  # Every row is a draw, which we number\n  mutate(draw = 1:n()) |>\n  # Since each column is an observation f(x) for x indices, we pivot\n  pivot_longer(starts_with(\"f\"), names_to = \"x\", values_to = \"y\") |>\n  # And map the x index back to an x value\n  mutate(\n    idx = as.numeric(str_extract(x, \"[0-9]+\")),\n    x = x_vals[idx],\n    y = as.numeric(y)\n  )\n\nggplot() +\n  geom_line(data = draws, mapping = aes(x = x, y = y, group = draw), alpha = 0.01) +\n  geom_point(data = df6, mapping = aes(x = x, y = y), colour = \"red\") +\n  geom_text(data = mutate(df6, n = rank(x)), mapping = aes(x = x, y = y, label = n), nudge_x = 0, nudge_y = 1.1) +\n  geom_segment(data = df6, mapping = aes(x = x, y = y, yend = y + 1), linetype = \"dashed\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/fig-draws-1.png){#fig-draws width=672}\n:::\n:::\n\n\n\nThe observations are shown as red points, and I've numbered them for ease of reference. You can see the predictions of $f(x)$ (the black lines) don't deviate far from the observations, and some intervals between observations are very well-defined too. However for certain intervals they take a wide variety of lines, such as the interval crossing $x=0$. This is intuitive: imagine asking a group of humans to draw a smooth line between the points. Most of them would draw the same lines directly between points 2-3 and 4-5, but there would be much disagreement about what point to turn at between 3-4.\n\n## The effect of noise\n\nIn that model $\\sigma$ controls the amount of noise we believe we have on the observations. Increasing that number will increase the uncertainty of the model, and correspondingly will make it more likely the MCMC simulation diverges.\n\n## Optimising with Cholesky decomposition\n\nWe can speed up the simulation with a trick call Cholesky decomposition. We decompose $K = LL^T$  and reparameterise $f = \\mu + L\\eta$. If $\\eta$ is normally distributed with mean zero and unit variance, this is equivalent.\n\n```stan\ndata {\n  // Observed data\n  int n_data;\n  array[n_data] real x_data;\n  array[n_data] real y_data;\n  \n  // Observation error\n  real<lower=0> sigma;\n  \n  // x values for which we aim to predict y\n  int n_pred;\n  array[n_pred] real x_pred;\n  \n  // Hyperparameters for the kernel\n  real alpha;\n  real lambda;\n}\ntransformed data {\n  // We join the x observations and desired x prediction points\n  int n = n_data + n_pred;\n  \n  array[n] real x;\n  x[1 : n_data] = x_data;\n  x[(n_data + 1): n] = x_pred;\n  \n  // We calculate the Kernel values for all x\n  matrix[n, n] K;\n  K = gp_exp_quad_cov(x, alpha, lambda);\n  \n  // Add nugget on diagonal for numerical stability\n  for (i in 1 : n) {\n    K[i, i] = K[i, i] + 1e-6;\n  }\n}\nparameters {\n  // This is what we want to estimate\n  vector[n] f;\n}\nmodel {\n  // Likelihood is tested against the observations\n  y_data ~ normal(f[1 : n_data], sigma);\n\n  // f is sampled from GP\n  // The domain of the GP prior is all x\n  // We assume the mean is always 0\n  f ~ multi_normal(rep_vector(0, n), K);\n}\n\n``` \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- cmdstan_model(stan_file = \"gp-cholesky.stan\", exe = \"gp-cholesky.stan.bin\")\n\nsamples <- model$sample(\n  list(\n    n_data = nrow(df6),\n    x_data = as.array(df6$x),\n    y_data = as.array(df6$y),\n    sigma = 0.1,\n    n_pred = length(x_pred),\n    x_pred = x_pred,\n    alpha = 1,\n    lambda = 1\n  ),\n  parallel_chains = 4,\n  show_messages = FALSE # disabled to avoid polluting the blog post, should be TRUE\n)\n\nsamples\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n variable   mean median   sd  mad     q5    q95 rhat ess_bulk ess_tail\n   lp__   -25.77 -25.48 4.77 4.69 -34.13 -18.42 1.00     1469     2478\n   eta[1]  -0.80  -0.80 0.10 0.10  -0.96  -0.64 1.00     5898     3414\n   eta[2]  -0.84  -0.84 0.10 0.10  -1.00  -0.68 1.00     6116     2987\n   eta[3]   1.26   1.26 0.12 0.12   1.06   1.46 1.00     5400     3419\n   eta[4]  -0.02  -0.02 0.11 0.11  -0.20   0.16 1.00     5701     2939\n   eta[5]  -1.38  -1.38 0.10 0.10  -1.54  -1.22 1.00     8086     2528\n   eta[6]   0.40   0.40 0.11 0.11   0.22   0.58 1.00     5329     3242\n   eta[7]   0.01   0.00 0.98 0.95  -1.62   1.62 1.00     7582     2950\n   eta[8]   0.00   0.01 1.00 1.00  -1.65   1.65 1.00     6981     3104\n   eta[9]   0.00   0.01 0.99 1.00  -1.68   1.62 1.00     7338     2935\n\n # showing 10 of 95 rows (change via 'max_rows' argument or 'cmdstanr_max_rows' option)\n```\n\n\n:::\n:::\n\n\n\nIf we run that version convergence is much faster.\n\n# OK but why?\n\nWhat's the benefit of this approach? It's pretty trivial to fit a polynomial or a spline to our data to get some idea of the shape of the function. The polynomial is no good for extrapolation, but neither is the Gaussian Process approach.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- df6$x\ny <- df6$y\npolyfit <- lm(y ~ poly(x, degree=4))\np_y <- predict(polyfit, data.frame(x=-5:5))\n\nggplot() +\n    aes(x=x, y=y) +\n    geom_point(data=df6, colour=\"red\") +\n    geom_line(data=data.frame(x=-5:5, y=p_y), stat=\"smooth\") +\n    theme_minimal()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/fig-poly-1.png){#fig-poly width=672}\n:::\n:::\n\n\n\nI can think of three benefits to using a Gaussian Process:\n\n1. You can visualize the uncertainty around certain regions of the function.\n2. You can use the samples to answer questions like \"what's the probability that $f(0) > 0$\" or \"what's the expected value and variance of $f(1.5)$\".\n3. You can provide an informative prior, if you have some understanding of what the function should look like already.\n\nThese are all the typical benefits to a Bayesian approach. Number two is used in MIPRO prompt optimisation, to determine which region to evaluate next. (Though to be clear my level of understanding with MIPRO is \"skimmed the paper, think I can use it\" rather than anything impressive.)\n\n# Wrapping up\n\nSo we've seen:\n\n- what Gaussian Processes are\n- how to they are parameterised by a kernel, and the hyperparameters affect the process\n- how to sample functions from a GP\n- how to model a GP and estimate its posterior with Stan (a GP regression)\n- how to optimise the computation with a Cholesky decomposition\n- why this is more interesting/useful than a simple polynomial fit\n\nWhere next? The next thing to figure out is Bayesian Optimisation itself, which adds an _acquisition function_ into the mix. That is a cost function that will decide what regions to explore to discover the maximum of the function. But that will have to wait for another day.",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}