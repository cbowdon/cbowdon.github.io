{
  "hash": "b2af3a04d1333b4be63be112150b3a65",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Ultra-processed food: an AI polling simulation\"\ndate: \"2025-06-23\"\ncategories: [food, uk, r, ai, llm, surveys]\ncode-fold: true\nwarning: false\n---\n\n\n\nThere's a little drip drip drip of scare stories about ultra-processed food (UPF) a phrase that I'd never heard until this year. I'm starting to get worried. Is everyone else worried? Well it turns out the Food Standards Agency (FSA) has been running a tracker survey containing this question since mid-2023, so we can find out.\n\nAn idea that's generating a lot of excitement recently is the suggestion that you can [simulate public opinion polls with AI](https://www.cambridge.org/core/journals/political-analysis/article/abs/out-of-one-many-using-language-models-to-simulate-human-samples/035D7C8A55B237942FB6DBAD7CAA4E49). The concept is very simple: you calibrate multiple LLM assistants to represent your different demographics and then sample their responses to questions. This is quicker and cheaper than polling real people, _if_ you can make it reliable.\n\nThat sounds incredibly fun! In this post I'll explore the FSA tracker survey data, and then build an AI simulation model.\n\n# The survey data\n\nFirst we need to scrape the survey data. While it's lovely that the FSA publishes all the monthly survey results (which were run by YouGov) it is annoying that they don't publish compiled statistics, so we need to get our tibbles dirty.\n\nAs usual, if you like coding with R you can see the gory details under the folds.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(httr2)\nlibrary(rvest)\nlibrary(knitr)\n\nif (!str_ends(getwd(), \"posts/food\")) {\n  setwd(\"posts/food\")\n}\n\ndownload_files <- function() {\n  url <- \"https://data.food.gov.uk/catalog/datasets/0bfd916a-4e01-4cb8-ba16-763f0b36b50c\"\n\n  html_content <- request(url) |>\n    req_perform() |>\n    resp_body_string()\n\n  # Assuming html_content is already defined\n  doc <- read_html(html_content)\n  xlsx_links <- doc %>% html_nodes(\"a[href$='.xlsx']\") %>% html_attr(\"href\")\n  for (link in xlsx_links) {\n    file_url <- link\n    file_name <- basename(file_url)\n    destfile <- sprintf(\"data/%s\", file_name)\n    if (!file.exists(destfile)) {\n      download.file(url = file_url, destfile = destfile)\n    }\n  }\n}\n\ndownload_files()\n```\n:::\n\n\n\n(Is there an easy way to read tracker survey spreadsheets into R? They have a common format, but I never found anything that did the job and had to roll my own. Anyway, eventually we end up with a single dataset.)\n\n\n\n::: {#tbl-monthly-spreadsheets .cell tbl-cap='Example of the survey data.'}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(knitr)\n\nextract_month_year <- function(file) {\n  decoded_filename <- URLdecode(file)\n\n  month_year_regex <- \"^.*(January|February|March|April|May|June|July|August|September|October|November|December)\\\\s?(\\\\d{2}|\\\\d{4}).*.xlsx$\"\n\n  if (grepl(month_year_regex, decoded_filename)) {\n    extracted_month_year <- regmatches(\n      decoded_filename,\n      regexec(month_year_regex, decoded_filename)\n    )\n\n    # Extract month and year\n    month <- extracted_month_year[[1]][[2]]\n    year <- extracted_month_year[[1]][[3]]\n\n    if (str_length(year) == 2) {\n      year <- paste0(\"20\", year)\n    }\n\n    return(list(filename = file, month = month, year = year))\n  } else {\n    print(file)\n    return(NULL)\n  }\n}\n\nextract_question <- function(file, question_prefix) {\n  fmy <- extract_month_year(file)\n\n  data <- read_excel(\n    file,\n    sheet = \"Percents\",\n    col_names = FALSE\n  )\n  headers <- as.vector(data[5, ], mode = \"character\")\n  headers[1] <- \"QCategory\"\n  headers <- zoo::na.locf(headers)\n  subheaders <- as.vector(data[6, ], mode = \"character\")\n  subheaders <- zoo::na.fill(subheaders, \"\")\n  combined_headers <- map2(\n    headers,\n    subheaders,\n    \\(x, y) str_c(x, y, sep = \":: \")\n  )\n  combined_headers[1] <- \"QCategory\"\n  combined_headers[2] <- \"Total\"\n\n  colnames(data) <- combined_headers\n\n  has_question <- any(str_starts(pull(data, QCategory), question_prefix))\n\n  if (!is.na(has_question)) {\n    data |>\n      filter(!is.na(QCategory)) |>\n      slice(\n        which(str_starts(QCategory, question_prefix)):n()\n      ) |>\n      head(10) |> # take the cats\n      tail(-1) |> # drop the header\n      mutate(\n        filename = fmy$filename,\n        monthname = fmy$month,\n        year = as.integer(fmy$year),\n        month = match(\n          monthname,\n          c(\n            \"January\",\n            \"February\",\n            \"March\",\n            \"April\",\n            \"May\",\n            \"June\",\n            \"July\",\n            \"August\",\n            \"September\",\n            \"October\",\n            \"November\",\n            \"December\"\n          )\n        )\n      )\n  }\n}\n\nquestion_df <- function(question_prefix) {\n  df_filename <- sprintf(\"data/%s.rds\", question_prefix)\n  if (file.exists(df_filename)) {\n    df <- read_rds(df_filename)\n  } else {\n    xlsx_files <- list.files(\n      path = \"data\",\n      pattern = \"*.xlsx\",\n      full.names = TRUE\n    )\n    df <- xlsx_files |>\n      sort(decreasing = TRUE) |>\n      lapply(\\(f) extract_question(f, question_prefix)) |>\n      keep(\\(x) !is.null(x)) |>\n      bind_rows() |>\n      mutate(Question = question_prefix)\n\n    df |> write_rds(df_filename)\n  }\n  df\n}\n\ndf <- question_df(\"Q12_14\")\n\ntidy_df <- df |>\n  filter(QCategory != \"Unweighted base\" & QCategory != \"Base: All\") |>\n  pivot_longer(\n    c(Total, contains(\"::\")),\n    names_to = \"Demographic\",\n    values_to = \"Value\"\n  ) |>\n  mutate(\n    Period = as.Date(sprintf(\"%04d-%02d-01\", year, month)),\n    Response = QCategory,\n    Value = as.numeric(Value)\n  ) |>\n  select(Question, Demographic, Period, Response, Value) |>\n  arrange(Question, Period, Response, Demographic)\n\ntidy_df |> head() |> kable()\n```\n\n::: {.cell-output-display}\n\n\n|Question |Demographic |Period     |Response   |  Value|\n|:--------|:-----------|:----------|:----------|------:|\n|Q12_14   |Age:: 16-24 |2023-08-01 |Don't know | 0.0384|\n|Q12_14   |Age:: 25-34 |2023-08-01 |Don't know | 0.0518|\n|Q12_14   |Age:: 35-44 |2023-08-01 |Don't know | 0.0156|\n|Q12_14   |Age:: 45-54 |2023-08-01 |Don't know | 0.0142|\n|Q12_14   |Age:: 55-74 |2023-08-01 |Don't know | 0.0161|\n|Q12_14   |Age:: 75+   |2023-08-01 |Don't know | 0.0204|\n\n\n:::\n:::\n\n\n\nFinally we have ✨tidy✨ data and we can visualise it.\n  \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(patchwork)\n\nRESPONSE_CATS <- c(\n  \"Highly concerned\",\n  \"Somewhat concerned\",\n  \"Not very concerned\",\n  \"Not concerned at all\",\n  \"Don't know\"\n)\n\nnet_concern <- tidy_df |>\n  filter(Demographic == \"Total\" & str_starts(Response, \"Net: \"))\n\nbreakdown <- tidy_df |>\n  filter(Demographic == \"Total\" & !str_starts(Response, \"Net: \")) |>\n  mutate(\n    Response = if_else(Response %in% RESPONSE_CATS, Response, \"Don't know\"),\n    Response = factor(Response, levels = RESPONSE_CATS, ordered = TRUE)\n  ) |>\n  # Re-normalise after removing the IDK cat\n  group_by(Question, Demographic, Period) |>\n  mutate(Value = Value / sum(Value)) |>\n  ungroup()\n\nplot_net_concern <- ggplot(net_concern) +\n  aes(x = Period, y = Value) +\n  scale_y_continuous(limits = c(0, 1)) +\n  geom_point() +\n  stat_smooth(method = \"lm\") +\n  labs(title = \"Net concern\", x = NULL, y = \"Proportion of respondents\")\n\nplot_breakdown <- ggplot(breakdown) +\n  aes(x = Period, y = Value, group = Response, colour = Response) +\n  geom_point() +\n  stat_smooth(method = \"lm\") +\n  labs(title = \"Response breakdown\", x = NULL, y = \"Proportion of respondents\")\n\nplot_net_concern + plot_breakdown\n```\n\n::: {.cell-output-display}\n![Tracker for \"At the moment, how concerned, if at all, do you personally feel about ultra-processed, or over-processing of food?\" Left is net concern, right is all responses.](index_files/figure-html/fig-responses-1.png){#fig-responses width=672}\n:::\n:::\n\n\n\nThe number of people with concerns is evidently slowly increasing. A linear regression fits net concern very well, so it's reasonable to expect continued growth at the same rate (pending any intervention like policy changes). Presumably the line bends as we approach the inevitable core of stubborn sods who will never admit to being concerned about anything, but the data we have doesn't support that kind of model.\n\nThe breakdown by response type is interesting. We have to watch our step, given the noisiness, but it does seem to show a migration towards being _highly_ concerned.\n\nNote that I have merged the two categories \"I don't know enough to comment\" and \"Don't know\" in the original data for two reasons: first, there's a technical limit of 5 categories coming later, and second there is very little difference between these two.\n\n# The media landscape\n\nBesides calibrating the simulated demographics, the major challenge is ensuring that the models are exposed to the same information. If interested in current events, this information is most likely not in the training data. This means you need to ensure the AI has been exposed to the same media.\n\nNow, here's an interesting observation: a surprisingly small number of publications account for a large proportion of UK readership. See this data from [JournoFinder](https://journofinder.com/blog/uk-newspaper-circulation-figures).\n\n![](https://framerusercontent.com/images/1BjmhUMPE6dsrYLtK1qkJV8p5k.png)\n\nThe UK population is approximately 70 million people. Given that the Guardian and the Daily Mail target very different demographics (left and right wing) we wouldn't expect much overlap between their readership, i.e. their combined monthly online readership alone probably covers half the news-reading British population. Note also that BBC News isn't included in that table, but has a similar level of traffic, albeit with higher overlap.\n\nThis suggests that if we're interested in understanding how the media shapes public perception in the UK, we can do well by analysing the output of just a handful of sources. This is particularly true in the modern media landscape, in which:\n\n- Live television is a dying medium. (I was at a brilliant R meetup hosted by [Datacove](https://datacove.co.uk) recently when the session discussion turned to media channels. No one present had watched live TV in the last 24 hours, and only around 1 in 10 in the last month.)\n- Radio has had its time, has had its power (though yet to have its finest hour 🎸).\n- Social media is increasingly popular, but news content on social media is dominated by reposts of articles from traditional journalists.\n\nSo let's create a corpus of news articles about ultra-processed food from the top 10 sources in the table above, plus the BBC. They have a combined readership of 64.8 million monthly readers in the UK (plus the BBC's unknown-but-high readership); even with high overlap, this clearly represents the majority of the UK population. It's enough to give us a solid understanding of what's going on.\n\nI happen to work at [Polecat](https://www.polecat.com) who have licensed access to this data, so I can analyse it very easily. (Can't share the raw data here though. 🙊)\n\nSince November 2023 there have been over 2.5 million articles from those sources, but only around 2000 mentioned ultra-processed food, and <500 seem to be focused on it, as opposed to mentioning it alongside other food or public health issues. Though all mentions would help with awareness, focus articles have a stronger effect on informing opinion.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsource(\"articles.R\")\n\nupf_articles <- load_upf_articles()\ndomain_counts <- load_domain_counts()\n\nggplot(\n  upf_articles |>\n    mutate(\n      mention_type = if_else(is_focus, \"Focus article\", \"Mentions UPFs\")\n    )\n) +\n  aes(x = period, group = is_focus) +\n  geom_histogram(stat = \"count\") +\n  facet_wrap(~mention_type) +\n  labs(\n    title = \"Articles mentioning ultra-processed food from top UK sources, by month.\",\n    x = NULL,\n    y = \"Article count\"\n  )\n```\n\n::: {.cell-output-display}\n![Counts of articles mentioning ultra-processed food from the top UK online news sources, by month.](index_files/figure-html/fig-article-counts-1.png){#fig-article-counts width=672}\n:::\n:::\n\n\n\nN.B. The Sun and the Times have further licence restrictions, so aren't included here.\n\nAlthough the number of articles focused on UPFs is not showing any trend, the number of articles mentioning UPFs is increasing over time. The fluctuation is most likely seasonal, and the trend is apparently upwards. There's _just_ about enough data to fix a SARIMA model. (We ought to have two years, we have eighteen months but the cycles seem to be sub-year.)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmention_counts <- ts(\n  (group_by(upf_articles, period) |> count())$n,\n  start = c(2024, 1),\n  deltat = 1 / 12\n)\nafit <- forecast::auto.arima(mention_counts, d = 1, D = 1)\nplot(forecast::forecast(afit, h = 5, level = 90))\n```\n\n::: {.cell-output-display}\n![A (very uncertain) SARIMA fit of the mention counts.](index_files/figure-html/fig-sarima-1.png){#fig-sarima width=672}\n:::\n:::\n\n\n\nIt's tempting to look at the upward trend in mentions and the upward trend in net concern, and call it a day. Media mentions go up, net concern goes up, quod erat demonstratum! But this is a bit flimsy.\n\n## Why can't we just count mentions and be done with it?\n\n1. Even considering that we've already reasoned about UK audiences and scoped our search to selected top publications, this is still a poor model of readership. Every mention in every article counts equally, even though we can see that the Guardian has **over ten times** the number of readers the Financial Times does.\n2. This model assumes that every mention of UPFs contributes an equal amount of concern. Some of them might be positive, or cause different levels of concern.\n3. Net concern has increased by <5%, whereas the mentions have doubled. We have reason to be skeptical that mentions count is the whole story.\n\nWe need to take a more nuanced approach.\n\n# A better readership model\n\nLet's start with a look at how the articles are distributed amongst the sources.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(\n  upf_articles |> group_by(domain, period) |> count()\n) +\n  aes(x = period, y = n, group = domain, colour = domain) +\n  facet_wrap(~domain, nrow = 2) +\n  geom_point(show.legend = F) +\n  geom_line(linewidth = 1, show.legend = F)\n```\n\n::: {.cell-output-display}\n![Articles focusing on ultra-processed food by source, since Jan 2024.](index_files/figure-html/fig-source-dist-1.png){#fig-source-dist width=672}\n:::\n:::\n\n\n\nPerhaps unsurprisingly, the Daily Mail has by far the most. Who'd have thought? They are also showing the most obvious increase in attention to the topic.  (Incidentally, the Daily Mail also has the most articles about cancer, bone disease, and early death. They must employ a lot of medical professionals.) To be completely fair to the Daily Mail though, we should note it is a smaller proportion of their total output, which is enormous.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(\n  group_by(domain_counts, domain) |> summarise(n = sum(n_articles))\n) +\n  aes(\n    y = fct_reorder(domain, n),\n    x = n,\n    group = domain,\n    colour = domain,\n    fill = domain\n  ) +\n  geom_col(linewidth = 1, show.legend = F)\n```\n\n::: {.cell-output-display}\n![Total number of articles output by top sources, since Jan 2024.](index_files/figure-html/fig-source-count-context-1.png){#fig-source-count-context width=672}\n:::\n:::\n\n\n\n## Modelling\n\nWe now have enough information to build a Monte Carlo model of media exposure. We'll keep it as simple as we can without compromising too much on representativeness.\n\n> Everything should be as simple as possible, but no simpler. (Einstein?)\n\nLet's start by assuming the UK population is exposed to articles from each source in proportion to that source's estimated readership. We're going to get a bit maths now.\n\nLet every reader $r$ sample a number of articles $n_s$ from each source $s$ according to a Poisson distribution - i.e. a natural discrete distribution of counts. The Poisson distribution is parameterised according to relative readership levels, so for example $\\lambda_{BBC} > \\lambda_{FT}$ and on average $n_{BBC} > n_{FT}$ but with lots of randomisation, so some of our simulated readers will draw more from the Mail, some more from the Metro, etc.\n\nThe reader then draws $n_s$ articles at random from each source. (This is a simplification of course.) To represent the fact that only a small proportion of articles from each source in each period are about UPFs, we draw the $n_s$ observations from a binomial distribution parameterised by $p_s$, the relative frequency of UPF articles for the source. The count of UPF-related articles we get back is denoted $d_s$.\n\n$$\nn_s \\sim Pois(\\lambda_s)\n$$\n$$\nd_s \\sim Binom(n_s, p_s)\n$$\n\nSince $p_s$ is very small, this will be very inefficient and we will mostly draw zeros. But maths comes to the rescue, because the unconditional distribution of $d_s$ can be given by:\n\n$$\nd_s \\sim Pois(\\lambda_s p_s)\n$$\n\nThis is much faster to compute. We need to do so for each of our simulated readers $r$ to get a count of relevant articles they will read from each source. We then materialise these articles by sampling from the actual UPF articles from the soure in that time period, generate the LLM assessment for the articles, and take the mean assessment.\n\nThis is quite a complicated model, but it's also leaving A LOT out. We're ignoring the demographics of each source's readership, for example, and we're making the incorrect assumption that all articles are equally likely to be read. It would be better to have a multinomial distribution across the articles rather than our simple binomial. Ideally that would be informed by accurate article-level readership statistics, though those are difficult if not impossible to get for all sources. We're also ignoring incomplete reads or misreads.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreadership <- tibble(\n  domain = factor(DOMAINS, levels = DOMAINS, ordered = TRUE),\n  monthly_readers = 1000000 *\n    c(59, 19.7, 12.3, 8.3, 5.9, 5.7, 4.6, 2.6, 2.5, 1.3) # BBC estimated from OFCOM survey\n) |>\n  merge(\n    domain_counts |>\n      group_by(domain, period) |>\n      summarise(n_total_articles = sum(n_articles)),\n    all.x = TRUE\n  ) |>\n  merge(\n    upf_articles |> group_by(domain, period) |> summarise(n_upf_articles = n()),\n    all.x = TRUE\n  ) |>\n  mutate(\n    n_upf_articles = coalesce(n_upf_articles, 0),\n    prob_upf = n_upf_articles / n_total_articles,\n    # Number of articles in the time period, calibrated such\n    # that the expected number of BBC articles is approx 1/day.\n    # This is an educated guess, not much research to back it I'm afraid. Pew found in 2015 that\n    # \"An overwhelming majority of both long-form readers (72%) and short-form readers (79%)\n    # view just one article on a given site over the course of a month on their cellphone.\"\n    # - https://www.pewresearch.org/journalism/2016/05/05/long-form-reading-shows-signs-of-life-in-our-mobile-news-world/\n    # That was specific to the heady earlier days of the mobile web though.\n    # I found various industry-backed studies suggesting it could be higher, such as this NewsWorks study that\n    # found young people reading 6/day, however this is really suspicious and they have an\n    # obvious incentive to inflate the numbers.\n    # https://pressgazette.co.uk/media-audience-and-business-data/media_metrics/young-people-news/\n    lambda = 30.4 * monthly_readers / max(monthly_readers),\n  )\n```\n:::\n\n\n\nI've had to make an educated guess on the expected number of articles read per month - details are in the code above.\n\nWe can now build the model using R's statistical functions and simulate a number of readers. Then we can examine how many articles on UPFs the readers are likely to read each month.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\n\nsample_article_counts <- function(month_readership, n_sim_readers) {\n  result <- matrix(\n    ncol = length(DOMAINS),\n    nrow = n_sim_readers,\n    dimnames = list(1:n_sim_readers, DOMAINS)\n  )\n  for (i in 1:nrow(month_readership)) {\n    source <- as.list(month_readership[i, ])\n    n <- rpois(n_sim_readers, source$lambda * source$prob_upf)\n    result[, i] <- n\n  }\n  result\n}\n\nPERIODS <- seq.Date(\n  from = as.Date(\"2024-01-01\"),\n  to = as.Date(\"2025-05-01\"),\n  by = \"1 month\"\n)\n\nN_SIM_READERS = 10000\n\nsim_counts <- PERIODS |>\n  map(\n    function(p) {\n      sample_article_counts(\n        month_readership = filter(readership, period == p),\n        n_sim_readers = N_SIM_READERS\n      )\n    }\n  )\n```\n:::\n\n::: {#tbl-article-reads-monthly .cell tbl-caption='Distribution of average number of UPF articles read per month'}\n\n```{.r .cell-code}\nmonthly_sim_count_totals <- sim_counts |> map(~ table(rowSums(.)))\n\nmonthly_sim_count_totals |>\n  map(as.data.frame) |>\n  bind_rows() |>\n  rename(n_upf_articles_read_in_month = Var1) |>\n  group_by(n_upf_articles_read_in_month) |>\n  summarise(percent_of_readers = 100 * mean(Freq) / N_SIM_READERS) |>\n  kable(digits = 1)\n```\n\n::: {.cell-output-display}\n\n\n|n_upf_articles_read_in_month | percent_of_readers|\n|:----------------------------|------------------:|\n|0                            |               96.3|\n|1                            |                3.6|\n|2                            |                0.1|\n|3                            |                0.0|\n\n\n:::\n:::\n\n\n\nThis suggests that around 4% of news readers will read an article on UPFs every month. That seems quite reasonable. The number has grown from around 2% to around 6% over the last year and a half. This is more realistic than our original model, which simply noted a doubling in the number of articles. Our more sophisticated model says the number of articles read has probably tripled, but that they only influence a very small proportion of readers.\n\nA little reminder: we have modelled just the top 10 UK news sources, which have orders of magnitude more eyeballs than all other online news sources. We would expect the contribution from local news, blogs, trade pubs, etc. to be negligible.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nupf_readership_over_time <- map2(\n  PERIODS,\n  monthly_sim_count_totals,\n  function(p, t) {\n    tibble(\n      period = p,\n      pc_sim_readers_reading_at_least_one_article = 100 *\n        (N_SIM_READERS - t[[1]]) /\n        N_SIM_READERS\n    )\n  }\n) |>\n  bind_rows()\n\nggplot(upf_readership_over_time) +\n  aes(x = period, y = pc_sim_readers_reading_at_least_one_article) +\n  geom_line() +\n  scale_y_continuous(\n    limits = c(0, 20),\n    breaks = seq(0, 20, 10),\n    minor_breaks = seq(0, 20, 5)\n  ) +\n  labs(\n    x = NULL,\n    y = \"Percent of simulated readers\",\n  )\n```\n\n::: {.cell-output-display}\n![Change in percent of simulated readers who read at least one UPF article in a month over time.](index_files/figure-html/fig-growth-in-readers-1.png){#fig-growth-in-readers width=672}\n:::\n:::\n\n\n\nBy examining the model parameters, we can see that the sources that contribute most are the Guardian, the Telegraph, and the BBC.\n\n\n\n::: {#tbl-domain-params .cell tbl-cap='Mean monthly model parameters for each domain.'}\n\n```{.r .cell-code}\n#|\nreadership |>\n  group_by(domain) |>\n  summarise(\n    lambda = mean(lambda),\n    prob_upf = mean(prob_upf),\n    monthly_readers = mean(monthly_readers),\n    monthly_total_articles = mean(n_total_articles),\n    monthly_upf_articles = mean(n_upf_articles),\n    exp_monthly_upf_articles = mean(lambda * prob_upf)\n  ) |>\n  mutate(\n    exp_monthly_upf_articles = exp_monthly_upf_articles /\n      sum(exp_monthly_upf_articles)\n  ) |>\n  arrange(desc(exp_monthly_upf_articles)) |>\n  kable(\n    digits = c(0, 1, 4, 0, 0, 0, 2),\n    format.args = list(decimal.mark = \".\", big.mark = \",\")\n  )\n```\n\n::: {.cell-output-display}\n\n\n|domain            | lambda| prob_upf| monthly_readers| monthly_total_articles| monthly_upf_articles| exp_monthly_upf_articles|\n|:-----------------|------:|--------:|---------------:|----------------------:|--------------------:|------------------------:|\n|theguardian.com   |   10.2|   0.0012|      19,700,000|                  7,665|                   10|                     0.33|\n|telegraph.co.uk   |    2.4|   0.0033|       4,600,000|                  5,167|                   17|                     0.20|\n|bbc.co.uk         |   30.4|   0.0001|      59,000,000|                 17,938|                    2|                     0.11|\n|dailymail.co.uk   |    6.3|   0.0006|      12,300,000|                 52,210|                   33|                     0.11|\n|independent.co.uk |    4.3|   0.0006|       8,300,000|                 14,587|                    9|                     0.07|\n|express.co.uk     |    3.0|   0.0008|       5,900,000|                 12,922|                   11|                     0.07|\n|mirror.co.uk      |    2.9|   0.0008|       5,700,000|                 13,484|                   10|                     0.06|\n|standard.co.uk    |    1.3|   0.0006|       2,600,000|                  6,563|                    4|                     0.02|\n|ft.com            |    0.7|   0.0009|       1,300,000|                 12,870|                   11|                     0.02|\n|metro.co.uk       |    1.3|   0.0004|       2,500,000|                  6,682|                    3|                     0.01|\n\n\n:::\n:::\n\n\n\nAgain this differs significantly from our simpler model, which would have pinned it all on the Daily Mail, as that source has the largest absolute number of UPF articles. The Telegraph is initially surprising given their comparatively low readership, but they have a very high probability of producing UPF articles.\n\nWith a more realistic model of readership achieved, we can move on to the AI polling. \n\n# Building the AI's world view\n\nThe next step is to sample specific articles for each reader given their counts. In our model, these are the articles that will influence opinions, i.e. those which inform the LLM survey respondents.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_index <- expand.grid(\n  period = 1:length(PERIODS),\n  reader = 1:N_SIM_READERS,\n  domain = 1:length(DOMAINS)\n)\n\nsampled_articles <- 1:nrow(sample_index) |>\n  map(\n    function(i) {\n      p <- sample_index[i, 1]\n      r <- sample_index[i, 2]\n      s <- sample_index[i, 3]\n\n      n <- sim_counts[[p]][r, s]\n\n      if (n > 0) {\n        upf_articles |>\n          filter(period == PERIODS[p] & domain == DOMAINS[s]) |>\n          slice_sample(n = n, replace = FALSE) |>\n          mutate(period = PERIODS[p], reader = r, n = n)\n      }\n    },\n    .progress = F\n  ) |>\n  bind_rows()\n\nprint(\n  sprintf(\n    \"Percent of readers who have read at least one UPF article in the total period: %.1f%%\",\n    100 * n_distinct(sampled_articles$reader) / N_SIM_READERS\n  )\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Percent of readers who have read at least one UPF article in the total period: 46.8%\"\n```\n\n\n:::\n:::\n\n\n\nAggregating this sample this tells us that 50% of our (simulated) reading population have read at least one article about UPFs in the last year and a half.\n\nFinally we're ready to see what effect these articles have on the readers. Break out the AI!\n  \n# Asking the AI questions\n\nNow we get to move on from that awful CPU-based statistic model (boo, dull) to an awesome GPU-based statistic model (wow, sexy).\n\nThe approach we take is to force the LLM into picking a category via structured outputs, and then reviewing the probabilities that it would have picked each category. R afficionados, we are using [ellmer](https://ellmer.tidyverse.org), which is more or less the de-facto AI package for R.\n\nTo avoid sharing the licensed news data with a third-party, the model is a local Qwen3 model running with a patched version of mlx-omni-server. Qwen3's reasoning has been disabled because that changes the conditioning of the model and we just want its immediate \"system 1\" response to the survey questions. Working with mlx-omni-server is also awesome because when you're confused about some of the finer points of logits and sampling you can just LOOK at the code and see what's happening.\n\nMost of the AI code is folded, but it's useful to see the prompts.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# This is the system prompt, which initiates every conversation by giving the AI a persona and instructions.\nSYSTEM_PROMPT <- \"You are a member of the British public, with your own life experiences and opinions.\n\nThis is your identity: \n<identity>\n{{simulated_identity}}\n</identity>\n\nYou are being asked for your views as part of a survey. Respond to each question from the pollster.\"\n\n# This is the question precisely as asked in the YouGov survey.\nUPF_QUESTION <- \"At the moment, how concerned, if at all, do you personally feel about ultra-processed, or over-processing of food?\"\n\n# This is how we wrap the question with relevant articles (and a date, for context).\nQUESTION_ARTICLE_WRAPPER <- \"Answer the question, recalling that you have seen the following articles that may or may not have influenced your opinion.\n\n<articles>\n{{headlines}}\n</articles>\n\n(Today's date is {{date_today}}.)\n\nQUESTION: {{question}}\"\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ellmer)\n\ncreate_respondent <- function(\n  simulated_identity = \"Average person\"\n) {\n  chat_openai(\n    base_url = \"http://0.0.0.0:10240/v1/\",\n    # Gemma is good\n    #model = \"mlx-community/gemma-3-1b-it-4bit-DWQ\",\n    #model = \"mlx-community/gemma-3-27b-it-4bit-DWQ\",\n    #model = \"mlx-community/gemma-3-27b-it-qat-8bit\",\n    # Qwen is great\n    #model = \"mlx-community/Qwen3-0.6B-4bit-DWQ-053125\",\n    #model = \"mlx-community/Qwen3-1.7B-4bit-DWQ-053125\",\n    #model = \"mlx-community/Qwen3-4B-4bit-DWQ-053125\",\n    model = \"mlx-community/Qwen3-14B-4bit-DWQ-053125\",\n    # Uncensored?\n    #model = \"mlx-community/Josiefied-Qwen3-0.6B-abliterated-v1-4bit\",\n    # GPT for debugging, do not use with data\n    #model = \"gpt-4.1-nano\",\n    api_key = \"n/a\",\n    system_prompt = interpolate(SYSTEM_PROMPT),\n    params = params(\n      # Single output (all the local server supports)\n      n = 1,\n      # Get max available log probs\n      log_probs = TRUE,\n      top_logprobs = 5,\n      # Keep the full distribution for sampling\n      top_p = 1,\n      # Do not adjust the distribution, so no cascading randomness\n      temperature = 0\n    ),\n    api_args = list(\n      # Ensure we disable reasoning from Qwen\n      enable_thinking = FALSE\n    )\n  )\n}\n\nextract_answer_dist <- function(respondent, choices) {\n  rc <- tibble(\n    token = LETTERS[1:length(choices)],\n    Response = factor(choices, levels = choices, ordered = TRUE)\n  )\n\n  token <- respondent$last_turn()@json$choices[[1]]$logprobs$content |>\n    keep(~ str_detect(.x$token, paste(rc$token, collapse = \"|\"))) |>\n    first()\n\n  if (is.null(token)) {\n    stop(\n      respondent$last_turn()@json$choices[[1]]\n    )\n  }\n\n  token$top_logprobs |>\n    bind_rows() |>\n    select(token, logprob) |>\n    unique() |>\n    merge(rc) |>\n    mutate(Probability = exp(logprob)) |>\n    select(Response, Probability, logprob) |>\n    arrange(Response)\n}\n\nquestion_with_articles <- function(question, articles, date_today = today()) {\n  headlines <- articles |>\n    map(function(a) {\n      with(\n        a,\n        {\n          excerpts <- paste(\n            map(highlights, ~ sprintf(\"> %s\", .)),\n            collapse = \"\\n\"\n          )\n          sprintf(\n            \"HEADLINE: %s (%s, %s)\\nEXCERPTS:\\n%s\\n\\n\",\n            title,\n            domain,\n            publish_date,\n            excerpts\n          )\n        }\n      )\n    })\n\n  interpolate(QUESTION_ARTICLE_WRAPPER)\n}\n\n\nmultiple_choice_question <- function(question, choices) {\n  paste(\n    c(\n      question,\n      \"\",\n      imap(choices, ~ sprintf(\"%s) %s\", LETTERS[.y], .x)),\n      \"\",\n      \"Respond with the letter of your chosen answer.\"\n    ),\n    collapse = \"\\n\"\n  )\n}\n\nask <- function(respondent, question, choices) {\n  mcq <- multiple_choice_question(question, choices)\n  tryCatch(\n    {\n      respondent$chat_structured(\n        mcq,\n        type = type_enum(\n          description = \"Answer letter\",\n          values = LETTERS[1:length(choices)]\n        )\n      )\n      extract_answer_dist(respondent, choices)\n    },\n    error = function(e) {\n      print(respondent$last_turn()@json)\n      stop(e)\n    }\n  )\n}\n\nask(create_respondent(\"Average person\"), UPF_QUESTION, RESPONSE_CATS) |>\n  kable(digits = 3)\n```\n\n::: {.cell-output-display}\n\n\n|Response             | Probability| logprob|\n|:--------------------|-----------:|-------:|\n|Highly concerned     |       0.434|  -0.834|\n|Somewhat concerned   |       0.462|  -0.771|\n|Not very concerned   |       0.059|  -2.834|\n|Not concerned at all |       0.011|  -4.521|\n|Don't know           |       0.033|  -3.396|\n\n\n:::\n:::\n\n\n\nWe can now get the probability that the LLM would have selected each response in this conversation, given its identity.\n\nNote that I've removed the response category \"I don't know enough to comment\" for the practical reason that only 5 top log probs are available from the LLM.\n\nAlthough we have fixed the temperature to zero to avoid cascading randomness from token sampling, there is still some non-determinism at the hardware level. It almost never affects the most likely token, but it does affect the log probs, so we need to sample until the distribution starts to converge.\n\n\n\n::: {#tbl-converged .cell tbl-cap='Converged response to UPF question.'}\n\n```{.r .cell-code}\nask_converged <- function(\n  respondent_fn,\n  question,\n  choices,\n  min_n_samples = 3,\n  max_n_samples = 30,\n  tolerance = 0.01\n) {\n  samples <- matrix(0, nrow = max_n_samples, ncol = length(choices))\n\n  old_col_means <- colSums(samples) # zeros\n\n  for (i in 1:max_n_samples) {\n    respondent <- respondent_fn()\n    answer <- ask(respondent, question, choices)\n    samples[i, ] = answer$Probability\n\n    new_col_means <- colSums(samples) / i\n\n    max_diff <- max(abs(new_col_means - old_col_means))\n    old_col_means <- new_col_means\n\n    if (i >= min_n_samples && max_diff < tolerance) {\n      print(sprintf(\"Converged after %d iters\", i))\n      break\n    }\n  }\n  tibble(\n    Response = choices,\n    Probability = new_col_means\n  )\n}\n\nconverged_answer <- ask_converged(\n  create_respondent,\n  UPF_QUESTION,\n  RESPONSE_CATS\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Converged after 8 iters\"\n```\n\n\n:::\n\n```{.r .cell-code}\nconverged_answer |> kable(digits = 3)\n```\n\n::: {.cell-output-display}\n\n\n|Response             | Probability|\n|:--------------------|-----------:|\n|Highly concerned     |       0.472|\n|Somewhat concerned   |       0.346|\n|Not very concerned   |       0.082|\n|Not concerned at all |       0.023|\n|Don't know           |       0.077|\n\n\n:::\n:::\n\n::: {#tbl-llm-simulation .cell tbl-cap='Example of simulated AI polling response to a fictional alarming headline.'}\n\n```{.r .cell-code  code-fold=\"false\"}\nfake_articles <- list(\n  list(\n    title = \"Ultra-processed food linked to potential bowel cancer risk, scientists say\",\n    domain = \"dailymail.co.uk\",\n    publish_date = as.Date(\"2025-05-01\"),\n    highlights = c(\n      \"Ultra-processed foods increase the cancer risk by 10000%.\",\n      \"We should all be concerned about ultra-processed foods.\"\n    )\n  )\n)\n\nask_converged(\n  \\() create_respondent(\"Average person\"),\n  question_with_articles(UPF_QUESTION, fake_articles),\n  RESPONSE_CATS\n) |>\n  kable(digits = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Converged after 3 iters\"\n```\n\n\n:::\n\n::: {.cell-output-display}\n\n\n|Response             | Probability|\n|:--------------------|-----------:|\n|Highly concerned     |       0.879|\n|Somewhat concerned   |       0.071|\n|Not very concerned   |       0.013|\n|Not concerned at all |       0.007|\n|Don't know           |       0.030|\n\n\n:::\n:::\n\n\n\n We can also condition the model on a different identity to get a different response distribution.\n\n\n\n::: {#tbl-concerned .cell tbl-cap='Response distribution of LLM again, this time with the identity of someone very concerned about their health.'}\n\n```{.r .cell-code  code-fold=\"false\"}\nask_converged(\n  \\() create_respondent(\"Someone very concerned about their health\"),\n  question_with_articles(UPF_QUESTION, fake_articles),\n  RESPONSE_CATS\n) |>\n  kable(digits = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Converged after 3 iters\"\n```\n\n\n:::\n\n::: {.cell-output-display}\n\n\n|Response             | Probability|\n|:--------------------|-----------:|\n|Highly concerned     |       0.950|\n|Somewhat concerned   |       0.021|\n|Not very concerned   |       0.006|\n|Not concerned at all |       0.004|\n|Don't know           |       0.018|\n\n\n:::\n:::\n\n\n\nA basic model, but already quite interesting. Let's test it for bias.\n\n\n\n::: {#tbl-gender-test .cell tbl-cap='Response distribution of LLM for different genders.'}\n\n```{.r .cell-code}\ngender_resp <- c(\"Female\", \"Male\") |>\n  map(\n    ~ ask_converged(\n      \\() create_respondent(.),\n      UPF_QUESTION,\n      RESPONSE_CATS\n    ) |>\n      mutate(simulated_identity = .)\n  ) |>\n  bind_rows()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Converged after 4 iters\"\n[1] \"Converged after 6 iters\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# gender_resp |> mutate(Net = Response %in% c(\"Highly concerned\", \"Somewhat concerned\")) |> group_by(simulated_identity, Net) |> summarise(p = sum(Probability))\n\ngender_resp |>\n  pivot_wider(\n    id_cols = Response,\n    names_from = simulated_identity,\n    values_from = Probability\n  ) |>\n  kable(digits = 3)\n```\n\n::: {.cell-output-display}\n\n\n|Response             | Female|  Male|\n|:--------------------|------:|-----:|\n|Highly concerned     |  0.701| 0.661|\n|Somewhat concerned   |  0.233| 0.251|\n|Not very concerned   |  0.019| 0.023|\n|Not concerned at all |  0.009| 0.012|\n|Don't know           |  0.038| 0.052|\n\n\n:::\n:::\n\n\n\nThere is a little bias there. Normally we would seek to avoid this, but that's the point of AI simulated polling, to exploit the biases to model \"typical\" responses. If we look at the actual responses by gender, we can see that there really is a gender difference i.e. female respondents were more likely to be highly concerned compared to males.\n\nIn my testing, larger models were more likely to demonstrate bias in the same direction as real respondents (i.e. females having more net concern). Not all model families were suitable, for example Gemma 3 was more than 90% weighted towards \"Highly concerned\".\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngender_breakdown <- tidy_df |>\n  filter(str_starts(Demographic, \"Gender\") & !str_starts(Response, \"Net: \")) |>\n  mutate(\n    Response = if_else(Response %in% RESPONSE_CATS, Response, \"Don't know\"),\n    Response = factor(Response, levels = RESPONSE_CATS, ordered = TRUE),\n    Demographic = str_replace(Demographic, \"Gender:: \", \"\")\n  ) |>\n  # Re-normalise after removing the IDK cat\n  group_by(Question, Demographic, Period) |>\n  mutate(Value = Value / sum(Value)) |>\n  ungroup()\n\nplot_gender_dist <- ggplot(\n  gender_breakdown |>\n    group_by(Demographic, Response) |>\n    summarise(Value = mean(Value)) |>\n    mutate(Value = if_else(Demographic == \"Male\", Value, -Value))\n) +\n  aes(x = Value, y = fct_rev(Response), fill = Demographic) +\n  scale_fill_manual(values = c(\"Male\" = \"orange\", \"Female\" = \"purple\")) +\n  geom_col() +\n  labs(\n    title = \"Gender response distribution\",\n    x = \"Mean proportion of respondents\",\n    y = NULL\n  )\n\nplot_gender_time <- ggplot(gender_breakdown) +\n  aes(x = Period, y = Value, group = Response, colour = Response) +\n  facet_wrap(~Demographic) +\n  geom_point() +\n  stat_smooth(method = \"lm\") +\n  labs(\n    title = \"Response breakdown over time\",\n    x = NULL,\n    y = \"Proportion of respondents\"\n  )\n\nplot_gender_dist + plot_gender_time + plot_layout(nrow = 2, ncol = 1)\n```\n\n::: {.cell-output-display}\n![Actual response distribution for different genders. Females are more likely to say they are \"highly concerned\" about ultra-processed foods.](index_files/figure-html/fig-actual-gender-split-1.png){#fig-actual-gender-split width=672}\n:::\n:::\n\n\n\nTo make a useful gender comparison, we'd need to calibrate the LLM such that its predictions are consistent with known gender biases. The original paper from Argyle et al. did this by conditioning the LLM on backstories - where we just have \"male\" or \"female\", they have a more comprehensive paragraph - and sampling from a set of backstories designed to be representative of the population.\n\n# Running AI over the dataset\n\nLet's apply this to our articles now. We can use `furrr` to run every simulated survey respondent in parallel. We'll start with a single \"average person\" identity.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(furrr)\n\nsamples_per_reader <- sampled_articles |>\n  group_by(period, reader) |>\n  summarize(\n    sample_key = str_flatten_comma(sort(article_id)),\n    article_ids = list(sort(article_id)),\n    n_article = n_distinct(article_id),\n  )\n\nunique_samples <- samples_per_reader |>\n  distinct(period, sample_key, article_ids)\n\nsim_ident <- \"Average person\"\n\nsimulate_reader <- function(row) {\n  tryCatch(\n    {\n      arts <- upf_articles |> filter(article_id %in% row$article_ids)\n      dist <- ask(\n        create_respondent(sim_ident),\n        question_with_articles(\n          UPF_QUESTION,\n          pmap(arts, list),\n          date_today = row$period\n        ),\n        RESPONSE_CATS\n      )\n      dist |>\n        mutate(sample_key = row$sample_key, simulated_identity = sim_ident)\n    },\n    error = \\(e) print(e, row)\n  )\n}\n\nmap_simulated_readers <- function(unique_samples, sim_ident, force = FALSE) {\n  datafile <- sprintf(\"data/article-responses - %s.rds\", sim_ident)\n\n  if (!file.exists(datafile) || force) {\n    with(plan(multisession, workers = 4), {\n      tasks <- future_map(\n        pmap(unique_samples, list),\n        simulate_reader,\n        .progress = TRUE\n      )\n    })\n\n    article_responses <- bind_rows(tasks) |> as_tibble()\n    article_responses |> write_rds(datafile)\n  }\n  read_rds(datafile)\n}\n\narticle_responses <- map_simulated_readers(\n  unique_samples,\n  sim_ident,\n  #force = TRUE\n)\n\nsim_results <- samples_per_reader |> merge(article_responses)\n```\n:::\n\n\n\nWe have now asked every simulated reader the poll question every month. We have a probability distribution of possible answers for each, so we can up-sample. We also can assume that the readers we didn't simulate saw nothing that would change their minds. This is ignoring word-of-mouth and other forms of media, so for a better model we will need to consider those.\n\nWhat we need to do now is combine the default probability distribution with the article-readers.\nWe'll do that month by month, and we can sample from the converged distribution for the non-readers and sample from the individual distributions for readers. That would be much the same as a weighted sum, which is the simpler option.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nperiod_sim_dists <- matrix(nrow = length(PERIODS), ncol = length(RESPONSE_CATS))\n\nfor (i in seq_along(PERIODS)) {\n  period_sim_results <- sim_results |> filter(period == PERIODS[i])\n  n_non_readers <- N_SIM_READERS - n_distinct(period_sim_results$reader)\n\n  non_reader_dist <- n_non_readers * converged_answer$Probability\n  reader_dist <- (period_sim_results |>\n    group_by(Response) |>\n    summarise(p = sum(Probability)))$p\n  total_dist <- non_reader_dist + reader_dist\n  period_sim_dists[i, ] <- total_dist / sum(total_dist)\n}\n\nrownames(period_sim_dists) <- as.character(PERIODS)\ncolnames(period_sim_dists) <- RESPONSE_CATS\n\nsim_breakdown <- as_tibble(period_sim_dists) |>\n  mutate(Period = PERIODS) |>\n  pivot_longer(-c(Period), names_to = \"Response\", values_to = \"Value\") |>\n  mutate(Response = factor(Response, levels = RESPONSE_CATS, ordered = TRUE))\n\nsim_net_concern <- sim_breakdown |>\n  mutate(\n    Concerned = Response %in% c(\"Highly concerned\", \"Somewhat concerned\")\n  ) |>\n  group_by(Period, Concerned) |>\n  summarise(Value = sum(Value)) |>\n  filter(Concerned)\n\nplot_actual_breakdown <- ggplot(breakdown) +\n  aes(x = Period, y = Value, group = Response, colour = Response) +\n  geom_point(show.legend = FALSE) +\n  stat_smooth(method = \"lm\", show.legend = FALSE) +\n  labs(\n    title = \"Actual response breakdown\",\n    x = NULL,\n    y = \"Proportion of respondents\"\n  )\n\nplot_sim_breakdown <- ggplot(sim_breakdown) +\n  aes(x = Period, y = Value, group = Response, colour = Response) +\n  geom_point() +\n  stat_smooth(method = \"lm\") +\n  labs(\n    title = \"Simulated response breakdown\",\n    x = NULL,\n    y = \"Proportion of simulated respondents\"\n  )\n\nplot_actual_breakdown + plot_sim_breakdown\n```\n\n::: {.cell-output-display}\n![Comparison of actual and simulated responses over time.](index_files/figure-html/fig-sum-distibutions-1.png){#fig-sum-distibutions width=672}\n:::\n:::\n\n\n\nThis is encouraging! Remember the AI has _not_ been calibrated on the actual survey data, only on our media model, and yet its predictions are not too far from reality.\n\n## Digging deeper\n\nLet's dig into our simulated responses a bit.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(\n  article_responses |>\n    group_by(Response) |>\n    summarise(Probability = mean(Probability))\n) +\n  aes(y = forcats::fct_rev(Response), x = Probability, fill = Response) +\n  geom_col(show.legend = F) +\n  scale_x_continuous(limits = c(0, 1)) +\n  labs(x = \"Mean probability across all articles\", y = \"Response\")\n```\n\n::: {.cell-output-display}\n![Response distribution](index_files/figure-html/fig-response-distribution-1.png){#fig-response-distribution width=672}\n:::\n:::\n\n\n\nAgain, we haven't calibrated the LLM on real human responses, but among the headlines there are plenty about cancer, bone disease, and early death. \"Highly concerned\" is not unrealistic.\n\nThis suggests that we could be underweighting the influence of media in our model. Considering just the contribution of simulated respondents who were exposed to media shows how these respondents have influenced the overall trend.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(\n  sim_results |>\n    rename(Period = period) |>\n    group_by(Period, Response) |>\n    summarise(Value = sum(Probability) / N_SIM_READERS)\n) +\n  aes(x = Period, y = Value, group = Response, colour = Response) +\n  geom_point() +\n  stat_smooth() +\n  labs(x = NULL, y = \"Proportion of all respondents\")\n```\n\n::: {.cell-output-display}\n![Contribution of simulated readers to response trends.](index_files/figure-html/fig-media-sim-over-time-1.png){#fig-media-sim-over-time width=672}\n:::\n:::\n\n\n\nIt's interesting that the articles appear to have polarised the simulated readers, with both \"Highly concerned\" and \"Not concerned at all\" growing. The rise in \"Don't know\" might be confusion from contradictory articles, or might be due to articles that only mentioned ultra-processed foods in passing.\n\n# Where next?\n\nWe could make some big improvements:\n\n- Better results would come from refining our media model. It seems likely that we're underestimating the media influence.\n- So far we've only simulated thousands of \"Average person\" identities. Some basic demographic splits - the \"silicon sampling\" approach - would be expected to improve realism. That said, UPFs are not a highly polarised issue so the improvement would most likely be modest here.\n\nHowever, this little project could easily turn into a months-long labour. Time to stop, go outside, touch grass.\n\n# Wrapping up\n\nWe've covered a lot of ground here:\n\n1. Collated and visualised the FSA's tracker survey for concern about UPFs.\n2. Built a probabilistic media readership model that reflects real UK news consumption.\n3. Built an LLM-based survey respondent simulator.\n4. Combined the Monte Carlo media model and the LLM respondents to simulate how British people exposed to news media over the last 18 months would respond to the UPF question, i.e. simulated the FSA's tracker survey with AI.\n\nWe were following the ideas and approach outlined by Argyle et al, but with a much more niche survey question (British public opinion on UPFs vs US voting preference) and with a media model to capture the information that would truly inform opinion.\n\nThe simulated survey showed some promising correlation with the actual survey. It really is exciting considering the relative effort compared to polling a thousand of people every month. I can't wait to see how this develops.",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}