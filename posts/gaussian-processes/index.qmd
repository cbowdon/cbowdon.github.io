---
title: "Gaussian Processes in R"
date: "2025-01-25"
categories: [gaussian, bayesian, stan, r]
draft: true
freeze: false
---

I stumbled upon Gaussian Processes when looking into how the MIPRO prompt optimisation algorithm works, and it is a most fascinating idea. There's something about this way of modelling the world that really appeals to me.

This post is just following [this lesson](https://carpentries-incubator.github.io/statistical-probabilistic-programming-r/instructor/gaussian-processes.html).

# WTH are Gaussian Processes?

Imagine your data $y$ are noisy outputs of a function of $x$, which is the function you're trying to model.

$$
y \sim N(f(x), \sigma^2)
$$

We don't know $f(x)$, so we imagine that there's some distribution $GP$ that generates functions and the $f(x)$ we've observed is one function drawn from that distribution. $GP$ (the Gaussian Process) is parameterised by a mean, $\mu$ and a kernel, $K$.

$$
f(x) \sim GP(\mu, K)
$$


What's the point of this? Well it allows us to use our Bayesian toolbox (hello [Stan](https://mc-stan.org)) to refine our estimate of $GP$, and so the samples we draw from a posterior of $GP$ are closer to the true $f(x)$.

## The Kernel $K$

$K$ is the covariance of the Gaussian Process. There's some art to what function is chosen for $K$, but I don't want to get hung up on that at this point. Suffice it to say that the popular choice is squared exponential covariance, which is parameterised by $\alpha$ and $\lambda$.

$$
K(x, x') = \alpha^2 e^{\frac{(x - x')^2}{2\lambda}}
$$

## Visualising some samples of $f(x)$

```{r}
#| warning: false
set.seed(42)
library(tidyverse)

sq_exp_cov <- function(x, lambda, alpha) {
  n <- length(x)

  K <- matrix(0, n, n)

  for (i in 1:n) {
    for (j in 1:n) {
      diff <- sqrt(sum((x[i] - x[j])^2))
      K[i, j] <- alpha^2 * exp(-diff^2 / (2 * lambda^2))
    }
  }

  K
}


x <- 0:100
alpha <- 1
lambda <- 25

K <- sq_exp_cov(x, lambda, alpha)

GP <- function(n_samples) MASS::mvrnorm(n_samples, mu = rep(0, length(x)), Sigma = K)

samples <- GP(10) |>
  t() |>
  data.frame() |>
  mutate(x = x) |>
  pivot_longer(c(-x), names_to = "sample", values_to = "y")

ggplot(samples) +
  aes(x = x, y = y, group = sample, colour = sample) +
  geom_line(show.legend = FALSE) +
  labs(title = "Samples of f(x) from GP") +
  theme_minimal()
```

# Gaussian Process Regression - using data to refine $GP$

We've observed some (noisy) data from a mystery function.

```{r}
df6 <- data.frame(
  x = c(-2.76, 2.46, -1.52, -4.34, 4.54, 1),
  y = c(-0.81, -0.85, 0.76, -0.41, -1.48, 0.2)
)
```

We intend to predict the values of $y$ at some other $x$ beyond the ones we've observed.

We model it all in Stan.

```
{{< include f1.stan >}}
```


```{r}
library(cmdstanr)
check_cmdstan_toolchain(fix = TRUE, quiet = TRUE)

model <- cmdstan_model(stan_file = "gp.stan", exe = "gp.stan.bin")

samples <- model$sample(
  list(
    n_data = nrow(df6),
    x_data = as.array(df6$x),
    y_data = as.array(df6$y),

    sigma = 0.1,

    n_pred = length(-5:5),
    x_pred = -5:5,

    alpha = 1,
    lambda = 1
  )
)
```