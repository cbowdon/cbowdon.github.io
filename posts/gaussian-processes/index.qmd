---
title: "Gaussian Processes in R"
date: "2025-01-25"
categories: [gaussian, bayesian, stan, r]
draft: true
freeze: false
---

I stumbled upon Gaussian Processes when looking into how the MIPRO prompt optimisation algorithm works, and it is a most fascinating idea. There's something about this way of modelling the world that really appeals to me.

This post is just following [this lesson](https://carpentries-incubator.github.io/statistical-probabilistic-programming-r/instructor/gaussian-processes.html).

# WTH are Gaussian Processes?

Imagine your data $y$ are noisy outputs of a function of $x$, which is the function you're trying to model.

$$
y \sim N(f(x), \sigma^2)
$$

We don't know $f(x)$, so we imagine that there's some distribution $GP$ that generates functions and the $f(x)$ we've observed is one function drawn from that distribution. $GP$ (the Gaussian Process) is parameterised by a mean, $\mu$ and a kernel, $K$. It's a multivariate normal distribution, i.e. the normal distribution generalised to multiple dimensions.

$$
f(x) \sim GP(\mu, K)
$$


What's the point of this? Well it allows us to use our Bayesian toolbox (hello [Stan](https://mc-stan.org)) to refine our estimate of $GP$, and so the samples we draw from a posterior of $GP$ are closer to the true $f(x)$.

## The Kernel $K$

$K$ is the covariance of the Gaussian Process. There's some art to what function is chosen for $K$, but I don't want to get hung up on that at this point. Suffice it to say that the popular choice is squared exponential covariance, which is parameterised by $\alpha$ and $\lambda$.

$$
K(x, x') = \alpha^2 e^{\frac{(x - x')^2}{2\lambda}}
$$

## Visualising $f(x)$ samples for different kernels

If we implement the kernel and draw from $GP$, we can see some potential functions. Each draw from $GP$ will give us a vector of $f(x)$ for the input $x$ vector. We get a discrete approximation of the continuous function.

```{r}
#| label: fig-kernels
#| warning: false
set.seed(42)
library(tidyverse)

sq_exp_cov <- function(x, lambda, alpha) {
  n <- length(x)

  K <- matrix(0, n, n)

  for (i in 1:n) {
    for (j in 1:n) {
      diff <- sqrt(sum((x[i] - x[j])^2))
      K[i, j] <- alpha^2 * exp(-diff^2 / (2 * lambda^2))
    }
  }

  K
}

x <- 0:100

GP <- function(alpha, lambda, n_samples = 10) {
  K <- sq_exp_cov(x, lambda, alpha)
  mat <- MASS::mvrnorm(
    n_samples,
    mu = rep(0, length(x)), # Set the average level of the process to 0
    Sigma = K
  )
  t(mat) |>
    data.frame() |>
    mutate(alpha = alpha, lambda = lambda, x = x) |>
    pivot_longer(c(-x, -alpha, -lambda), names_to = "sample", values_to = "y")
}

grd <- expand.grid(alpha = c(0.5, 1, 2), lambda = c(1, 10, 100))
samples <- map2(grd$alpha, grd$lambda, GP) |> bind_rows()

labeller <- function(variable, value) {
  if (variable == "alpha") {
    sprintf("α = %s", value)
  } else {
    sprintf("λ = %s", value)
  }
}

ggplot(samples) +
  aes(x = x, y = y, group = sample, colour = sample) +
  facet_wrap(~ alpha * ~lambda, labeller = labeller) +
  geom_line(show.legend = FALSE) +
  labs(title = "Samples of f(x) from GP") +
  theme_minimal()
```

Varying $\alpha$ and $\lambda$ gives us an idea of how these hyperparameters change the functions we draw. $\alpha$ controls the standard deviation and $\lambda$ controls the correlation between $x$ points.

# Gaussian Process Regression - using data to refine $GP$

Imagine we've observed some (noisy) data from a mystery function.

```{r}
#| label: df6
df6 <- data.frame(
  x = c(-2.76, 2.46, -1.52, -4.34, 4.54, 1),
  y = c(-0.81, -0.85, 0.76, -0.41, -1.48, 0.2)
)
```

We intend to predict the values of $y$ at some other $x$ beyond the ones we've observed, to see what $f(x)$ might look like. We'll choose [-5, 5] as the domain.

```{r}
#| label: x-pred
x_pred <- seq(-5, 5, by = 0.25)
# you'll get a smoother approximation for smaller increments, but at increase computation cost
```

Then we model it all in Stan. Personally I find Stan models easier to read in the order `model`, `data`, `parameters`.

```stan
{{< include gp.stan >}}
``` 

The `model` section gives the high-level view of what we're doing here: simulating those equations from earlier. The main thing to understand is that the domain of GP is both the $x$ observations and the additional points. We're interested in the model's uncertainty about what $f$ looks like at the points we haven't observed.

Here's the R code to run that model. I prefer to use `CmdStanR` - [see here](../using-stan-from-r/) for more on that and some general tips.

```{r}
#| label: run-stan
#| echo: false
library(cmdstanr)
check_cmdstan_toolchain(fix = TRUE, quiet = TRUE)

model <- cmdstan_model(stan_file = "gp.stan", exe = "gp.stan.bin")

samples <- model$sample(
  list(
    n_data = nrow(df6),
    x_data = as.array(df6$x),
    y_data = as.array(df6$y),
    sigma = 0.1,
    n_pred = length(x_pred),
    x_pred = x_pred,
    alpha = 1,
    lambda = 1
  ),
  parallel_chains = 4,
  show_messages = FALSE # disabled to avoid polluting the blog post, should be TRUE
)

samples
```

The R-hat and ESS summaries tell you if the model has converged. This one has. Let's now plot the draws of $f(x)$ from the $GP$ posterior.

```{r}
#| label: fig-draws
x_vals <- c(df6$x, x_pred)

draws <- samples$draws(format = "draws_matrix") |>
  as_tibble() |>
  # Every row is a draw, which we number
  mutate(draw = 1:n()) |>
  # Since each column is an observation f(x) for x indices, we pivot
  pivot_longer(starts_with("f"), names_to = "x", values_to = "y") |>
  # And map the x index back to an x value
  mutate(
    idx = as.numeric(str_extract(x, "[0-9]+")),
    x = x_vals[idx],
    y = as.numeric(y)
  )

ggplot() +
  geom_line(data = draws, mapping = aes(x = x, y = y, group = draw), alpha = 0.01) +
  geom_point(data = df6, mapping = aes(x = x, y = y), colour = "red") +
  geom_text(data = mutate(df6, n = rank(x)), mapping = aes(x = x, y = y, label = n), nudge_x = 0, nudge_y = 1.1) +
  geom_segment(data = df6, mapping = aes(x = x, y = y, yend = y + 1), linetype = "dashed") +
  theme_minimal()
```

The observations are shown as red points, and I've numbered them for ease of reference. You can see the predictions of $f(x)$ (the black lines) don't deviate far from the observations, and some intervals between observations are very well-defined too. However for certain intervals they take a wide variety of lines, such as the interval crossing $x=0$. This is intuitive: imagine asking a group of humans to draw a smooth line between the points. Most of them would draw the same lines directly between points 2-3 and 4-5, but there would be much disagreement about what point to turn at between 3-4.

## The effect of noise

In that model $\sigma$ controls the amount of noise we believe we have on the observations. Increasing that number will increase the uncertainty of the model, and correspondingly will make it more likely the MCMC simulation diverges.

## Optimising with Cholesky decomposition

We can speed up the simulation with a trick call Cholesky decomposition. We decompose $K = LL^T$  and reparameterise $f = \mu + L\eta$. If $\eta$ is normally distributed with mean zero and unit variance, this is equivalent.

```stan
{{< include gp.stan >}}
``` 

```{r}
#| label: cholesky
model <- cmdstan_model(stan_file = "gp-cholesky.stan", exe = "gp-cholesky.stan.bin")

samples <- model$sample(
  list(
    n_data = nrow(df6),
    x_data = as.array(df6$x),
    y_data = as.array(df6$y),
    sigma = 0.1,
    n_pred = length(x_pred),
    x_pred = x_pred,
    alpha = 1,
    lambda = 1
  ),
  parallel_chains = 4,
  show_messages = FALSE # disabled to avoid polluting the blog post, should be TRUE
)

samples
```

If we run that version convergence is much faster.