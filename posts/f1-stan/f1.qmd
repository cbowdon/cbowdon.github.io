---
title: "Latent performance of F1 cars and drivers: fun with Bayesian models"
author: "Chris Bowdon"
date: "2024-12-27"
categories: [bayesian, r, f1]
warning: false
freeze: false
code-fold: true
---

For a bit of fun, let's try and model the performance of F1 drivers and constructors using Bayesian models. This is of course using data from the fantastic [Ergast](https://ergast.com/mrd/).

My model will be an imitation of the f1-metrics model. We have latent variables of driver skill and car quality, which predict points/positions. However I never did get hold of the paper for that, and am instead following the amazing [Rasmus Bååth's blog](https://www.sumsar.net/blog/2013/07/modeling-match-results-in-la-liga-part-one/) where he models the latent skill of football teams in La Liga.

I downloaded the CSVs from Ergast which we'll first wrangle into shape. The weapon of choice is R and the tidyverse, since nothing else is quite so ergonomic for this type of analysis. You can find the code behind the folds if you want to play along.

```{r}
#| label: data-loading
#| warning: false
library(tidyverse)
library(knitr)

qualifying <- read_csv("data/qualifying.csv") |> rename(quali_position = position)
races <- read_csv("data/races.csv") |>
  select(raceId, year, round, circuitId, name) |>
  rename(gp = name)
results <- read_csv("data/results.csv") |> select(resultId, raceId, driverId, constructorId, grid, position, points, milliseconds)
constructors <- read_csv("data/constructors.csv") |> select(constructorId, constructorRef)
drivers <- read_csv("data/drivers.csv") |> select(driverId, driverRef)
circuits <- read_csv("data/circuits.csv") |> select(circuitId, circuitRef)

f1data <- races |>
  merge(results) |>
  merge(qualifying) |>
  merge(constructors) |>
  merge(drivers) |>
  merge(circuits) |>
  select(raceId, gp, year, round, circuitRef, constructorRef, driverRef, q1, q2, q3, quali_position, grid, position) |>
  mutate(position = ifelse(position == "\\N", NA, as.numeric(position)))
```

In the interests of being educational - and because Lord knows I hate editing - I am writing this post as I go, including any blind-alleys and debugging.

Let's start with some visualisations of the data we have.

```{r}
#| label: fig-data-2024
#| fig-cap: Histograms of driver performance in F1 2024.
f2024 <- f1data |> filter(year == 2024)

ggplot(f2024) +
  aes(x = position) +
  facet_wrap(vars(driverRef)) +
  geom_histogram(fill = "dodgerblue4", alpha = 0.5, colour = "black", linewidth = 0.1, binwidth = 1)
```

The model goes like this. Points are simulated as a draw from a Poisson distribution parameterised by the driver performance and constructor performance.

$$
\text{points} \sim \text{Poisson}(\text{perf}_{\text{drv}} + \text{perf}_{\text{ctr}})
$$

Driver performance is simulated as a draw from a normal distribution.

$$
\text{perf}_{\text{drv}} \sim \text{Normal}(\mu_{\text{drv}}, \sigma_{\text{drv}}^2)
$$

Likewise constructor performance is simulated as a draw from a normal distribution.

$$
\text{perf}_{\text{ctr}} \sim \text{Normal}(\mu_{\text{ctr}}, \sigma_{\text{ctr}}^2)
$$

We will therefore have a likelihood function that is the product of the Poisson density of the points for each driver for each race. The number of parameters is very high, as a performance score for each driver and for each constructor.

```{r}
#| label: log-likelihood-1
log_likelihood <- function(drv_race_pts, drv_perf, ctr_perf) {
  ... # whew
}
```

# Constructor performance

Let's start simpler, with a view of just the constructor performance. Assume that at least one driver for every constructor maximised the car's performance each weekend. THIS ISN'T TRUE. If Albon had a poor weekend, Sargeant wasn't going to step up, as one of many examples on the grid in 2024. However it is a useful approximation that let's us analyse just the constructor performance.

```{r}
#| label: fig-simpler-model
#| fig-cap: Histogram of re-ranked constructor results in F1 2024.
ctr_positions <- f2024 |>
  group_by(round, constructorRef) |>
  summarise(quali_position = min(quali_position), position = min(position)) |>
  mutate(
    # re-rank
    quali_position = rank(quali_position),
    position = rank(position)
  ) |>
  arrange(round, position)

ggplot(ctr_positions) +
  aes(x = position) +
  facet_wrap(vars(constructorRef)) +
  geom_histogram(fill = "dodgerblue4", alpha = 0.5, colour = "black", linewidth = 0.1, binwidth = 1)
```

A key point here is that we've recalculated positions using only the max driver's position for each constructor, i.e. there are now only 10 finishing positions.

Again we can simulate the position $p$ as a draw from a Poisson distribution, this time where $\lambda_\text{ctr}$ is the performance of the constructor only.

$$
p \sim \text{Poisson}(\lambda_{\text{ctr}})
$$

To keep things simple, let's start with an uninformative prior, a uniform distribution for the performance of the constructors.

$$
\lambda_{ctr} \sim \text{Uniform}(1, 10)
$$

Then our likelihood function is much simpler. (We won't actually need this, but I find it useful to write out for the sake of understanding.)

```{r}
#| label: log-likelihood-2
log_lik_ctr <- function(ctr_positions, ctr_perfs) {
  ctrs <- sort(unique(ctr_positions$constructorRef))
  log_lik <- 1.0
  for (i in 1:length(ctrs)) {
    race_posns <- filter(ctr_positions, constructorRef == ctrs[i])$position
    log_lik <- log_lik + sum(dpois(race_posns, lambda = ctr_perfs[i], log = TRUE))
  }
  log_lik
}
```

Let's throw Stan at this now. Here's our Stan model file.

```
{{< include f1.stan >}}
```

And here's the R code for interacting with it via CmdStanR. I settled on CmdStanR rather than RStan after hitting too many nameless runtime errors through RStan - see [here](/posts/using-stan-from-r/).

```{r}
#| label: run-stan
#| warnings: false
library(cmdstanr, quietly = TRUE)
check_cmdstan_toolchain(fix = TRUE, quiet = TRUE)

ctrs <- sort(unique(ctr_positions$constructorRef))
ctr_index <- tibble(
  constructorId = 1:length(ctrs),
  constructorRef = ctrs,
)

make_data_list <- function(ctr_positions) {
  tidy_ctr_posns <- ctr_positions |> merge(ctr_index)

  data_list <- list(
    n_ctrs = n_distinct(tidy_ctr_posns$constructorId),
    n_obs = nrow(tidy_ctr_posns),
    ctrs = tidy_ctr_posns$constructorId,
    positions = tidy_ctr_posns$position
  )
}

run_stan <- function(data_list, model_file = "f1.stan") {
  mod <- cmdstan_model(model_file, exe_file = str_c(model_file, ".bin"))
  mod$sample(data_list, seed = 42, show_messages = FALSE)
}

fit <- run_stan(make_data_list(ctr_positions))
fit
```

Let's tidy that up though, mapping parameters back to constructor names.

```{r}
#| label: fig-ctr-lambda-posteriors
#| fig-cap: Posterior samples for constructor performance
sample_posterior <- function(fit) {
  df <- fit$draws("lambda", format = "draws_matrix") |> as_tibble()
  colnames(df) <- ctrs
  df |>
    pivot_longer(
      all_of(ctrs),
      names_to = "constructorRef", values_to = "position"
    ) |>
    mutate(position = as.double(position))
}

plot_posterior <- function(fit) {
  ggplot(sample_posterior(fit)) +
    aes(x = position) +
    facet_wrap(vars(constructorRef)) +
    geom_density(fill = "dodgerblue4", alpha = 0.5, colour = "black", linewidth = 0.1)
}

plot_posterior(fit)
```

The interesting thing is that this suggests the Red Bull was clearly the third-fastest car. I'm a little suspicious of that, because it claimed a sequence of 1-2s in the first third of the season. Let's take a closer look at that.

The first thing to check is whether my dumb "max driver, 10 positions" model is problematic. Let's look at Red Bull's results.

```{r}
#| label: tbl-red-bull-results
#| tbl-cap: Red Bull's results in F1 2024

f2024 |>
  filter(constructorRef == "red_bull") |>
  select(gp, round, driverRef, position) |>
  pivot_wider(names_from = "driverRef", values_from = "position") |>
  arrange(round) |>
  merge(
    ctr_positions |> filter(constructorRef == "red_bull") |> select(round, position)
  ) |>
  knitr::kable()
```

Round three is clearly a balls-up: how can Pere'z 5th place become the 10th constructor position? Let's work through it again.

```{r}
f2024 |>
  group_by(round, constructorRef) |>
  summarise(position = min(position)) |>
  mutate(reranked = rank(position)) |>
  filter(round == 3) |>
  knitr::kable()
```

`NA` strikes again! Forgot to add the crucial `na.rm` parameter to the min. Right, a do-over.

```{r}
#| label: fig-updated-ctr-positions
#| fig-cap: Constructor position histograms after fixing NA sorting.
#| warning: false
ctr_positions <- f2024 |>
  group_by(round, constructorRef) |>
  summarise(
    # in the event of a double DNF, assign the last position
    quali_position = pmin(min(quali_position, na.rm = TRUE), 10),
    position = pmin(min(position, na.rm = TRUE), 10)
  ) |>
  mutate(
    # re-rank
    quali_position = rank(quali_position),
    position = rank(position)
  ) |>
  arrange(round, position)

ggplot(ctr_positions) +
  aes(x = position) +
  facet_wrap(vars(constructorRef)) +
  geom_histogram(fill = "dodgerblue4", alpha = 0.5, colour = "black", linewidth = 0.1, binwidth = 1)
```

That looks more like I'd expect. Let's try Stan again.

```{r}
#| label: fig-updated-posterior-samples
#| fig-cap: Posterior samples after fixing NA sorting.
fit <- run_stan(make_data_list(ctr_positions))
plot_posterior(fit)
```

That is more intuitive: Red Bull is now much closer to McLaren and marginally ahead of Ferrari (despite coming third in the constructor's championship). This tells us that yes, Verstappen probably would have won the driver's championship in a McLaren - but contrary to claims, not in a Ferrari, since it wasn't faster than the Red Bull overall.

There's a lot we can improve about this model:

- Our priors for constructor performance are uniformative, and so our posteriors basically just reflect the data. Can we add some useful information to inform the model?
- The cars were regularly upgraded throughout the season, and received "big" updates at some races. There were also some key events like Newey leaving Red Bull. Can we model this?

## More informative priors

What makes some constructors better than others? ~~Adrian Newey~~ Money. Let's reflect this in our priors.

$$
\lambda_{\text{ctr}} \sim \text{Normal}(\text{budget}_\text{ctr}, \sigma)
$$

### Budget data

This is a bit of a secret, so we have to estimate. We do know that the budget cap is 135M USD and it's unlikely that anyone is operating below the cap. We also know that driver and top executive salaries are exempt from the cap. The top teams are spending about 100M USD on their drivers, and prior to the budget cap's implementation likely invested in facilities that they can use for free (like wind tunnels).

With that in mind, I asked 4o to search the web and estimate budgets for the teams. I sanity checked against [this Blackbook Motorsport](https://www.blackbookmotorsport.com/content/f1-team-finances-2023-financial-results-revenue-profit-budget-cap/) article.

These are probably reasonable estimates, and if we put a reasonable variance on the priors it will reflect the uncertainty.

```{r}
#| label: tbl-ctr-budgets
#| tbl-cap: Constructor budget estimates (USD millions)
budgets <- tribble(
  ~constructorRef, ~budget,
  "red_bull", 400,
  "mercedes", 400,
  "ferrari", 400,
  "mclaren", 250,
  "aston_martin", 250,
  "alpine", 200,
  "rb", 150,
  "haas", 150,
  "williams", 150,
  "sauber", 150
) |>
  mutate(
    norm_budget = 1 - budget / max(budget)
  )
knitr::kable(budgets)
```

Let's update our model and run Stan again.

```
{{< include f1.b.stan >}}
```

```{r}
#| label: fig-run-stan-b
#| fig-cap: Posterior samples from the model with budget-based priors.
data_list_b <- make_data_list(ctr_positions)
data_list_b$ctr_budgets <- 10 * arrange(budgets, constructorRef)$norm_budget
data_list_b$ctr_budget_sd <- 1.5
fit.b <- run_stan(data_list_b, model_file = "f1.b.stan")

plot_posterior(fit.b)
```

This hasn't made much difference. McLaren has eased back towards Red Bull because their budget is smaller. You could say they did a lot better than Red Bull relative to their budget.

```{r}
#| label: tbl-mu-sig-fit-b
#| tbl-cap: Mean and SD from posterior sample.
sp <- sample_posterior(fit.b)
sp |>
  group_by(constructorRef) |>
  summarise(mu = mean(position), sigma = sd(position)) |>
  arrange(mu) |>
  knitr::kable()
```

This model suggests there's very little nothing in it between Red Bull and McLaren, though the posterior distribution is wider for the top three teams than the others so there is more uncertainty.

Was budget a bad choice for the prior? Not necessarily: this model of constructor performance incorporates an important factor that isn't clear from the results, but which should affect an objective assessment of which teams are better. Or to put it another way, if I were Max Verstappen I'd still rather be driving the Red Bull than the McLaren next year, because Red Bull's budget suggests a more capable team overall. (Not to mention the increased wind tunnel time that third place in the constructors' gets relative to the winner.)

```{r}
#| label: fig-plot-with-priors
#| fig-cap: Comparison of prior and posterior samples.
plot_posterior_with_prior <- function(ctrs, ctr_budgets, ctr_budget_sd, fit) {
  s_post <- sample_posterior(fit) |> mutate(sample = "posterior")

  ctr_index <- tibble(constructorRef = ctrs, constructorId = 1:length(ctrs))

  s_prior <- ctr_budgets |>
    lapply(\(mu) rnorm(100000, mu, ctr_budget_sd)) |>
    lapply(as_tibble) |>
    imap(\(t, i) mutate(t, constructorId = i, sample = "prior")) |>
    bind_rows() |>
    rename(position = value) |>
    merge(ctr_index)

  df <- bind_rows(s_prior, s_post)

  ggplot(df) +
    aes(x = position, fill = sample) +
    facet_wrap(vars(constructorRef)) +
    geom_density(alpha = 0.5, colour = "black", linewidth = 0.1) +
    scale_x_continuous(breaks = seq(1, 10, by = 2), limits = c(1, 10))
}

plot_posterior_with_prior(ctrs, data_list_b$ctr_budgets, data_list_b$ctr_budget_sd, fit.b)
```

Plotting the priors and posteriors together shows how McLaren over-performed and Mercedes continued to underperform.

### Past performance

Another, very easy way to set the priors is to consider the constructors' performance over the previous seasons with the same regulations, in this case 2022 and 2023 (the ground-effect era). This is really easy to do - we would just take a weighted mean of the constructors' finishing positions as mu for our priors, and perhaps set the variance proportional to the difference. Not particularly innovative or interesting though, so let's skip it.

### Two-by-two circuits

Long-time F1 fans will know that at some tracks, the cars almost always finish in pairs. Somehow at these circuits driver skill makes little difference. That's no fun to watch, but I wonder if it's useful for establishing car performance?

We'll start by looking back at historic race results to see if:

1. Some circuits are more likely to finish in two-by-two order. We can score this by the median difference in position between _competitive_ finishing drivers.
2. This order correlates well with the constructor rankings at the end of the season.

```{r}
#| label: fig-position-gaps
#| fig-cap: Distribution of finishing position gaps between teammates by circuit, with median line.

circuits2024 <- races |>
  merge(circuits) |>
  filter(year == 2024) |>
  select(circuitRef)

position_gaps <- f1data |>
  filter(year > 2014) |> # hybrid era onwards, to try and keep it relevant to modern F1 car sizes
  merge(circuits2024) |> # circuits from 2024 only
  select(year, round, circuitRef, constructorRef, driverRef, position) |>
  filter(position != "\\N") |> # ignore DNFs
  mutate(position = as.integer(position)) |>
  group_by(year, round, circuitRef, constructorRef) |>
  filter(n() == 2) |> # must be a double finish
  summarise(gap = max(position) - min(position), max_position = max(position)) |>
  group_by(circuitRef) |>
  mutate(median_gap = median(gap))

ggplot(position_gaps) +
  aes(x = gap) +
  facet_wrap(vars(circuitRef)) +
  geom_histogram(fill = "dodgerblue4", binwidth = 1) +
  geom_vline(mapping = aes(xintercept = median_gap), colour = "dodgerblue1")
```

Perhaps unsurprisingly, the gap between drivers of the same car is usually very low. I went back and forth on whether to discard cars that finished outside the top ten points-paying positions, as this often includes cars that were retired to save expense or were wounded but finished the race well below their potential for the sake of data gathering. In the end I kept them in because we need two-car finishes to calculate the statistics and setting a threshold would mean re-coding all the finish positions.

It turns out to make little difference anyway. The race with the highest probability of teammates finishing in consecutive positions is Suzuka, whether or not you apply a threshold.

```{r}
#| label: tbl-position-gaps-medians
#| tbl-cap: Circuits with highest probability of having teammates finishing together.
rounds2024 <- races |>
  filter(year == 2024) |>
  merge(circuits) |>
  select(circuitRef, round) |>
  rename(round2024 = round)

p_gap_1 <- position_gaps |>
  count(circuitRef, gap) |>
  group_by(circuitRef) |>
  mutate(p_gap = n / sum(n)) |>
  filter(gap == 1) |>
  ungroup() |>
  merge(rounds2024)

p_gap_1 |>
  slice_max(p_gap, n = 10) |>
  kable()
```

How did the cars line up at Suzuka in 2024, noting that this was round 4 of a 24-round championship?

```{r}
#| label: tbl-suzuka-2024
#| tbl-cap: Results for Suzuka 2024 (two-car finishes only)
suzuka2024 <- f1data |>
  filter(year == 2024) |>
  filter(circuitRef == "suzuka") |>
  select(year, round, circuitRef, constructorRef, driverRef, quali_position, position)

suzuka2024 |>
  drop_na() |>
  group_by(constructorRef) |>
  filter(n() == 2) |>
  arrange(position) |>
  kable()
```

The cars are quite well in order here, if you ignore the gulf between Alonso and Stroll. This would suggest that in round 4, McLaren and Mercedes were quite evenly matched.

#### Counter-arguments

##### Suzuka is a track "drivers love"! Surely it's a more skill-dependent track?

I would have expected so, but statistically drivers are more likely to end up in pairs in Suzuka than anywhere else, which suggests the opposite. Suzuka is described as a technical track, with minimal room for error. Perhaps there aren't as many potential lines that a driver could take to distinguish themselves. It's also a good track for overtaking, so any driver that does qualify out of position is able to recover.

##### Suzuka was an early round. Perhaps car performance converges over the season?

On the one hand we'd expect to see teams converge on performance throughout the season, as the teams spot innovations on other cars that they can copy. Or secret design documents that they can copy, ahem. On the other hand, constructors in a tight battle would be motivated to develop their cars further - for example in 2022 when Red Bull overhauled an early Ferrari advantage.

Historically Suzuka has been mid-to-late season, which suggests that this isn't the reason.

```{r}
#| label: tbl-suzuka-round-numbers
#| tbl-cap: Suzuka's placement in each season
races |>
  filter(year >= 2014) |>
  group_by(year) |>
  mutate(n_rounds = n_distinct(round)) |>
  ungroup() |>
  merge(circuits) |>
  filter(circuitRef == "suzuka") |>
  select(year, round, n_rounds) |>
  arrange(year, round) |>
  kable()
```

##### Are we sure that consecutive finishes aren't indicative of driver skill?

The top teams can also hire the best drivers, is it not possible that we're still seeing the drivers finishing in skill order? Hence Alonso and Stroll? On the other hand, between 2023 and 2024 there were no line-up changes among the top 5 teams, so we can compare circuits across the two years.

```{r}
#| label: tbl-suzuka-2023
#| tbl-cap: Results for Suzuka 2024 (two-car finishes only)
suzuka2023 <- f1data |>
  filter(year == 2023) |>
  filter(circuitRef == "suzuka") |>
  select(year, round, circuitRef, constructorRef, driverRef, quali_position, position)

suzuka2023 |>
  drop_na() |>
  group_by(constructorRef) |>
  filter(n() == 2) |>
  arrange(position) |>
  kable()
```

The order of constructors is very different, despite all drivers being the same except for Alpha Tauri/RB. Just for fun, here's a side-by-side comparison for each driver.

```{r}
#| label: tbl-drivers-suzuka-2023-2024
#| tbl-cap: Drivers' results for Suzuka in 2023 and 2024
s24 <- suzuka2024 |>
  # reinstate the old names for joining
  # (yes I could have used constructorId, but opaque IDs are so tedious to handle)
  mutate(constructorRef = ifelse(constructorRef == "rb", "alphatauri", constructorRef)) |>
  mutate(constructorRef = ifelse(constructorRef == "sauber", "alfa", constructorRef)) |>
  select(constructorRef, driverRef, position) |>
  rename(position2024 = position)

s23 <- suzuka2023 |>
  select(constructorRef, driverRef, position) |>
  rename(position2023 = position)

merge(s23, s24, all = TRUE) |>
  arrange(position2023) |>
  kable()
```

More on this later.
