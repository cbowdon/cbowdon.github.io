---
title: "Latent skill of F1 drivers: fun with Bayesian models"
author: "Chris Bowdon"
date: "2024-12-27"
categories: [bayesian, r, f1]
warning: false
freeze: false
---

For a bit of fun, let's try and model the performance of F1 drivers and constructors using Bayesian models. This is of course using data from the fantastic [Ergast](https://ergast.com/mrd/).

```{r}
#| label: data-loading
#| warning: false
library(tidyverse)

qualifying <- read_csv("data/qualifying.csv") |> rename(quali_position = position)
races <- read_csv("data/races.csv") |>
  select(raceId, year, round, circuitId, name) |>
  rename(gp = name)
results <- read_csv("data/results.csv") |> select(resultId, raceId, driverId, constructorId, grid, position, points, milliseconds)
constructors <- read_csv("data/constructors.csv") |> select(constructorId, constructorRef)
drivers <- read_csv("data/drivers.csv") |> select(driverId, driverRef)

f1data <- races |>
  merge(results) |>
  merge(qualifying) |>
  merge(constructors) |>
  merge(drivers) |>
  select(raceId, gp, year, round, constructorRef, driverRef, q1, q2, q3, quali_position, grid, position) |>
  mutate(position = ifelse(position == "\\N", NA, as.numeric(position)))
```

My model is an imitation of the f1-metrics model. We have latent variables of driver skill and car quality, which predict points. However I never did get hold of the paper for that, and am instead following the amazing [Rasmus Bååth's blog](https://www.sumsar.net/blog/2013/07/modeling-match-results-in-la-liga-part-one/) where he models the latent skill of football teams in La Liga.

In the interests of being educational - and because Lord knows I hate editing - I am writing this post as I go, including any blind-alleys and debugging.

```{r}
#| label: fig-data-2024
#| fig-cap: Histograms of driver and constructor performance in F1 2024.
f2024 <- f1data |> filter(year == 2024)

ggplot(f2024) +
  aes(x = position) +
  facet_wrap(vars(driverRef)) +
  geom_histogram(fill = "dodgerblue4", alpha = 0.5, colour = "black", linewidth = 0.1, binwidth = 1)

ggplot(f2024) +
  aes(x = position) +
  facet_wrap(vars(constructorRef)) +
  geom_histogram(fill = "dodgerblue4", alpha = 0.5, colour = "black", linewidth = 0.1, binwidth = 1)
```

Points is simulated as a draw from a Poisson distribution parameterised by the driver performance and constructor performance.

$$
\text{points} \sim \text{Poisson}(\text{perf}_{\text{drv}} + \text{perf}_{\text{ctr}})
$$

Driver performance is simulated as a draw from a normal distribution.

$$
\text{perf}_{\text{drv}} \sim \text{Normal}(\mu_{\text{drv}}, \sigma_{\text{drv}}^2)
$$

Likewise constructor performance is simulated as a draw from a normal distribution.

$$
\text{perf}_{\text{ctr}} \sim \text{Normal}(\mu_{\text{ctr}}, \sigma_{\text{ctr}}^2)
$$

We will therefore have a likelihood function that is the product of the Poisson density of the points for each driver for each race. The number of parameters is very high, as a performance score for each driver and for each constructor.

```{r}
#| label: log-likelihood-1
log_likelihood <- function(drv_race_pts, drv_perf, ctr_perf) {
  ... # whew
}
```

# Car performance

Let's start simpler, with a view of the car performance. Assume that at least one driver for every constructor maximised the car's performance each weekend. THIS ISN'T TRUE. If Albon had a poor weekend, Sargeant wasn't going to step up, as one of many examples on the grid in 2024. However it is a useful approximation.

```{r}
#| label: fig-simpler-model
#| fig-cap: Histogram of re-ranked constructor results in F1 2024.
ctr_positions <- f2024 |>
  group_by(round, constructorRef) |>
  summarise(quali_position = min(quali_position), position = min(position)) |>
  mutate(
    # re-rank
    quali_position = rank(quali_position),
    position = rank(position)
  ) |>
  arrange(round, position)

ggplot(ctr_positions) +
  aes(x = position) +
  facet_wrap(vars(constructorRef)) +
  geom_histogram(fill = "dodgerblue4", alpha = 0.5, colour = "black", linewidth = 0.1, binwidth = 1)
```

A key point here is that we've recalculated positions using only the max driver's position for each constructor, i.e. there are now only 10 finishing positions.

Again we can simulate the position $p$ as a draw from a Poisson distribution, this time where $\lambda_\text{ctr}$ is the performance of the constructor only.

$$
p \sim \text{Poisson}(\lambda_{\text{ctr}})
$$

To keep things simple, let's start with an uninformative prior, a uniform distribution for the performance of the constructors.

$$
\lambda_{ctr} \sim \text{Uniform}(1, 10)
$$

Then our likelihood function is much simpler. (We won't actually need this, but I find it useful to write out for the sake of understanding.)

```{r}
#| label: log-likelihood-2
log_lik_ctr <- function(ctr_positions, ctr_perfs) {
  ctrs <- sort(unique(ctr_positions$constructorRef))
  log_lik <- 1.0
  for (i in 1:length(ctrs)) {
    race_posns <- filter(ctr_positions, constructorRef == ctrs[i])$position
    log_lik <- log_lik + sum(dpois(race_posns, lambda = ctr_perfs[i], log = TRUE))
  }
  log_lik
}
```

Let's throw Stan at this now. Here's our Stan model file.

```
{{< include f1.stan >}}
```

And here's the R code for interacting with it via CmdStanR. I settled on CmdStanR rather than RStan after hitting too many nameless runtime errors through RStan - see [here](/posts/using-stan-from-r/).

```{r}
#| label: run-stan
#| warnings: false
library(cmdstanr, quietly = TRUE)
check_cmdstan_toolchain(fix = TRUE, quiet = TRUE)

ctrs <- sort(unique(ctr_positions$constructorRef))
ctr_index <- tibble(
  constructorId = 1:length(ctrs),
  constructorRef = ctrs,
)

do.stan <- function(ctr_positions) {
  tidy_ctr_posns <- ctr_positions |> merge(ctr_index)

  data_list <- list(
    n_ctrs = n_distinct(tidy_ctr_posns$constructorId),
    n_obs = nrow(tidy_ctr_posns),
    ctrs = tidy_ctr_posns$constructorId,
    positions = tidy_ctr_posns$position
  )
  mod <- cmdstan_model("f1.stan", exe_file = "f1.bin")
  mod$sample(data_list, seed = 42, show_messages = FALSE)
}

fit <- do.stan(ctr_positions)
fit
```

Let's tidy that up though, mapping parameters back to constructor names.

```{r}
#| label: fig-ctr-lambda-posteriors
#| fig-cap: Posterior samples for constructor performance
plot_fit <- function(fit) {
  df <- fit$draws("lambda", format = "draws_matrix") |> as_tibble()
  colnames(df) <- ctrs

  ggplot(pivot_longer(df, all_of(ctrs))) +
    aes(x = value) +
    facet_wrap(vars(name)) +
    geom_histogram(fill = "dodgerblue4", alpha = 0.5, colour = "black", linewidth = 0.1, binwidth = 0.2)
}

plot_fit(fit)
```

The interesting thing is that this suggests the Red Bull was clearly the third-fastest car. I'm a little suspicious of that, because it claimed a sequence of 1-2s in the first third of the season. Let's take a closer look at that.

The first thing to check is whether my dumb "max driver, 10 positions" model is problematic. Let's look at Red Bull's results.

```{r}
#| label: tbl-red-bull-results
#| tbl-cap: Red Bull's results in F1 2024

f2024 |>
  filter(constructorRef == "red_bull") |>
  select(gp, round, driverRef, position) |>
  pivot_wider(names_from = "driverRef", values_from = "position") |>
  arrange(round) |>
  merge(
    ctr_positions |> filter(constructorRef == "red_bull") |> select(round, position)
  ) |>
  knitr::kable()
```

Round three is clearly a balls-up: how can Pere'z 5th place become the 10th constructor position? Let's work through it again.

```{r}
f2024 |>
  group_by(round, constructorRef) |>
  summarise(position = min(position)) |>
  mutate(reranked = rank(position)) |>
  filter(round == 3) |>
  knitr::kable()
```

`NA` strikes again! Forgot to add the crucial `na.rm` parameter to the min. Right, a do-over.

```{r}
#| label: fig-updated-ctr-positions
#| fig-cap: Constructor position histograms after fixing NA sorting.
#| warning: false
ctr_positions <- f2024 |>
  group_by(round, constructorRef) |>
  summarise(
    # in the event of a double DNF, assign the last position
    quali_position = pmin(min(quali_position, na.rm = TRUE), 10),
    position = pmin(min(position, na.rm = TRUE), 10)
  ) |>
  mutate(
    # re-rank
    quali_position = rank(quali_position),
    position = rank(position)
  ) |>
  arrange(round, position)

ggplot(ctr_positions) +
  aes(x = position) +
  facet_wrap(vars(constructorRef)) +
  geom_histogram(fill = "dodgerblue4", alpha = 0.5, colour = "black", linewidth = 0.1, binwidth = 1)
```

That looks more like I'd expect. Let's try Stan again.

```{r}
#| label: fig-updated-posterior-samples
#| fig-cap: Posterior samples after fixing NA sorting.
fit <- do.stan(ctr_positions)
plot_fit(fit)
```

That is more intuitive: Red Bull is now much closer to McLaren and marginally ahead of Ferrari (despite coming third in the constructor's championship).

That'll do for now, but there's a lot we can improve about this model.