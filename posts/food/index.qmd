---
title: "Ultra-processed food and AI polling simulation"
date: "2025-05-19"
categories: [food, uk, r, ai, llm, surveys]
code-fold: true
freeze: false
---

There's a little drip drip drip of scare stories about ultra-processed food (UPF) a phrase that I'd never heard until this year. I'm starting to get worried. Is everyone else worried? Well it turns out the Food Standards Agency (FSA) has been running a tracker survey containing this question since mid-2023, so we can find out.

An idea that's generating a lot of excitement recently is the suggestion that you can [simulate public opinion polls with AI](https://www.cambridge.org/core/journals/political-analysis/article/abs/out-of-one-many-using-language-models-to-simulate-human-samples/035D7C8A55B237942FB6DBAD7CAA4E49). The concept is very simple: you calibrate multiple LLM assistants to represent your different demographics and then sample their responses to questions. This is quicker and cheaper than polling real people, _if_ you can make it reliable.

That sounds incredibly fun! In this post I'll explore the FSA tracker survey data, and then build an AI simulation model.

# The survey data

First we need to scrape the survey data. While it's lovely that the FSA publishes all the monthly survey results (which were run by YouGov) it is annoying that they don't publish compiled statistics, so we need to get our tibbles dirty.

As usual, if you like coding with R you can see the gory details under the folds.

```{r}
#| label: download-data
#| warning: false
library(tidyverse)
library(httr2)
library(rvest)
library(knitr)

if (!str_ends(getwd(), "posts/food")) {
  setwd("posts/food")
}

download_files <- function() {
  url <- "https://data.food.gov.uk/catalog/datasets/0bfd916a-4e01-4cb8-ba16-763f0b36b50c"

  html_content <- request(url) |>
    req_perform() |>
    resp_body_string()

  # Assuming html_content is already defined
  doc <- read_html(html_content)
  xlsx_links <- doc %>% html_nodes("a[href$='.xlsx']") %>% html_attr("href")
  for (link in xlsx_links) {
    file_url <- link
    file_name <- basename(file_url)
    destfile <- sprintf("data/%s", file_name)
    if (!file.exists(destfile)) {
      download.file(url = file_url, destfile = destfile)
    }
  }
}

download_files()
```

(Is there an easy way to read tracker survey spreadsheets into R? They have a common format, but I never found anything that did the job and had to roll my own. Anyway, eventually we end up with a single dataset.)

```{r}
#| label: tbl-monthly-spreadsheets
#| tbl-cap: Example of the survey data.
#| warning: false
library(tidyverse)
library(readxl)
library(knitr)

extract_month_year <- function(file) {
  decoded_filename <- URLdecode(file)

  month_year_regex <- "^.*(January|February|March|April|May|June|July|August|September|October|November|December)\\s?(\\d{2}|\\d{4}).*.xlsx$"

  if (grepl(month_year_regex, decoded_filename)) {
    extracted_month_year <- regmatches(
      decoded_filename,
      regexec(month_year_regex, decoded_filename)
    )

    # Extract month and year
    month <- extracted_month_year[[1]][[2]]
    year <- extracted_month_year[[1]][[3]]

    if (str_length(year) == 2) {
      year <- paste0("20", year)
    }

    return(list(filename = file, month = month, year = year))
  } else {
    print(file)
    return(NULL)
  }
}

extract_question <- function(file, question_prefix) {
  fmy <- extract_month_year(file)

  data <- read_excel(
    file,
    sheet = "Percents",
    col_names = FALSE
  )
  headers <- as.vector(data[5, ], mode = "character")
  headers[1] <- "QCategory"
  headers <- zoo::na.locf(headers)
  subheaders <- as.vector(data[6, ], mode = "character")
  subheaders <- zoo::na.fill(subheaders, "")
  combined_headers <- map2(
    headers,
    subheaders,
    \(x, y) str_c(x, y, sep = ":: ")
  )
  combined_headers[1] <- "QCategory"
  combined_headers[2] <- "Total"

  colnames(data) <- combined_headers

  has_question <- any(str_starts(pull(data, QCategory), question_prefix))

  if (!is.na(has_question)) {
    data |>
      filter(!is.na(QCategory)) |>
      slice(
        which(str_starts(QCategory, question_prefix)):n()
      ) |>
      head(10) |> # take the cats
      tail(-1) |> # drop the header
      mutate(
        filename = fmy$filename,
        monthname = fmy$month,
        year = as.integer(fmy$year),
        month = match(
          monthname,
          c(
            "January",
            "February",
            "March",
            "April",
            "May",
            "June",
            "July",
            "August",
            "September",
            "October",
            "November",
            "December"
          )
        )
      )
  }
}

question_df <- function(question_prefix) {
  df_filename <- sprintf("data/%s.rds", question_prefix)
  if (file.exists(df_filename)) {
    df <- read_rds(df_filename)
  } else {
    xlsx_files <- list.files(
      path = "data",
      pattern = "*.xlsx",
      full.names = TRUE
    )
    df <- xlsx_files |>
      sort(decreasing = TRUE) |>
      lapply(\(f) extract_question(f, question_prefix)) |>
      keep(\(x) !is.null(x)) |>
      bind_rows() |>
      mutate(Question = question_prefix)

    df |> write_rds(df_filename)
  }
  df
}

df <- question_df("Q12_14")

tidy_df <- df |>
  filter(QCategory != "Unweighted base" & QCategory != "Base: All") |>
  pivot_longer(
    c(Total, contains("::")),
    names_to = "Demographic",
    values_to = "Value"
  ) |>
  mutate(
    Period = as.Date(sprintf("%04d-%02d-01", year, month)),
    Response = QCategory,
    Value = as.numeric(Value)
  ) |>
  select(Question, Demographic, Period, Response, Value) |>
  arrange(Question, Period, Response, Demographic)

tidy_df |> head() |> kable()
```

Finally we have âœ¨tidyâœ¨ data and we can visualise it.
  
```{r}
#| label: fig-responses
#| fig-cap: Tracker for "At the moment, how concerned, if at all, do you personally feel about ultra-processed, or over-processing of food?" Left is net concern, right is all responses.
#| warning: false
library(patchwork)

RESPONSE_CATS <- c(
  "Highly concerned",
  "Somewhat concerned",
  "Not very concerned",
  "Not concerned at all",
  "Don't know"
)

net_concern <- tidy_df |>
  filter(Demographic == "Total" & str_starts(Response, "Net: "))

breakdown <- tidy_df |>
  filter(Demographic == "Total" & !str_starts(Response, "Net: ")) |>
  mutate(
    Response = if_else(Response %in% RESPONSE_CATS, Response, "Don't know"),
    Response = factor(Response, levels = RESPONSE_CATS, ordered = TRUE)
  ) |>
  # Re-normalise after removing the IDK cat
  group_by(Question, Demographic, Period) |>
  mutate(Value = Value / sum(Value)) |>
  ungroup()

plot_net_concern <- ggplot(net_concern) +
  aes(x = Period, y = Value) +
  scale_y_continuous(limits = c(0, 1)) +
  geom_point() +
  stat_smooth(method = "lm") +
  labs(title = "Net concern", x = NULL, y = "Proportion of respondents")

plot_breakdown <- ggplot(breakdown) +
  aes(x = Period, y = Value, group = Response, colour = Response) +
  geom_point() +
  stat_smooth(method = "lm") +
  labs(title = "Response breakdown", x = NULL, y = "Proportion of respondents")

plot_net_concern + plot_breakdown
```

The number of people with concerns is evidently slowly increasing. A linear regression fits net concern very well, so it's reasonable to expect continued growth at the same rate (pending any intervention like policy changes). Presumably the line bends as we approach the inevitable core of stubborn sods who will never admit to being concerned about anything, but the data we have doesn't support that kind of model.

The breakdown by response type is interesting. We have to watch our step, given the noisiness, but it does seem to show a migration towards being _highly_ concerned.

Note that I have merged the two categories "I don't know enough to comment" and "Don't know" in the original data for two reasons: first, there's a technical limit of 5 categories coming later, and second there is very little difference between these two.

# The media landscape

Besides calibrating the simulated demographics, the major challenge is ensuring that the models are exposed to the same information. If interested in current events, this information is most likely not in the training data. This means you need to ensure the AI has been exposed to the same media.

Now, here's an interesting observation: a surprisingly small number of publications account for a large proportion of UK readership. See this data from [JournoFinder](https://journofinder.com/blog/uk-newspaper-circulation-figures).

![](https://framerusercontent.com/images/1BjmhUMPE6dsrYLtK1qkJV8p5k.png)

The UK population is approximately 70 million people. Given that the Guardian and the Daily Mail target very different demographics (left and right wing) we wouldn't expect much overlap between their readership, i.e. their combined monthly online readership alone probably covers half the news-reading British population. Note also that BBC News isn't included in that table, but has a similar level of traffic, albeit with higher overlap.

This suggests that if we're interested in understanding how the media shapes public perception in the UK, we can do well by analysing the output of just a handful of sources. This is particularly true in the modern media landscape, in which:

- Live television is a dying medium. (I was at a brilliant R meetup hosted by [Datacove](https://datacove.co.uk) recently when the session discussion turned to media channels. No one present had watched live TV in the last 24 hours, and only around 1 in 10 in the last month.)
- Radio has had its time, has had its power (though yet to have its finest hour ðŸŽ¸).
- Social media is increasingly popular, but news content on social media is dominated by reposts of articles from traditional journalists.

So let's create a corpus of news articles about ultra-processed food from the top 10 sources in the table above, plus the BBC. They have a combined readership of 64.8 million monthly readers in the UK (plus the BBC's unknown-but-high readership); even with high overlap, this clearly represents the majority of the UK population. It's enough to give us a solid understanding of what's going on.

I happen to work at [Polecat](https://www.polecat.com) who have licensed access to this data, so I can analyse it very easily. (Can't share the raw data here though. ðŸ™Š)

Since November 2023 there have been over 2.5 million articles from those sources, but only around 2000 mentioned ultra-processed food, and <500 seem to be focused on it, as opposed to mentioning it alongside other food or public health issues. Though all mentions would help with awareness, focus articles have a stronger effect on informing opinion.

```{r}
#| label: fig-article-counts
#| fig-cap: Counts of articles mentioning ultra-processed food from the top UK online news sources, by month.
#| warning: false
source("articles.R")

upf_articles <- load_upf_articles()
domain_counts <- load_domain_counts()

ggplot(
  upf_articles |>
    mutate(
      mention_type = if_else(is_focus, "Focus article", "Mentions UPFs")
    )
) +
  aes(x = period, group = is_focus) +
  geom_histogram(stat = "count") +
  facet_wrap(~mention_type) +
  labs(
    title = "Articles mentioning ultra-processed food from top UK sources, by month.",
    x = NULL,
    y = "Article count"
  )
```

N.B. The Sun and the Times have further licence restrictions, so aren't included here.

Although the number of articles focused on UPFs is not showing any trend, the number of articles mentioning UPFs is increasing over time. The fluctuation is most likely seasonal, and the trend is apparently upwards. There's _just_ about enough data to fix a SARIMA model. (We ought to have two years, we have eighteen months but the cycles seem to be sub-year.)

```{r}
#| label: fig-sarima
#| fig-cap: A (very uncertain) SARIMA fit of the mention counts.
#| warning: false
mention_counts <- ts(
  (group_by(upf_articles, period) |> count())$n,
  start = c(2024, 1),
  deltat = 1 / 12
)
afit <- forecast::auto.arima(mention_counts, d = 1, D = 1)
plot(forecast::forecast(afit, h = 5, level = 90))
```

It's tempting to look at the upward trend in mentions and the upward trend in net concern, and call it a day. Media mentions go up, net concern goes up, quod erat demonstratum! But this is a bit flimsy.

## Why can't we just count mentions and be done with it?

1. Even considering that we've already reasoned about UK audiences and scoped our search to selected top publications, this is still a poor model of readership. Every mention in every article counts equally, even though we can see that the Guardian has **over ten times** the number of readers the Financial Times does.
2. This model assumes that every mention of UPFs contributes an equal amount of concern. Some of them might be positive, or cause different levels of concern.
3. Net concern has increased by <5%, whereas the mentions have doubled. We have reason to be skeptical that mentions count is the whole story.

We need to take a more nuanced approach.

# A better readership model

Let's start with a look at how the articles are distributed amongst the sources.

```{r}
#| label: fig-source-dist
#| fig-cap: Articles focusing on ultra-processed food by source, since Jan 2024.
#| warning: false
ggplot(
  upf_articles |> group_by(domain, period) |> count()
) +
  aes(x = period, y = n, group = domain, colour = domain) +
  facet_wrap(~domain, nrow = 2) +
  geom_point(show.legend = F) +
  geom_line(linewidth = 1, show.legend = F)
```

Perhaps unsurprisingly, the Daily Mail has by far the most. Who'd have thought? They are also showing the most obvious increase in attention to the topic.  (Incidentally, the Daily Mail also has the most articles about cancer, bone disease, and early death. They must employ a lot of medical professionals.) To be completely fair to the Daily Mail though, we should note it is a smaller proportion of their total output, which is enormous.

```{r}
#| label: fig-source-count-context
#| fig-cap: Total number of articles output by top sources, since Jan 2024.
ggplot(
  group_by(domain_counts, domain) |> summarise(n = sum(n_articles))
) +
  aes(
    y = fct_reorder(domain, n),
    x = n,
    group = domain,
    colour = domain,
    fill = domain
  ) +
  geom_col(linewidth = 1, show.legend = F)
```

## Modelling

We now have enough information to build a Monte Carlo model of media exposure. We'll keep it as simple as we can without compromising too much on representativeness.

> Everything should be as simple as possible, but no simpler. (Einstein?)

Let's start by assuming the UK population is exposed to articles from each source in proportion to that source's estimated readership. We're going to get a bit maths now.

Let every reader $r$ sample a number of articles $n_s$ from each source $s$ according to a Poisson distribution - i.e. a natural discrete distribution of counts. The Poisson distribution is parameterised according to relative readership levels, so for example $\lambda_{BBC} > \lambda_{FT}$ and on average $n_{BBC} > n_{FT}$ but with lots of randomisation, so some of our simulated readers will draw more from the Mail, some more from the Metro, etc.

The reader then draws $n_s$ articles at random from each source. (This is a simplification of course.) To represent the fact that only a small proportion of articles from each source in each period are about UPFs, we draw the $n_s$ observations from a binomial distribution parameterised by $p_s$, the relative frequency of UPF articles for the source. The count of UPF-related articles we get back is denoted $d_s$.

$$
n_s \sim Pois(\lambda_s)
$$
$$
d_s \sim Binom(n_s, p_s)
$$

Since $p_s$ is very small, this will be very inefficient and we will mostly draw zeros. But maths comes to the rescue, because the unconditional distribution of $d_s$ can be given by:

$$
d_s \sim Pois(\lambda_s p_s)
$$

This is much faster to compute. We need to do so for each of our simulated readers $r$ to get a count of relevant articles they will read from each source. We then materialise these articles by sampling from the actual UPF articles from the soure in that time period, generate the LLM assessment for the articles, and take the mean assessment.

This is quite a complicated model, but it's also leaving A LOT out. We're ignoring the demographics of each source's readership, for example, and we're making the incorrect assumption that all articles are equally likely to be read. It would be better to have a multinomial distribution across the articles rather than our simple binomial. Ideally that would be informed by accurate article-level readership statistics, though those are difficult if not impossible to get for all sources. We're also ignoring incomplete reads or misreads.

```{r}
#| label: readership
#| warning: false

readership <- tibble(
  domain = factor(DOMAINS, levels = DOMAINS, ordered = TRUE),
  monthly_readers = 1000000 *
    c(59, 19.7, 12.3, 8.3, 5.9, 5.7, 4.6, 2.6, 2.5, 1.3) # BBC estimated from OFCOM survey
) |>
  merge(
    domain_counts |>
      group_by(domain, period) |>
      summarise(n_total_articles = sum(n_articles)),
    all.x = TRUE
  ) |>
  merge(
    upf_articles |> group_by(domain, period) |> summarise(n_upf_articles = n()),
    all.x = TRUE
  ) |>
  mutate(
    n_upf_articles = coalesce(n_upf_articles, 0),
    prob_upf = n_upf_articles / n_total_articles,
    # Number of articles in the time period, calibrated such
    # that the expected number of BBC articles is approx 1/day.
    # This is an educated guess, not much research to back it I'm afraid. Pew found in 2015 that
    # "An overwhelming majority of both long-form readers (72%) and short-form readers (79%)
    # view just one article on a given site over the course of a month on their cellphone."
    # - https://www.pewresearch.org/journalism/2016/05/05/long-form-reading-shows-signs-of-life-in-our-mobile-news-world/
    # That was specific to the heady earlier days of the mobile web though.
    # I found various industry-backed studies suggesting it could be higher, such as this NewsWorks study that
    # found young people reading 6/day, however this is really suspicious and they have an
    # obvious incentive to inflate the numbers.
    # https://pressgazette.co.uk/media-audience-and-business-data/media_metrics/young-people-news/
    lambda = 30.4 * monthly_readers / max(monthly_readers),
  )
```

I've had to make an educated guess on the expected number of articles read per month - details are in the code above.

We can now build the model using R's statistical functions and simulate a number of readers. The code for this is very simple.

```{r}
#| label: tbl-model
#| tbl-cap: Article count model
#| code-fold: false

set.seed(42)

sample_article_counts <- function(month_readership, n_sim_readers) {
  result <- matrix(
    ncol = length(DOMAINS),
    nrow = n_sim_readers,
    dimnames = list(1:n_sim_readers, DOMAINS)
  )
  for (i in 1:nrow(month_readership)) {
    source <- as.list(month_readership[i, ])
    n <- rpois(n_sim_readers, source$lambda * source$prob_upf)
    result[, i] <- n
  }
  result
}

PERIODS <- seq.Date(
  from = as.Date("2024-01-01"),
  to = as.Date("2025-05-01"),
  by = "1 month"
)

N_SIM_READERS = 10000

sim_counts <- PERIODS |>
  map(
    function(p) {
      sample_article_counts(
        month_readership = filter(readership, period == p),
        n_sim_readers = N_SIM_READERS
      )
    }
  )
```

We can examine how many articles on UPFs the readers are likely to read each month.

```{r}
#| label: tbl-article-reads-monthly
#| tbl-caption: Distribution of average number of UPF articles read per month
monthly_sim_count_totals <- sim_counts |> map(~ table(rowSums(.)))

monthly_sim_count_totals |>
  map(as.data.frame) |>
  bind_rows() |>
  rename(n_upf_articles_read_in_month = Var1) |>
  group_by(n_upf_articles_read_in_month) |>
  summarise(percent_of_readers = 100 * mean(Freq) / N_SIM_READERS) |>
  kable(digits = 1)
```

This suggests that around 4% of news readers will read an article on UPFs every month. That seems quite reasonable. The number has grown from around 2% to around 6% over the last year and a half. This is more realistic than our original model, which simply noted a doubling in the number of articles. Our more sophisticated model says the number of articles read has probably tripled, but that they only influence a very small proportion of readers.

A little reminder: we have modelled just the top 10 UK news sources, which have orders of magnitude more eyeballs than all other online news sources. We would expect the contribution from local news, blogs, trade pubs, etc. to be negligible.

```{r}
#| label: fig-growth-in-readers
#| fig-cap: Change in percent of simulated readers who read at least one UPF article in a month over time.
upf_readership_over_time <- map2(
  PERIODS,
  monthly_sim_count_totals,
  function(p, t) {
    tibble(
      period = p,
      pc_sim_readers_reading_at_least_one_article = 100 *
        (N_SIM_READERS - t[[1]]) /
        N_SIM_READERS
    )
  }
) |>
  bind_rows()

ggplot(upf_readership_over_time) +
  aes(x = period, y = pc_sim_readers_reading_at_least_one_article) +
  geom_line() +
  scale_y_continuous(
    limits = c(0, 20),
    breaks = seq(0, 20, 10),
    minor_breaks = seq(0, 20, 5)
  ) +
  labs(
    x = NULL,
    y = "Percent of simulated readers",
  )
```

By examining the model parameters, we can see that the sources that contribute most are the Guardian, the Telegraph, and the BBC.

```{r}
#| label: tbl-domain-params
#| tbl-cap: Mean monthly model parameters for each domain.
#|
readership |>
  group_by(domain) |>
  summarise(
    lambda = mean(lambda),
    prob_upf = mean(prob_upf),
    monthly_readers = mean(monthly_readers),
    monthly_total_articles = mean(n_total_articles),
    monthly_upf_articles = mean(n_upf_articles),
    exp_monthly_upf_articles = mean(lambda * prob_upf)
  ) |>
  mutate(
    exp_monthly_upf_articles = exp_monthly_upf_articles /
      sum(exp_monthly_upf_articles)
  ) |>
  arrange(desc(exp_monthly_upf_articles)) |>
  kable(digits = c(0, 1, 4, 0, 0, 0, 2))
```

Again this differs significantly from our simpler model, which would have pinned it all on the Daily Mail, as that source has the largest absolute number of UPF articles. The Telegraph is initially surprising given their comparatively low readership, but they have a very high probability of producing UPF articles.

With a more realistic model of readership achieved, we can move on to the AI polling. 

# Building the AI's world view

The next step is to sample specific articles for each reader given their counts. In our model, these are the articles that will influence opinions, i.e. those which inform the LLM survey respondents.

```{r}
#| label: tbl-sampled-articles
#| tbl-cap: Sampled articles for each reader

sample_index <- expand.grid(
  period = 1:length(PERIODS),
  reader = 1:N_SIM_READERS,
  domain = 1:length(DOMAINS)
)

sampled_articles <- 1:nrow(sample_index) |>
  map(
    function(i) {
      p <- sample_index[i, 1]
      r <- sample_index[i, 2]
      s <- sample_index[i, 3]

      n <- sim_counts[[p]][r, s]

      if (n > 0) {
        upf_articles |>
          filter(period == PERIODS[p] & domain == DOMAINS[s]) |>
          slice_sample(n = n, replace = FALSE) |>
          mutate(period = PERIODS[p], reader = r, n = n)
      }
    },
    .progress = T
  ) |>
  bind_rows()

print(
  sprintf(
    "Percent of readers who have read at least one UPF article in the total period: %.1f%%",
    100 * n_distinct(sampled_articles$reader) / N_SIM_READERS
  )
)

head(sampled_articles)
```

Aggregating this table this tells us that 50% of our reading population have read at least one article about UPFs in the last year and a half.

Finally we're ready to see what effect these articles have on the readers. Break out the AI!
  
# Asking the AI questions

Here's an exciting GPU-based statistical model bit (ooh, sexy) as a break from that awful CPU-based statistical model bit (ugh, dull).

R afficionados, we are using [ellmer](https://ellmer.tidyverse.org), which is more or less the de-facto AI package for R. The approach we take is to force the LLM into picking a category via structured outputs, and then reviewing the probabilities that it would have picked each category.

```{r}
#| label: ai-sim-functions
library(ellmer)

response_cats <- tibble(
  # Each cat is a single token!
  Cat = LETTERS[1:5],
  Response = RESPONSE_CATS
)

llm_answer_distribution <- function(
  articles,
  simulated_identity = "Statistically average person"
) {
  chat <- chat_openai(
    #base_url = "http://localhost:8080/v1/",
    #model = "gemma-3-1b-it",
    model = "gpt-4.1-nano",
    system_prompt = sprintf(
      "You are a member of the British public, with your own life experiences and opinions.

    This is your identity: 
    <identity>
    %s
    </identity>
    
    You are being asked for your views as part of a survey. Respond to each question from the pollster.",
      simulated_identity
    ),
    params = params(
      # Single output
      n = 1,
      # Get max available log probs
      log_probs = TRUE,
      top_logprobs = 5,
      # Keep the full distribution for sampling
      top_p = 1,
      # Do not adjust the distribution
      # (Whether log probs are pre- or post- temperature depends on the provider)
      temperature = 1,
      # Reproducibility
      seed = 42,
    )
  )

  headlines <- articles |>
    map(function(a) {
      sprintf(
        "HEADLINE: %s (%s, %s)",
        a$headline,
        a$source,
        format.Date(a$date, "%d %b %Y")
      )
    })

  # Now we need a specific answer.
  response <- chat$chat_structured(
    sprintf(
      "Consider the following news articles then answer the question.

<articles>
%s
</articles>

QUESTION: At the moment, how concerned, if at all, do you personally feel about ultra-processed, or over-processing of food?
  
A) Highly concerned
B) Somewhat concerned
C) Not very concerned
D) Not concerned at all
E) Don't know

Respond with the letter of your chosen answer.",
      paste(headlines, sep = "\n")
    ),
    type = type_enum(description = "Answer letter", values = LETTERS[1:5])
  )

  answer_logprobs <- chat$last_turn()@json$choices[[1]]$logprobs$content[[
    4 # token 4 is our letter, and the only token that changes
  ]]$top_logprobs

  answer_probs <- tibble(
    token = sapply(answer_logprobs, \(x) x$token),
    logprob = sapply(answer_logprobs, \(x) x$logprob)
  ) |>
    mutate(Probability = exp(logprob)) |>
    rename(Cat = token) |>
    merge(response_cats) |>
    select(Response, Probability)
}
```

We can now get the probability that the LLM would have selected each response in this conversation, given its identity.

Note that I've removed the response category "I don't know enough to comment" for the practical reason that only 5 top log probs are available from the LLM.

```{r}
#| label: tbl-llm-simulation
#| tbl-cap: Example of simulated AI polling response to a fictional alarming headline.
#| code-fold: false
fake_article <- list(
  list(
    headline = "Ultra-processed food linked to potential bowel cancer risk, scientists say",
    source = "Daily Mail",
    date = as.Date("2024-05-01")
  )
)

llm_answer_distribution(fake_article) |> kable(digits = 3)
```

 We can also manipulate that identity to get a different response distribution.

```{r}
#| label: tbl-hypochondriac
#| tbl-cap: Response distribution of LLM again, this time with "hypochondriac" identity.
llm_answer_distribution(fake_article, simulated_identity = "Hypochondriac") |>
  kable(digits = 3)
```

A basic model, but already quite interesting. Let's test it for bias.

```{r}
#| label: tbl-gender-test
#| tbl-cap: Response distribution of LLM for different genders.

gender_resp <- c("British man", "British woman") |>
  map(
    ~ llm_answer_distribution(fake_article, simulated_identity = .) |>
      mutate(simulated_identity = .)
  ) |>
  bind_rows()

gender_resp |>
  pivot_wider(
    id_cols = Response,
    names_from = simulated_identity,
    values_from = Probability
  ) |>
  kable(digits = 3)
```

Wow. Stereotype much? But to be brutally honest, that's kind of the point of AI simulated polling, to exploit the biases to model "typical" responses. If we look at the actual responses by gender, we can see that there really is a gender difference in the direction the model predicts (though not as pronounced).

```{r}
#| label: fig-actual-gender-split
#| fig-cap: Actual response distribution for different genders. Females are more likely to say they are "highly concerned" about ultra-processed foods.

gender_breakdown <- tidy_df |>
  filter(str_starts(Demographic, "Gender") & !str_starts(Response, "Net: ")) |>
  mutate(
    Response = if_else(Response %in% RESPONSE_CATS, Response, "Don't know"),
    Response = factor(Response, levels = RESPONSE_CATS, ordered = TRUE),
    Demographic = str_replace(Demographic, "Gender:: ", "")
  ) |>
  # Re-normalise after removing the IDK cat
  group_by(Question, Demographic, Period) |>
  mutate(Value = Value / sum(Value)) |>
  ungroup()

plot_gender_dist <- ggplot(
  gender_breakdown |>
    group_by(Demographic, Response) |>
    summarise(Value = mean(Value)) |>
    mutate(Value = if_else(Demographic == "Male", Value, -Value))
) +
  aes(x = Value, y = fct_rev(Response), fill = Demographic) +
  scale_fill_manual(values = c("Male" = "orange", "Female" = "purple")) +
  geom_col() +
  labs(
    title = "Gender response distribution",
    x = "Mean proportion of respondents",
    y = NULL
  )

plot_gender_time <- ggplot(gender_breakdown) +
  aes(x = Period, y = Value, group = Response, colour = Response) +
  facet_wrap(~Demographic) +
  geom_point() +
  stat_smooth(method = "lm") +
  labs(
    title = "Response breakdown over time",
    x = NULL,
    y = "Proportion of respondents"
  )

plot_gender_dist + plot_gender_time + plot_layout(nrow = 2, ncol = 1)
```

To make a useful gender comparison, we'd need to calibrate the LLM such that it's predictions are consistent with known gender biases. One way to achieve this would be rescaling the response distribution.

# Running AI over the dataset

Let's apply this to our articles. We can use `furrr` to run every simulated survey respondent in parallel. We'll start with a single "average person" identity.

```{r}
#| label: dataset-to-run

samples_per_reader <- sampled_articles |>
  group_by(period, reader) |>
  summarize(
    article_ids = list(sort(article_id)),
    n_article = n_distinct(article_id),
  )
```

```{r}
#| label: tbl-example
#| tbl-cap: Example of the simulated response.
#| warning: false
library(furrr)

sim_ident <- "Average person"

simulate_responses <- function(articles, sim_ident) {
  datafile <- sprintf("data/article-responses - %s.rds", sim_ident)

  if (!file.exists(datafile)) {
    rows <- lapply(1:nrow(articles), function(i) as.list(articles[i, ]))

    with(plan(multisession, workers = 4), {
      tasks <- future_map(
        rows,
        function(row) {
          dist <- llm_answer_distribution(
            headline = row$Title,
            source = row$Domain,
            date = row$Date,
            simulated_identity = sim_ident
          )
          dist |>
            mutate(ArticleId = row$ArticleId, simulated_identity = sim_ident)
        }
      )
    })

    article_responses <- bind_rows(tasks) |>
      merge(articles) |>
      as_tibble() |>
      mutate(
        # responses as ordered factor for plotting
        Response = factor(
          Response,
          RESPONSE_CATS,
          ordered = TRUE
        )
      )

    article_responses |> write_rds(datafile)
  }
  read_rds(datafile)
}

article_responses <- simulate_responses(articles, sim_ident) |>
  merge(select(articles, ArticleId)) |>
  as_tibble()

article_responses |>
  group_by(ArticleId) |>
  filter(Probability == max(Probability)) |>
  head() |>
  kable(digits = 3)
```

Let's do the simplest thing possible: how does the trend in simulated responses compare to the trend in actual survey responses? Note that the implicit model here is that everyone reads everything and their opinion is the average of what they read.

```{r}
#| label: fig-probability
#| fig-cap: Simulated responses (probability) and actual responses over time.
#| warning: false

period_dist <- article_responses |>
  mutate(Period = floor_date(Date, unit = "month") + duration(1, "month")) |>
  group_by(Period, Response) |>
  summarise(Probability = mean(Probability), nArticles = n_distinct(ArticleId))

period_dist_net <- period_dist |>
  filter(Response %in% c("Highly concerned", "Somewhat concerned")) |>
  group_by(Period) |>
  summarise(Probability = sum(Probability))

plot_net5 <- ggplot(net_concern) +
  aes(x = Period, y = Value) +
  geom_point() +
  stat_smooth(method = "lm") +
  scale_y_continuous(limits = c(0, 1)) +
  labs(title = "Net concern", x = NULL, y = "Proportion of respondents")

plot_sim_net <- ggplot(period_dist_net) +
  aes(x = Period, y = Probability) +
  geom_point() +
  stat_smooth(method = "lm") +
  scale_y_continuous(limits = c(0, 1)) +
  labs(
    title = "Simulated net concern",
    x = NULL,
    y = "Proportion of respondents"
  )

plot_breakdown5 <- ggplot(breakdown) +
  aes(
    x = Period,
    y = Value,
    colour = Response,
    fill = Response
  ) +
  geom_point(show.legend = FALSE) +
  stat_smooth(method = "lm", show.legend = FALSE) +
  labs(title = "Response breakdown", x = NULL, y = "Proportion of respondents")

plot_sim_breakdown <- ggplot(period_dist) +
  aes(
    x = Period,
    y = Probability,
    group = Response,
    colour = Response,
    fill = Response
  ) +
  geom_point(show.legend = FALSE) +
  stat_smooth(method = "lm") +
  labs(
    title = "Simulated response breakdown",
    x = NULL,
    y = "Proportion of respondents"
  )

plot_net5 +
  plot_sim_net +
  plot_breakdown5 +
  plot_sim_breakdown +
  plot_layout(ncol = 2, nrow = 2)
```

This is a mixed bag. The trends do align, although it's clearly miscalibrated. There's also a lot of noise.

## Digging deeper

If we look at the distribution of responses, we notice that the majority of articles generate a "Somewhat concerned" response. 

```{r}
#| label: fig-response-distribution
#| fig-cap: Response distribution
ggplot(
  article_responses |>
    group_by(Response) |>
    summarise(Probability = mean(Probability))
) +
  aes(y = forcats::fct_rev(Response), x = Probability, fill = Response) +
  geom_col(show.legend = F) +
  scale_x_continuous(limits = c(0, 1)) +
  labs(x = "Mean probability across all articles", y = "Response")
```

Now, we haven't calibrated the LLM on real human responses, but also if you just look at the headlines, anyone who takes them as fact most probably _would_ be concerned. They're all about cancer, bone disease, and early death. So arguably lack of calibration isn't our biggest problem here. The LLM might not be calibrated for the difference between "somewhat" and "highly" concerned, but it does accurately identify which articles are concern-raising. The problem is that the articles are _overwhelmingly_ concerning.

The thing that would make most difference to perception is how much the respondent trusted the headline, which itself is tightly coupled to which headlines the respondent is exposed to, because in practice people rarely read newspapers they don't trust.

To move past this, we're going to have to build a model with some weighted contribution from each publisher each month. We could go nuts with a complex model of what people are exposed to, but...



#### NEXT STEPS

- Build an exposure distribution.
- Start thinking about cumulative effects/time. The easiest way to do this is to adopt the same monthly resolution as the survey (remembering that the survey must lag the media).
- Include some weights we can train in this model, learn them, and try prediction.
- Maybe it's a Bayesian model, where our priors for Influence are drawn from the circulation estimates.


- Calibration: maybe we could rank the articles by concerning-ness, pick thresholds that correspond to the actual response breakdown, and train the LLM on these. We evaluate the accuracy by prediction.
- On the other hand, maybe there's a cumulative effect: multiple "somewhat" concerning articles could lead to becoming "highly concerned".
- Also consider, there's perhaps a max-level effect: a single "highly" concerning article probably takes precedence over having multiple "somewhat" concerning or "not very" concerning articles.
