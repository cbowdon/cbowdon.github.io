---
title: "Food"
date: "2025-05-19"
categories: [food, uk, r, stan, bayesian, timeseries, llm, surveys]
code-fold: true
freeze: false
---

There's a little drip drip drip of scare stories about ultra-processed food, a phrase that I'd never heard until this year. I'm starting to get worried. Is everyone else worried? Well it turns out the Food Standards Agency (FSA) has been running a tracker survey containing this question since mid-2023, so we can find out. It's a nice little dataset to practice my time series analysis skills on.

First we need to scrape the data. While it's lovely that the FSA publishes all the monthly survey results (which were run by YouGov) it is annoying that they don't publish compiled statistics, so we need to get our tibbles dirty.

As usual, if you like coding with R you can see the gory details under the folds.

```{r}
#| label: download-data
library(httr2)
library(rvest)

download_files <- function() {
  url <- "https://data.food.gov.uk/catalog/datasets/0bfd916a-4e01-4cb8-ba16-763f0b36b50c"

  html_content <- request(url) |>
    req_perform() |>
    resp_body_string()

  # Assuming html_content is already defined
  doc <- read_html(html_content)
  xlsx_links <- doc %>% html_nodes("a[href$='.xlsx']") %>% html_attr("href")
  for (link in xlsx_links) {
    file_url <- link
    file_name <- basename(file_url)
    destfile <- sprintf("data/%s", file_name)
    if (!file.exists(destfile)) {
      download.file(url = file_url, destfile = destfile)
    }
  }
}

download_files()
```

Is there an easy way to read tracker survey spreadsheets into R? They have a common format, but I never found anything that did the job and had to roll my own. Anyway, eventually we end up with a single dataset.

```{r}
#| label: tbl-monthly-spreadsheets
#| tbl-cap: Example of the survey data.
#| warning: false
library(tidyverse)
library(readxl)
library(knitr)

extract_month_year <- function(file) {
  decoded_filename <- URLdecode(file)

  month_year_regex <- "^.*(January|February|March|April|May|June|July|August|September|October|November|December)\\s?(\\d{2}|\\d{4}).*.xlsx$"

  if (grepl(month_year_regex, decoded_filename)) {
    extracted_month_year <- regmatches(
      decoded_filename,
      regexec(month_year_regex, decoded_filename)
    )

    # Extract month and year
    month <- extracted_month_year[[1]][[2]]
    year <- extracted_month_year[[1]][[3]]

    if (str_length(year) == 2) {
      year <- paste0("20", year)
    }

    return(list(filename = file, month = month, year = year))
  } else {
    print(file)
    return(NULL)
  }
}

extract_question <- function(file, question_prefix) {
  fmy <- extract_month_year(file)

  data <- read_excel(
    file,
    sheet = "Percents",
    col_names = FALSE
  )
  headers <- as.vector(data[5, ], mode = "character")
  headers[1] <- "QCategory"
  headers <- zoo::na.locf(headers)
  subheaders <- as.vector(data[6, ], mode = "character")
  subheaders <- zoo::na.fill(subheaders, "")
  combined_headers <- map2(
    headers,
    subheaders,
    \(x, y) str_c(x, y, sep = ":: ")
  )
  combined_headers[1] <- "QCategory"
  combined_headers[2] <- "Total"

  colnames(data) <- combined_headers

  has_question <- any(str_starts(pull(data, QCategory), question_prefix))

  if (!is.na(has_question)) {
    data |>
      filter(!is.na(QCategory)) |>
      slice(
        which(str_starts(QCategory, question_prefix)):n()
      ) |>
      head(10) |> # take the cats
      tail(-1) |> # drop the header
      mutate(
        filename = fmy$filename,
        monthname = fmy$month,
        year = as.integer(fmy$year),
        month = match(
          monthname,
          c(
            "January",
            "February",
            "March",
            "April",
            "May",
            "June",
            "July",
            "August",
            "September",
            "October",
            "November",
            "December"
          )
        )
      )
  }
}

question_df <- function(question_prefix) {
  df_filename <- sprintf("data/%s.rds", question_prefix)
  if (file.exists(df_filename)) {
    df <- read_rds(df_filename)
  } else {
    xlsx_files <- list.files(
      path = "data",
      pattern = "*.xlsx",
      full.names = TRUE
    )
    df <- xlsx_files |>
      sort(decreasing = TRUE) |>
      lapply(\(f) extract_question(f, question_prefix)) |>
      keep(\(x) !is.null(x)) |>
      bind_rows() |>
      mutate(Question = question_prefix)

    df |> write_rds(df_filename)
  }
  df
}

df <- question_df("Q12_14")

tidy_df <- df |>
  filter(QCategory != "Unweighted base" & QCategory != "Base: All") |>
  pivot_longer(
    c(Total, contains("::")),
    names_to = "Demographic",
    values_to = "Value"
  ) |>
  mutate(
    Period = as.Date(sprintf("%04d-%02d-01", year, month)),
    Response = QCategory,
    Value = as.numeric(Value)
  ) |>
  select(Question, Demographic, Period, Response, Value) |>
  arrange(Question, Period, Response, Demographic)

tidy_df |> head() |> kable()
```

Finally we have ✨tidy✨ data and we can visualise it.
  
```{r}
#| label: fig-responses
#| fig-cap: Tracker for "At the moment, how concerned, if at all, do you personally feel about ultra-processed, or over-processing of food?" Left is net concern, right is all responses.
#| warning: false
library(patchwork)

plot_net_concern <- ggplot(
  tidy_df |> filter(Demographic == "Total" & str_starts(Response, "Net: "))
) +
  aes(x = Period, y = Value) +
  scale_y_continuous(limits = c(0, 1)) +
  geom_line() +
  stat_smooth(method = "lm", linewidth = 0.1) +
  theme(
    axis.text.x = element_text(angle = 45)
  ) +
  labs(title = "Net concern", x = NULL, y = "Proportion of respondents")

plot_breakdown <- ggplot(
  tidy_df |> filter(Demographic == "Total" & !str_starts(Response, "Net: "))
) +
  aes(x = Period, y = Value, group = Response, colour = Response) +
  geom_line() +
  stat_smooth(method = "lm", linewidth = 0.1) +
  theme(
    axis.text.x = element_text(angle = 45)
  ) +
  labs(title = "Response breakdown", x = NULL, y = "Proportion of respondents")

plot_net_concern + plot_breakdown
```

The number of people with concerns is evidently slowly increasing. A linear regression fits net concern very well, so it's reasonable to expect continued growth at the same rate (pending any intervention like policy changes). Presumably the line bends as we approach the inevitable core of stubborn sods who will never admit to being concerned about anything, but the data we have doesn't support that kind of model.

The breakdown by response type is interesting. We have to watch our step, given the noisiness, but it does seem to show a migration towards being _highly_ concerned.

## What made people highly concerned?
  
I fitted a linear regression to each response above, but from the chunky confidence intervals you can see that this isn't necessarily the right model. You might expect that public concern is really driven by major media events, e.g. a major study linking ultra-processed food to cancer. So were there any particular points in time where the number of highly concerned people increased beyond the trend ("breaks")? Visually there are a few candidate points.

To start our analysis, let's look at how autocorrelated the responses are.
  
```{r}
#| label: fig-acf
#| fig-cap: ACF for "Highly concerned" responses.
hc <- tidy_df |>
  filter(Demographic == "Total" & Response == "Highly concerned")

acf(hc$Value)
```

There's definitely some autocorrelation.

```{r}
#| label: fig-pacf
#| fig-cap: PACF for "Highly concerned" responses.
pacf(hc$Value)
```

We can also do an autoregression, to understand how strongly each term depends on the previous terms.

```{r}
#| label: ar
ar.ols(hc$Value, order.max = 3, intercept = T, demean = F)
```

The ACF/PACF and the coefficient of 0.6 on AR(1) suggests the "highly concerned" survey responses depend strongly on their previous value (lag 1). The PACF is not significant beyond lag 1.

Let's now fit a simple Bayesian model where each value $y_t$ in the time series is drawn from a normal distribution centred on $\phi * y_{t-1} + c$ and examine the log likelihoods/residuals to identify potential breaks. To avoid double dipping, the model is using an uninformative prior for $\phi$, not based on the AR(1) coefficient. The Stan code is [here](ar1.stan).

```{r}
#| label: stan
#| warning: false
library(cmdstanr)

model <- cmdstan_model("ar1.stan", exe_file = "ar1.stan.exe")
outs <- model$sample(
  data = list(
    N = nrow(hc),
    y = hc$Value,
    sigma = 0.15
  ),
  parallel_chains = 4,
  show_messages = F # remember to re-enable this when you're actually working
)
outs
```

The model hasn't tightened up the prior much, so it hasn't learned a lot.

```{r}
#| label: fig-phi-dist
#| fig-cap: Posterior and prior.
phi_posterior <- outs$draws() |>
  as_tibble() |>
  select(contains("phi")) |>
  pivot_longer(everything()) |>
  mutate(name = "posterior")

phi_prior <- tibble(
  name = "prior",
  value = runif(40000)
)

ggplot(bind_rows(phi_prior, phi_posterior)) +
  aes(x = value, group = name, fill = name) +
  geom_density(alpha = 0.5)
```

We can look at the residuals and log likelihoods anyway, though I don't think they're telling us much more than a visual inspection.

```{r}
#| label: fig-resids
#| fig-cap: Log likelihood and residuals for the Bayesian fit.
log_liks <- outs$draws() |>
  as_tibble() |>
  select(contains("log_lik")) |>
  pivot_longer(everything()) |>
  mutate(t = as.integer(str_extract(name, "\\[(\\d+)\\]", group = 1))) |>
  group_by(t) |>
  summarise(value = mean(value)) |>
  arrange(t) |>
  ggplot() +
  aes(x = t, y = value) +
  geom_line() +
  labs(title = "Log likelihood")

resids <- outs$draws() |>
  as_tibble() |>
  select(contains("resid")) |>
  pivot_longer(everything()) |>
  mutate(t = as.integer(str_extract(name, "\\[(\\d+)\\]", group = 1))) |>
  group_by(t) |>
  summarise(value = mean(value)) |>
  arrange(t) |>
  ggplot() +
  aes(x = t, y = value) +
  geom_line() +
  labs(title = "Residuals")

resids + log_liks
```

The most interesting point is t=16 (Dec 2024) which has a low log likelihood and high residual.

It's hard to say if this corresponds to a particular media event. The Daily Mail regularly posts articles about the risks of ultra-processed food. Here's a sample:

> They're linked to cancer, heart disease, diabetes - and our shocking obesity crisis. But what exactly are they and how can you avoid them? Britain's top experts provide the definitive guide to cutting ultra-processed food from your diet
_Daily Mail, 17th Mar 2024_

> Britain facing cancer timebomb in the under-50s due to poor diet and gut health - and ultra-processed food could be to blame
_Daily Mail, 2nd Jun 2024_

> Ultra-processed food and obesity could be behind 'disturbing' rise in cancer hitting young people, warns UK's top doctors
_Daily Mail, 3rd Jun 2024_

> Yet ANOTHER deadly harm of ultra-processed food: Junk raises risk of debilitating bone disease, scientists discover
_Daily Mail, 4th Dec 2024_

Are such stories a major driver of public concern about ultra-processed food in the UK?

## The media

This leads us to an interesting observation: a very small number of publications account for a large proportion of UK readership. See this data from [JournoFinder](https://journofinder.com/blog/uk-newspaper-circulation-figures).

![](https://framerusercontent.com/images/1BjmhUMPE6dsrYLtK1qkJV8p5k.png)

The UK population is approx 70 million people. Given that the Guardian and the Daily Mail target very different demographics (left and right wing) we don't expect much overlap between their readership, i.e. their combined monthly online readership alone covers approximately half the British population. Note also that BBC News isn't included, but has a similar level of traffic (albeit with higher overlap).

This suggests that if we're interested in understanding how the media shapes public perception in the UK, we can do well from analysing the output of just a handful of sources. This is particularly true in the modern media landscape, in which:

- Live television is a dying medium. (I was at [Datacove's](https://datacove.co.uk) BristolR meetup recently with a fireside chat on marketing channel modelling. No one present had watched TV in the last 24 hours, and only a handful in the last month.)
- Radio has had its time, has had its power (though yet to have its finest hour 🎸).
- Social media is increasingly popular, but news content on social media is dominated by reposts of articles from traditional journalists.

So let's create a corpus of news articles about ultra-processed food from the top 10 sources in the table above, plus the BBC. They have a combined readership of 64.8 million monthly readers in the UK (plus the BBC's unknown-but-high readership); even with high overlap, this represents the majority of the UK population. It's enough to give us a grand understanding of what's going on.

I happen to work at [Polecat](https://www.polecat.com) who have licensed access to this data, so I can analyse it very easily. (Can't share the raw data here though.)

Since November 2023 there have been over 2.5 million articles from those sources, but <1000 mentioned ultra-processed food, and <100 seem to be focused on it, as opposed to mentioning it in passing alongside other public health issues. Though the passing mentions would help with awareness, these have a weaker effect on informing opinion.

```{r}
#| label: fig-article-counts
#| fig-cap: Counts of articles mentioning ultra-processed food from the top UK online news sources, by month.
bind_rows(
  jsonlite::read_json(
    "data/focus-articles-counts.json",
    simplifyVector = T
  ) |>
    as_tibble() |>
    mutate(MentionType = "Focus articles"),
  jsonlite::read_json(
    "data/all-articles-counts.json",
    simplifyVector = T
  ) |>
    as_tibble() |>
    mutate(MentionType = "Articles with passing mentions")
) |>
  mutate(Period = as.Date(key_as_string)) |>
  ggplot() +
  aes(x = Period, y = doc_count, group = MentionType) +
  geom_col() +
  facet_wrap(~MentionType) +
  labs(
    title = "Articles mentioning ultra-processed food from top UK sources by month.",
    x = NULL,
    y = "Article count"
  )
```

The takeaway here is that the number of articles mentioning ultra-processed food is _not_ increasing significantly. If we teased out the seasonality and squinted a bit we might say that there's a slight upward trend, but it wouldn't be a firm foundation. News articles aren't fungible commodities! You can't swap any one for another and expect the same impact on public perception. We need to take a more nuanced view.

## AI-simulated polling

An idea that's generating a lot of excitement recently is the suggestion that you can simulate public opinion polls with AI. The concept is very simple: you prime multiple LLM assistants to represent your different demographics and then sample their responses to questions.

Accurately representing demographics is a Sisyphean task, but I'm a dedicated disciple of Mr. Pareto so let's see what we can do for 20% of the effort.

R afficionados, we are using [ellmer](https://ellmer.tidyverse.org), which is becoming the de-facto AI package for R.

```{r}
#| label: ai-sim-functions
library(ellmer)

response_cats <- tibble(
  Cat = LETTERS[1:5],
  Response = c(
    "Highly concerned",
    "Somewhat concerned",
    "Not very concerned",
    "Not concerned at all",
    "Don't know"
  )
)

llm_answer_distribution <- function(
  headline,
  source,
  date,
  simulated_identity = "Statistically average person"
) {
  chat <- chat_openai(
    #base_url = "http://localhost:1234/v1/",
    #model = "ministral-8b-instruct-2410",
    model = "gpt-4.1-nano",
    system_prompt = sprintf(
      "You are a member of the British public, with your own life experiences and opinions.

    This is your identity: 
    <identity>
    %s
    </identity>
    
    
    You are being asked for your views as part of a survey. Respond to each question from the pollster.",
      simulated_identity
    ),
    params = params(
      log_probs = TRUE,
      seed = 42,
      n = 1,
      top_p = 1,
      top_logprobs = 5,
    )
  )

  # Now we need a specific answer.
  response <- chat$chat_structured(
    sprintf(
      "Consider the following headline then answer the question.

  HEADLINE: %s (%s, %s)

  QUESTION: At the moment, how concerned, if at all, do you personally feel about ultra-processed, or over-processing of food?
    
  A) Highly concerned
  B) Somewhat concerned
  C) Not very concerned
  D) Not concerned at all
  E) Don't know
  
  Respond with the letter of your chosen answer.",
      headline,
      source,
      format.Date(date, "%d %b %Y")
    ),
    type = type_enum(description = "Answer letter", values = LETTERS[1:5])
  )

  answer_logprobs <- chat$last_turn()@json$choices[[1]]$logprobs$content[[
    4
  ]]$top_logprobs

  answer_probs <- tibble(
    token = sapply(answer_logprobs, \(x) x$token),
    logprob = sapply(answer_logprobs, \(x) x$logprob)
  ) |>
    mutate(Probability = exp(logprob)) |>
    rename(Cat = token) |>
    merge(response_cats) |>
    select(Response, Probability)
}
```

We can now get the probability that the LLM would have selected each response in this conversation, given its identity.

```{r}
#| label: tbl-llm-simulation
#| tbl-cap: Example of simulated AI polling response to an alarming headline.
#| code-fold: false
llm_answer_distribution(
  headline = "Ultra-processed food linked to potential bowel cancer risk, scientists say",
  source = "Daily Mail",
  date = as.Date("2024-05-01")
) |>
  kable(digits = 3)
```

 We can also manipulate that identity to get a different response distribution.

```{r}
#| label: tbl-hypochondriac
#| tbl-cap: Response distribution of LLM again, this time with "hypochondriac" identity.
llm_answer_distribution(
  headline = "Ultra-processed food linked to potential bowel cancer risk, scientists say",
  source = "Daily Mail",
  date = as.Date("2024-05-01"),
  simulated_identity = "Hypochondriac"
) |>
  kable(digits = 3)
```

Let's apply this to our articles.

```{r}
#| label: tbl-articles
#| tbl-cap: A sample of the articles collected.
articles <- read_csv(
  "data/ultra-processed-food-docs-titles-only.csv",
  col_select = c("harvestTimeLong", "domain", "title"),
  show_col_types = F
) |>
  mutate(
    harvestTimeLong = parse_date(
      str_extract(harvestTimeLong, "(.*) @.*", 1),
      "%b %d, %Y"
    ),
    ArticleId = row_number()
  ) |>
  rename(Title = title, Date = harvestTimeLong, Domain = domain)

articles |> head() |> kable()
```

We can use `furrr` to run every article past the simulated survey respondent in parallel.

```{r}
#| label: tbl-example
#| tbl-cap: Example of the simulated response.
#| warning: false
library(furrr)

sim_ident <- "Average person"

rows <- lapply(1:nrow(articles), function(i) as.list(articles[i, ]))

with(plan(multisession, workers = 4), {
  tasks <- future_map(
    rows,
    function(row) {
      dist <- llm_answer_distribution(
        headline = row$Title,
        source = row$Domain,
        date = row$Date,
        simulated_identity = sim_ident
      )
      dist |>
        mutate(ArticleId = row$ArticleId, simulated_identity = sim_ident)
    }
  )
})

article_responses <- bind_rows(tasks) |>
  merge(articles) |>
  as_tibble() |>
  mutate(
    # responses as ordered factor for plotting
    Response = factor(
      Response,
      c(
        "Highly concerned",
        "Somewhat concerned",
        "Not very concerned",
        "Not concerned at all",
        "Don't know"
      ),
      ordered = TRUE
    )
  )

article_responses |>
  group_by(ArticleId) |>
  filter(Probability == max(Probability)) |>
  head() |>
  kable()
```

The simplest thing to do is plot the distribution of responses at each point in time, based on the article seen at that time.

```{r}
#| label: fig-probability
#| fig-cap: Probability distribution at each point in time.
ggplot(article_responses) +
  aes(
    x = Date,
    y = Probability,
    group = Response,
    fill = Response,
    colour = Response
  ) +
  geom_area()
```

In this plot we can see that more recently there has been more probability of "Highly concerning" and less of "Not concerned at all".

However this simplistic model assumes previous articles have no lasting impact, which is obviously wrong. Public opinion doesn't work like that.

How should we model the persistent effect of old news?

## A better model

Typing as I think, there are two ways to go. First we could build some mathematical model, which is a function of the point in time, the previous distribution, and the new distribution predicted for the latest article. This would be a semi-arbitrary rules-based approach, where we would pick how to weight older points. We can use some empirical results for this: the autoregression above suggests we put 0.6 weighting on the prior month.

Alternatively, we could build the prior articles into the LLM's context. The challenge there is representing the time that has passed between seeing the articles. Perhaps we give the LLM a sliding window of say 1-month, and ask it to predict based on only the articles in that month. But we'd still have to incorporate the LLM's "position" prior to the new article somehow. We could perhaps just tell it "you are Somewhat Concerned". Lots of fun, but is it scientific?

To be continued...
